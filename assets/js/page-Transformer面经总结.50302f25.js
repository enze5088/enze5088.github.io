(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{540:function(t,s,a){"use strict";a.r(s);var i=a(1),e=Object(i.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"transformer面经总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transformer面经总结"}},[t._v("#")]),t._v(" Transformer面经总结")]),t._v(" "),a("h2",{attrs:{id:"高频提问"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#高频提问"}},[t._v("#")]),t._v(" 高频提问")]),t._v(" "),a("ol",[a("li",[a("p",[t._v("介绍一下Transformer的原理。")]),t._v(" "),a("p",[a("strong",[t._v("延伸提问")])]),t._v(" "),a("ol",[a("li",[t._v("Encoder中的Feed Forward的结构是如何的?使用了什么激活函数？相关优缺点？")]),t._v(" "),a("li",[t._v("讲一下Transformer中的残差结构以及意义？")]),t._v(" "),a("li",[t._v("Encoder端和Decoder端是如何进行交互的？")]),t._v(" "),a("li",[t._v("Transformer为什么用+不用concat？")]),t._v(" "),a("li",[t._v("wordpiece的作用是什么，简单描述一下wordpiece model 和 byte pair encoding？")])])]),t._v(" "),a("li",[a("p",[t._v("Transformer为什么可以并行？")]),t._v(" "),a("p",[a("strong",[t._v("延伸提问")])]),t._v(" "),a("ol",[a("li",[t._v("Decoder端可以做并行化吗？")]),t._v(" "),a("li",[t._v("LSTM时间复杂度如何计算，Transformer 时间复杂度如何计算？")])])]),t._v(" "),a("li",[a("p",[t._v("Transformer的position embedding和BERT的position embedding的区别？")]),t._v(" "),a("p",[a("strong",[t._v("延伸提问")])]),t._v(" "),a("ol",[a("li",[t._v("你还了解哪些关于位置编码的技术，各自的优缺点是什么？")])])]),t._v(" "),a("li",[a("p",[t._v("Transformer里面LayerNorm的作用？")]),t._v(" "),a("p",[a("strong",[t._v("延伸提问")])]),t._v(" "),a("ol",[a("li",[t._v("为什么transformer块使用LayerNorm而不是BatchNorm？")]),t._v(" "),a("li",[t._v("LayerNorm 在Transformer的位置是哪里？")]),t._v(" "),a("li",[t._v("简答讲一下BatchNorm技术，以及它的优缺点。")])])]),t._v(" "),a("li",[a("p",[t._v("Transformer里多头注意力机制/自注意力机制的计算过程是怎样的？")]),t._v(" "),a("p",[a("strong",[t._v("延伸提问")])]),t._v(" "),a("ol",[a("li",[t._v("自注意力的计算公式是怎样的？")]),t._v(" "),a("li",[t._v("Multi-Head Attention是什么，有什么作用？")]),t._v(" "),a("li",[t._v("为什么在进行softmax之前需要对attention进行scaled/Attention计算时为什么要除根dk，q\\k\\v分别是如何算的？")]),t._v(" "),a("li",[t._v("Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？加性、乘性attention的公式？")]),t._v(" "),a("li",[t._v("Transformer的中的attention机制，其中self-attention和encoder-decoder attention之间的关系？")]),t._v(" "),a("li",[t._v("bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？")]),t._v(" "),a("li",[t._v("self-attention代码实现？")])])]),t._v(" "),a("li",[a("p",[t._v("Transformer的结构，作为特征处理器它跟LSTM、CNN的主要区别在哪里？各有哪些优缺点？")]),t._v(" "),a("p",[a("strong",[t._v("延伸提问")])]),t._v(" "),a("ol",[a("li",[t._v("Transformer相对于传统的RNN网络有什么好处？")]),t._v(" "),a("li",[t._v("Transformer相对RNN为什么能避免梯度消失？")]),t._v(" "),a("li",[t._v("Transformer与Bert，GPT的联系与区别？")]),t._v(" "),a("li",[t._v("Transformer XL和Transformer的主要区别是什么？XLNet有哪些突出的有点，有哪些创新的地方？")]),t._v(" "),a("li",[t._v("Transformer的计算代价瓶颈在哪里？")])])])]),t._v(" "),a("h2",{attrs:{id:"原始参考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#原始参考"}},[t._v("#")]),t._v(" 原始参考")]),t._v(" "),a("p",[t._v("1."),a("a",{attrs:{href:"https://www.nowcoder.com/discuss/648356",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.nowcoder.com/discuss/648356"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("transformer介绍一下原理，transformer为什么可以并行，它的计算代价瓶颈在哪？多头注意力机制计算过程？")]),t._v(" "),a("p",[t._v("BERT介绍一下原理，怎么用BERT计算文本相似度，有哪两种计算方法？（我回答的是，第一种方法是两个文本拼接作为BERT的输入，顶层加一个sigmod函数，第二种是，两个文本分别输入到BERT，得到特征向量，然后计算余弦相似度。）这两种方法的复杂度哪个高？")]),t._v(" "),a("p",[t._v("transformer（果然是nlp面试必考），介绍transformer里自注意力机制的计算过程，为什么要进行缩放，介绍下bert位置编码和transformer的区别，哪个好，为什么（为什么。。这个是真不会，我也如实说了不会，面试官说，开放题，说说你的理解，我就说通过学习出来的，可能会过拟合，也可能会学到更细微的特征）")]),t._v(" "),a("p",[t._v("2."),a("a",{attrs:{href:"https://www.nowcoder.com/discuss/648119",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.nowcoder.com/discuss/648119"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("你认为Transformer同LSTM这些有什么区别和关系?")]),t._v(" "),a("p",[t._v("transformer的position embedding和BERT的position embedding的区别.")]),t._v(" "),a("p",[t._v("了解seq2seq吗?有没有用过对应的transformer进行对应的使用项目?")]),t._v(" "),a("p",[t._v("3."),a("a",{attrs:{href:"https://www.nowcoder.com/discuss/641848",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.nowcoder.com/discuss/641848"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("介绍transformer（从encode端到decode端完整的说了一遍")]),t._v(" "),a("p",[t._v("bert介绍（跟transformer差不多把，就是多了两个预训练任务")]),t._v(" "),a("p",[t._v("4."),a("a",{attrs:{href:"https://www.nowcoder.com/discuss/639224",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.nowcoder.com/discuss/639224"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("说一下BERT和transformer的positional embedding有啥区别（多了一个矩阵，多了一个dropout几里哇啦）")]),t._v(" "),a("p",[t._v("5."),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/266540739",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://zhuanlan.zhihu.com/p/266540739"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("你讲下Transformer里面LayerNorm的作用？")]),t._v(" "),a("p",[t._v("Encoder中的Feed Forward?")]),t._v(" "),a("p",[t._v("由两层全连接层构成，第一层全连接层的W的维度为[3072，768]，bias的维度为[3072]；第二层的全连接层的W的维度为[768,3072]，bias的维度为[768]。输出再经过Gelu激活函数，就得到了FeedForward的输出。")]),t._v(" "),a("p",[t._v("6."),a("a",{attrs:{href:"https://blog.csdn.net/qq_40092110/article/details/109247383",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://blog.csdn.net/qq_40092110/article/details/109247383"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("Transformer模型原理和它的结构；")]),t._v(" "),a("p",[t._v("Multi-Head Attention是什么，有什么作用？multi head什么意思，attention计算时为什么要除根dk，q\\k\\v分别是如何算的")]),t._v(" "),a("p",[t._v("attention和self-attention")]),t._v(" "),a("p",[t._v("self-attention如何实现的，介绍了一下原理，面试官又问代码具体如何实现")]),t._v(" "),a("p",[t._v("7."),a("a",{attrs:{href:"https://www.nowcoder.com/discuss/486194",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.nowcoder.com/discuss/486194"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("bert 为什么scale product，transformer里encoder的什么部分输入给decoder， MLM 为什么mask一部分保留一部分， albert，roberta， electra做了什么改进")]),t._v(" "),a("p",[t._v("8."),a("a",{attrs:{href:"https://blog.csdn.net/weixin_40920183/article/details/107777228",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://blog.csdn.net/weixin_40920183/article/details/107777228"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("Transformer与GPT的联系与区别")]),t._v(" "),a("p",[t._v("9."),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/153333432",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://zhuanlan.zhihu.com/p/153333432"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("Transformer的结构，作为特征处理器它跟LSTM的主要区别在哪里？各有哪些优缺点？")]),t._v(" "),a("p",[t._v("Transformer XL和Transformer的主要区别是什么？XLNet有哪些突出的有点，有哪些创新的地方？")]),t._v(" "),a("p",[t._v("10."),a("a",{attrs:{href:"https://mp.weixin.qq.com/s?src=11&timestamp=1620807181&ver=3063&signature=SYGLpJUrt0gwGWRtN48oTtCOWxfBOtSRvZuwTBflU8rXZW4ToIKjj-HWiyMATUrVyDS6EW4ezMsM2-RqiOX6CvGSJhzW4-f2oEh1i9RRO8zpXlQkAp5iz9XtVjviY92V&new=1",target:"_blank",rel:"noopener noreferrer"}},[t._v("链接"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("bert如何使用transformer的encoding模块-bert的输入和transformer有什么不同")]),t._v(" "),a("p",[t._v("transformer的中的attention机制，其中self-attention和encoder-decoder attention之间的关系")]),t._v(" "),a("p",[t._v("transformer为什么用+不用concat")]),t._v(" "),a("p",[t._v("已有总结文章")]),t._v(" "),a("ol",[a("li",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/149799951",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://zhuanlan.zhihu.com/p/149799951"),a("OutboundLink")],1),t._v(" "),a("ol",[a("li",[t._v("Transformer为何使用多头注意力机制？（为什么不使用一个头）")]),t._v(" "),a("li",[t._v("Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？")]),t._v(" "),a("li",[t._v("为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解")]),t._v(" "),a("li",[t._v("Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？")]),t._v(" "),a("li",[t._v("在计算attention score的时候如何对padding做mask操作？")])])]),t._v(" "),a("li",[t._v("https://zhuanlan.zhihu.com/p/363466672\n"),a("ol",[a("li",[t._v("为什么在进行多头注意力的时候需要对每个head进行降维？")]),t._v(" "),a("li",[t._v("大概讲一下Transformer的Encoder模块？")]),t._v(" "),a("li",[t._v("为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？")]),t._v(" "),a("li",[t._v("简单介绍一下Transformer的位置编码？有什么意义和优缺点？")]),t._v(" "),a("li",[t._v("你还了解哪些关于位置编码的技术，各自的优缺点是什么？")]),t._v(" "),a("li",[t._v("简单讲一下Transformer中的残差结构以及意义。")]),t._v(" "),a("li",[t._v("为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？")]),t._v(" "),a("li",[t._v("简答讲一下BatchNorm技术，以及它的优缺点。")]),t._v(" "),a("li",[t._v("简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？")]),t._v(" "),a("li",[t._v("Encoder端和Decoder端是如何进行交互的？")]),t._v(" "),a("li",[t._v("Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？")]),t._v(" "),a("li",[t._v("Transformer的并行化提现在哪个地方？")]),t._v(" "),a("li",[t._v("Decoder端可以做并行化吗？")]),t._v(" "),a("li",[t._v("简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？")]),t._v(" "),a("li",[t._v("Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？")])])]),t._v(" "),a("li",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/151412524",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://zhuanlan.zhihu.com/p/151412524"),a("OutboundLink")],1),t._v(" "),a("ol",[a("li",[t._v("wordpiece的作用")]),t._v(" "),a("li",[t._v("self-attention相比lstm优点是什么？")])])]),t._v(" "),a("li",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/129409553",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://zhuanlan.zhihu.com/p/129409553"),a("OutboundLink")],1),t._v(" "),a("ol",[a("li",[t._v("Transformer相对于传统的RNN网络有什么好处；")]),t._v(" "),a("li",[t._v("Transformer里的Self-Attention作用是什么，有什么优势；")]),t._v(" "),a("li",[t._v("你提到了梯度消失的问题，那么Transformer相对RNN为什么能避免梯度消失；")])])]),t._v(" "),a("li",[t._v("https://zhuanlan.zhihu.com/p/359555994\n"),a("ol",[a("li",[t._v("LSTM时间复杂度，Transformer 时间复杂度\n"),a("ul",[a("li",[t._v("LSTM时间复杂度：序列长度*向量长度²")]),t._v(" "),a("li",[t._v("transformer时间复杂度：序列长度²*向量长度")])])])])]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.zhihu.com/question/318355038",target:"_blank",rel:"noopener noreferrer"}},[t._v("bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？"),a("OutboundLink")],1)])]),t._v(" "),a("p",[t._v("Transformer的position embedding和BERT的position embedding的区别？")]),t._v(" "),a("p",[t._v("答：Transformer在编码词向量时引入了位置编码（Position Embedding）的特征。其编码公式如下：")]),t._v(" "),a("p",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("P")]),a("mi",[t._v("E")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mn",[t._v("2")]),a("mi",[t._v("i")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("sin")]),a("mo",[t._v("⁡")]),a("mrow",[a("mo",{attrs:{fence:"true"}},[t._v("(")]),a("mfrac",[a("mrow",[a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")])],1),a("mrow",[a("mn",[t._v("1000")]),a("msup",[a("mn",[t._v("0")]),a("mfrac",[a("mrow",[a("mn",[t._v("2")]),a("mi",[t._v("i")])],1),a("msub",[a("mi",[t._v("d")]),a("mtext",[t._v("model ")])],1)],1)],1)],1)],1),a("mo",{attrs:{fence:"true"}},[t._v(")")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("P E( pos, 2 i)=\\sin \\left(\\frac{p o s}{10000^{\\frac{2 i}{d_{\\text {model }}}}}\\right)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05764em"}},[t._v("PE")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("p")]),a("span",{staticClass:"mord mathnormal"},[t._v("os")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord"},[t._v("2")]),a("span",{staticClass:"mord mathnormal"},[t._v("i")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"2.4em","vertical-align":"-0.95em"}}),a("span",{staticClass:"mop"},[t._v("sin")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"minner"},[a("span",{staticClass:"mopen delimcenter",staticStyle:{top:"0em"}},[a("span",{staticClass:"delimsizing size3"},[t._v("(")])]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mopen nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.7475em"}},[a("span",{staticStyle:{top:"-2.19em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.0395em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("1000")]),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.485em"}},[a("span",{staticStyle:{top:"-3.7375em","margin-right":"0.0714em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"sizing reset-size3 size1 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mopen nulldelimiter sizing reset-size1 size6"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.0465em"}},[a("span",{staticStyle:{top:"-2.468em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3448em"}},[a("span",{staticStyle:{top:"-2.3448em","margin-left":"0em","margin-right":"0.1em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.6944em"}}),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord text mtight"},[a("span",{staticClass:"mord mtight"},[t._v("model ")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3496em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-3.2255em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"frac-line mtight",staticStyle:{"border-bottom-width":"0.049em"}})]),a("span",{staticStyle:{top:"-3.387em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("i")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8816em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter sizing reset-size1 size6"})])])])])])])])])])])])]),a("span",{staticStyle:{top:"-3.2695em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.0395em"}}),a("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),a("span",{staticStyle:{top:"-3.4856em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.0395em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("p")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("os")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8495em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter"})]),a("span",{staticClass:"mclose delimcenter",staticStyle:{top:"0em"}},[a("span",{staticClass:"delimsizing size3"},[t._v(")")])])])])])]),t._v(" "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("P")]),a("mi",[t._v("E")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mn",[t._v("2")]),a("mi",[t._v("i")]),a("mo",[t._v("+")]),a("mn",[t._v("1")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("cos")]),a("mo",[t._v("⁡")]),a("mrow",[a("mo",{attrs:{fence:"true"}},[t._v("(")]),a("mfrac",[a("mtext",[t._v(" pos ")]),a("mrow",[a("mn",[t._v("1000")]),a("msup",[a("mn",[t._v("0")]),a("mfrac",[a("mrow",[a("mn",[t._v("2")]),a("mi",[t._v("i")])],1),a("msub",[a("mi",[t._v("d")]),a("mtext",[t._v("model ")])],1)],1)],1)],1)],1),a("mo",{attrs:{fence:"true"}},[t._v(")")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("P E( pos, 2 i+1)=\\cos \\left(\\frac{\\text { pos }}{10000^{\\frac{2 i}{d_{\\text {model }}}}}\\right)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05764em"}},[t._v("PE")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal"},[t._v("p")]),a("span",{staticClass:"mord mathnormal"},[t._v("os")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord"},[t._v("2")]),a("span",{staticClass:"mord mathnormal"},[t._v("i")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"2.4em","vertical-align":"-0.95em"}}),a("span",{staticClass:"mop"},[t._v("cos")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"minner"},[a("span",{staticClass:"mopen delimcenter",staticStyle:{top:"0em"}},[a("span",{staticClass:"delimsizing size3"},[t._v("(")])]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mopen nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.7475em"}},[a("span",{staticStyle:{top:"-2.19em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.0395em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("1000")]),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.485em"}},[a("span",{staticStyle:{top:"-3.7375em","margin-right":"0.0714em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"sizing reset-size3 size1 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mopen nulldelimiter sizing reset-size1 size6"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.0465em"}},[a("span",{staticStyle:{top:"-2.468em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3448em"}},[a("span",{staticStyle:{top:"-2.3448em","margin-left":"0em","margin-right":"0.1em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.6944em"}}),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord text mtight"},[a("span",{staticClass:"mord mtight"},[t._v("model ")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3496em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-3.2255em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"frac-line mtight",staticStyle:{"border-bottom-width":"0.049em"}})]),a("span",{staticStyle:{top:"-3.387em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("2")]),a("span",{staticClass:"mord mathnormal mtight"},[t._v("i")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8816em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter sizing reset-size1 size6"})])])])])])])])])])])])]),a("span",{staticStyle:{top:"-3.2695em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.0395em"}}),a("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),a("span",{staticStyle:{top:"-3.4856em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3.0395em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord text mtight"},[a("span",{staticClass:"mord mtight"},[t._v(" pos ")])])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8495em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter"})]),a("span",{staticClass:"mclose delimcenter",staticStyle:{top:"0em"}},[a("span",{staticClass:"delimsizing size3"},[t._v(")")])])])])])])]),t._v(" "),a("p",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("p")]),a("mi",[t._v("o")]),a("mi",[t._v("s")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("pos")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.625em","vertical-align":"-0.1944em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("p")]),a("span",{staticClass:"mord mathnormal"},[t._v("os")])])])]),t._v("示单词的位置， "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("i")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("i")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.6595em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("i")])])])]),t._v("表示单词的维度。关于位置编码的实现可在Google开源的算法中"),a("code",[t._v("get_timing_signal_1d()")]),t._v("函数找到对应的代码。")]),t._v(" "),a("p",[t._v("作者这么设计的原因是考虑到在NLP任务中，除了单词的绝对位置，单词的相对位置也非常重要。根据公式 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("sin")]),a("mo",[t._v("⁡")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("α")]),a("mo",[t._v("+")]),a("mi",[t._v("β")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("sin")]),a("mo",[t._v("⁡")]),a("mi",[t._v("α")]),a("mi",[t._v("cos")]),a("mo",[t._v("⁡")]),a("mi",[t._v("β")]),a("mo",[t._v("+")]),a("mi",[t._v("cos")]),a("mo",[t._v("⁡")]),a("mi",[t._v("α")]),a("mi",[t._v("sin")]),a("mo",[t._v("⁡")]),a("mi",[t._v("β")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\sin (\\alpha+\\beta)=\\sin \\alpha \\cos \\beta+\\cos \\alpha \\sin \\beta")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mop"},[t._v("sin")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8889em","vertical-align":"-0.1944em"}}),a("span",{staticClass:"mop"},[t._v("sin")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mop"},[t._v("cos")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8889em","vertical-align":"-0.1944em"}}),a("span",{staticClass:"mop"},[t._v("cos")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mop"},[t._v("sin")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")])])])]),t._v("以及"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("cos")]),a("mo",[t._v("⁡")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("α")]),a("mo",[t._v("+")]),a("mi",[t._v("β")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("cos")]),a("mo",[t._v("⁡")]),a("mi",[t._v("α")]),a("mi",[t._v("cos")]),a("mo",[t._v("⁡")]),a("mi",[t._v("β")]),a("mo",[t._v("−")]),a("mi",[t._v("sin")]),a("mo",[t._v("⁡")]),a("mi",[t._v("α")]),a("mi",[t._v("sin")]),a("mo",[t._v("⁡")]),a("mi",[t._v("β")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\cos (\\alpha+\\beta)=\\cos \\alpha \\cos \\beta-\\sin \\alpha \\sin \\beta")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mop"},[t._v("cos")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8889em","vertical-align":"-0.1944em"}}),a("span",{staticClass:"mop"},[t._v("cos")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mop"},[t._v("cos")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),a("span",{staticClass:"mbin"},[t._v("−")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8889em","vertical-align":"-0.1944em"}}),a("span",{staticClass:"mop"},[t._v("sin")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mop"},[t._v("sin")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")])])])]),t._v(" ，这表明位置 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("k")]),a("mo",[t._v("+")]),a("mi",[t._v("p")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("k+p")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.7778em","vertical-align":"-0.0833em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.625em","vertical-align":"-0.1944em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("p")])])])]),t._v(" 的位置向量可以表示为位置  "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("k")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("k")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.6944em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])])]),t._v("的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。")]),t._v(" "),a("h2",{attrs:{id:"all-attentions-in-transformer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#all-attentions-in-transformer"}},[t._v("#")]),t._v(" All attentions in Transformer")]),t._v(" "),a("p",[t._v("为了深入理解transformer中attention的应用。我们不妨先来回归一下 transformer的模型结构。套用这张经典的图片，可以看到，transformer采用的是典型的encoder-decoder架构。")]),t._v(" "),a("p",[t._v("而encoder和decoder又是由什么组成的呢？")]),t._v(" "),a("p",[t._v("对于公式(1)其实很好理解，注意力公式主要就是算 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("V")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("V")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.22222em"}},[t._v("V")])])])]),t._v(" 的加权后的表示，说到加权，必要得有权重啊。权重就是前面的"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("softmax")]),a("mo",[t._v("⁡")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mo",[t._v("∗")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\operatorname{softmax}(*)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mop"},[a("span",{staticClass:"mord mathrm"},[t._v("softmax")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord"},[t._v("∗")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("部分，为什么要加"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("softmax")]),a("mo",[t._v("⁡")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\operatorname{softmax}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.6944em"}}),a("span",{staticClass:"mop"},[a("span",{staticClass:"mord mathrm"},[t._v("softmax")])])])])]),t._v(" ，因为权重必须为概率分布即和为1。  里面部分"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mfrac",[a("mrow",[a("mi",[t._v("Q")]),a("msup",[a("mi",[t._v("K")]),a("mi",[t._v("T")])],1)],1),a("msqrt",[a("msub",[a("mi",[t._v("d")]),a("mi",[t._v("k")])],1)],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\frac{Q K^{T}}{\\sqrt{d_{k}}}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.6275em","vertical-align":"-0.538em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mopen nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.0895em"}},[a("span",{staticStyle:{top:"-2.5864em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord sqrt mtight"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8622em"}},[a("span",{staticClass:"svg-align",staticStyle:{top:"-3em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord mtight",staticStyle:{"padding-left":"0.833em"}},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3448em"}},[a("span",{staticStyle:{top:"-2.3488em","margin-left":"0em","margin-right":"0.0714em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.5em"}}),a("span",{staticClass:"sizing reset-size3 size1 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.1512em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-2.8222em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"hide-tail mtight",staticStyle:{"min-width":"0.853em",height:"1.08em"}},[a("svg",{attrs:{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice"}},[a("path",{attrs:{d:"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z"}})])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.1778em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-3.23em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),a("span",{staticStyle:{top:"-3.4461em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("Q")]),a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.9191em"}},[a("span",{staticStyle:{top:"-2.931em","margin-right":"0.0714em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.5em"}}),a("span",{staticClass:"sizing reset-size3 size1 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])])])])])])])])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.538em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter"})])])])]),t._v("算的就是注意力的原始分数，通过计算Q(query)与K(key)的点积得到相似度分数，其中"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msqrt",[a("msub",[a("mi",[t._v("d")]),a("mi",[t._v("k")])],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\sqrt{d_{k}}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1.04em","vertical-align":"-0.1828em"}}),a("span",{staticClass:"mord sqrt"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8572em"}},[a("span",{staticClass:"svg-align",staticStyle:{top:"-3em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord",staticStyle:{"padding-left":"0.833em"}},[a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("d")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3361em"}},[a("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])])])]),a("span",{staticStyle:{top:"-2.8172em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"hide-tail",staticStyle:{"min-width":"0.853em",height:"1.08em"}},[a("svg",{attrs:{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice"}},[a("path",{attrs:{d:"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z"}})])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.1828em"}},[a("span")])])])])])])]),t._v("起到一个调节作用，不至于过大或过小，导致 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("softmax")]),a("mo",[t._v("⁡")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\operatorname{softmax}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.6944em"}}),a("span",{staticClass:"mop"},[a("span",{staticClass:"mord mathrm"},[t._v("softmax")])])])])]),t._v(" 之后就非0即1。因此这种注意力的形式也叫缩放点积注意力机制。")]),t._v(" "),a("h3",{attrs:{id:"谈一谈decoder模块"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#谈一谈decoder模块"}},[t._v("#")]),t._v(" 谈一谈Decoder模块")]),t._v(" "),a("p",[t._v("本文主要是谈一些比较容易误解的细节点，说实话，把自己的理解用文字表达出来真是个细致活。")]),t._v(" "),a("p",[t._v("如果觉得对您有点帮助，帮忙点个在看或者赞。")]),t._v(" "),a("h4",{attrs:{id:"一个小小的问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一个小小的问题"}},[t._v("#")]),t._v(" 一个小小的问题")]),t._v(" "),a("p",[t._v("我先说一个自己花了点时间才琢磨出来的东西，其实不难，就是当时没转过弯来。")]),t._v(" "),a("p",[t._v("我们都知道，decoder的交互层，Q矩阵来自本身，K/V矩阵来自整个Encoder层输出。")]),t._v(" "),a("p",[t._v("但是对于每个单词都会有一个encoder的输出，那么K/V矩阵是用的其中哪个输出计算过来的？")]),t._v(" "),a("p",[t._v("我这个问题的问法其实是错误的。")]),t._v(" "),a("p",[t._v("我当时的理解背景是认为这个交互的过程很类似seq2seq的attention，它一般是使用最后一个时刻的隐层输出作为context vector。")]),t._v(" "),a("p",[t._v("我基于此产生了上面这个问题，这个K/V矩阵应该由哪个位置单词（对比RNN就是哪个时刻的单词）的输出生成。")]),t._v(" "),a("p",[t._v("后来看了一下代码，才明白自己错在哪里？")]),t._v(" "),a("p",[t._v("K/V矩阵的计算不是来自于某一个单词的输出，而是所有单词的输出汇总计算K/V矩阵。这个过程和在Encoder中计算K/V矩阵是一样的，只不过放在了交互层，一时没想明白。")]),t._v(" "),a("h4",{attrs:{id:"正文"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#正文"}},[t._v("#")]),t._v(" 正文")]),t._v(" "),a("p",[t._v("与Encoder很类似，Decoder同样由完全相同的N个大模块堆叠而成，原论文中N为6。")]),t._v(" "),a("p",[t._v("每个大的模块分为三部分：多头注意力层，交互层，前馈神经层；每个层内部尾端都含有 Add&Norm。")]),t._v(" "),a("p",[t._v("和Encoder重复的内容我就跳过了，之前讲过，没看过的同学可以去看那个文章。")]),t._v(" "),a("h5",{attrs:{id:"多头自注意力层"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#多头自注意力层"}},[t._v("#")]),t._v(" 多头自注意力层")]),t._v(" "),a("p",[t._v("首先谈一下多头自注意力层，这里需要注意的细节点是，需要对当前单词和之后的单词做mask。")]),t._v(" "),a("p",[t._v("为什么需要mask？")]),t._v(" "),a("p",[t._v("最常规的解释就是在预测阶段，你的模型看不见当前时刻的输出以及未来时刻单词。")]),t._v(" "),a("p",[t._v("这句话其实有点绕，如果读的不仔细会让人误解为mask的时候需要把当前时刻的单词也mask掉...(拖出去斩了吧)。")]),t._v(" "),a("p",[t._v("从代码角度讲，你只需要把当前时刻之后所有单词mask掉就好了。")]),t._v(" "),a("p",[t._v("我自己对这句话的理解是我们需要确保模型在训练和测试的时候没有GAP。")]),t._v(" "),a("p",[t._v('举个简单的例子来理解，如果做机器翻译，你需要翻译出来的句子是 "我/爱/吃/苹果"。')]),t._v(" "),a("p",[t._v("当前时刻是”爱“这个单词作为输入的一部分，另一部分是上一个时刻”我“作为输入的时候的输出值。")]),t._v(" "),a("p",[t._v("当然在机器翻译中，我们一般使用 teacher forcing加速收敛，所以这里就使用”我“作为当前时刻输入的另一个部分。")]),t._v(" "),a("p",[t._v("所以这个时候，输入就是”我“的编码信息和”爱“的编码信息（当然还有位置编码）。")]),t._v(" "),a("p",[t._v("我要预测的是”吃“这个单词。")]),t._v(" "),a("p",[t._v("如果我们没有mask，模型也是可以运行的，也就说此时”吃“和”苹果“两个词对”爱“这个时刻的输出是有贡献的。")]),t._v(" "),a("p",[t._v("那么问题来了，测试数据中你根本没有ground truth，你怎么办？")]),t._v(" "),a("p",[t._v("也就说，训练的时候，你的模型是基于知道这个时刻后面的单词进行的训练，但是测试的时候，做机器翻译，你不知道自己应该翻译出来什么东西。")]),t._v(" "),a("p",[t._v("这就是问题的核心。")]),t._v(" "),a("p",[t._v("你训练模型的时候，一部分精力花在了”吃“和”苹果“两个词上，这不就是无用功吗？")]),t._v(" "),a("p",[t._v("所以，确保模型在训练和测试的时候没有GAP，我们需要mask掉”吃“和”苹果“两个词。")]),t._v(" "),a("h5",{attrs:{id:"交互模块"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#交互模块"}},[t._v("#")]),t._v(" 交互模块")]),t._v(" "),a("p",[t._v("这一块需要注意的就是之前文章提到的，Q矩阵来自本身，K/V矩阵来自encoder的输出。")]),t._v(" "),a("p",[t._v("还有一个细节点是，K/V矩阵对应的是来自整个encoder的输出。")]),t._v(" "),a("p",[t._v("如果看transformer那个经典图的话，初期很容易理解为encoder和decode对应的每一层互相交互，这是不对的。")]),t._v(" "),a("p",[t._v("是整个输出与decoder做交互。")]),t._v(" "),a("p",[t._v("对于任意的"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("msub",[a("mi",[t._v("h")]),a("mi",[t._v("i")])],1),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("msub",[a("mi",[t._v("s")]),a("mi",[t._v("j")])],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("h_i,s_j")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.9805em","vertical-align":"-0.2861em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("h")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3117em"}},[a("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight"},[t._v("i")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[a("span")])])])])]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathnormal"},[t._v("s")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.3117em"}},[a("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.2861em"}},[a("span")])])])])])])])]),t._v(",以某种attention计算两者之间的分数，再将所有的结果形成的魏进行归一化。")]),t._v(" "),a("p",[t._v("Attention 机制计算过程大致可以分成三步：")]),t._v(" "),a("blockquote",[a("p",[t._v("① 信息输入：将 Q，K，V 输入模型\n用 "),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=X%3D[x_1%2Cx_2%2C...x_n]",alt:"[公式]",loading:"lazy"}}),t._v(" 表示输入权重向量")]),t._v(" "),a("p",[t._v("② 计算注意力分布 α：通过计算 Q 和 K 进行点积计算相关度，并通过 softmax 计算分数\n另 "),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=Q%3DK%3DV%3DX",alt:"[公式]",loading:"lazy"}}),t._v("，通过 softmax 计算注意力权重，"),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=α_i%3Dsoftmax(s(k_i%2Cq))%3Dsoftmax(s(x_i%2C+q))",alt:"[公式]",loading:"lazy"}})]),t._v(" "),a("p",[t._v("我们将 "),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=α_i",alt:"[公式]",loading:"lazy"}}),t._v(" 称之为注意力概率分布，"),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=s(x_i%2C+q)",alt:"[公式]",loading:"lazy"}}),t._v(" 为注意力打分机制，常见的有如下几种：\n加性模型："),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3Dv^Ttanh(Wx_i%2BUq)",alt:"[公式]",loading:"lazy"}}),t._v("\n点积模型："),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3Dx_i^Tq",alt:"[公式]",loading:"lazy"}}),t._v("\n缩放点积模型："),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3D{x_i^Tq}%2F\\sqrt{d_k}",alt:"[公式]",loading:"lazy"}}),t._v("\n双线性模型："),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3Dx_i^TWq",alt:"[公式]",loading:"lazy"}})]),t._v(" "),a("p",[t._v("③ 信息加权平均：注意力分布 "),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=α_i",alt:"[公式]",loading:"lazy"}}),t._v(" 来解释在上下文查询 "),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=q_i",alt:"[公式]",loading:"lazy"}}),t._v(" 时，第 "),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=i",alt:"[公式]",loading:"lazy"}}),t._v(" 个信息受关注程度。\n"),a("img",{attrs:{src:"https://www.zhihu.com/equation?tex=att(q%2CX)%3D\\sum_{i%3D1}^N{α_iX_i}",alt:"[公式]",loading:"lazy"}})])]),t._v(" "),a("h2",{attrs:{id:"attention的通用定义如下"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#attention的通用定义如下"}},[t._v("#")]),t._v(" Attention的通用定义如下：")]),t._v(" "),a("p",[t._v("给定一组向量集合values，以及查询向量query，我们根据query向量去计算values加权和，即成为attention机制。")]),t._v(" "),a("p",[t._v("attention的重点即为求这个集合values中每个value的权值。我们也称attention的机制叫做query的输出关注了（考虑到了）原文的不同部分。")]),t._v(" "),a("p",[t._v("如seq2seq模型中，St是后面的query向量，而编码过程的隐藏状态hi是values。其就是根据某些规则（或额外信息query）从向量表达集合values中抽取特定的向量进行加权组合的方法，只要从部分向量里用了加权和，计算使用了attention机制。")])])}),[],!1,null,null,null);s.default=e.exports}}]);
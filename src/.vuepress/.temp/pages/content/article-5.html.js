export const data = JSON.parse("{\"key\":\"v-4fe9850a\",\"path\":\"/content/article-5.html\",\"title\":\"The attention in transformer （面经问题总结）\",\"lang\":\"zh-CN\",\"frontmatter\":{\"summary\":\"The attention in transformer （面经问题总结） 在NLP算法相关的面试里，Transformer和其中的Attention机制显然是重中之重。而相关的高频提问笔者总结主要有如下几个问题形式。 1. 问：自注意力(self-attention)的计算公式是怎样的？ 答： $$\\\\text{Attention }(Q, K, V)=\\\\o\",\"head\":[[\"meta\",{\"property\":\"og:url\",\"content\":\"https://enze5088.github.io/content/article-5.html\"}],[\"meta\",{\"property\":\"og:site_name\",\"content\":\"炼丹拾遗\"}],[\"meta\",{\"property\":\"og:title\",\"content\":\"The attention in transformer （面经问题总结）\"}],[\"meta\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"meta\",{\"property\":\"og:locale\",\"content\":\"zh-CN\"}],[\"meta\",{\"property\":\"og:locale:alternate\",\"content\":\"en-US\"}],[\"link\",{\"rel\":\"alternate\",\"hreflang\":\"en-us\",\"href\":\"https://enze5088.github.io/en/content/article-5.html\"}]]},\"excerpt\":\"\",\"headers\":[{\"level\":3,\"title\":\"1. 问：自注意力(self-attention)的计算公式是怎样的？\",\"slug\":\"_1-问-自注意力-self-attention-的计算公式是怎样的\",\"link\":\"#_1-问-自注意力-self-attention-的计算公式是怎样的\",\"children\":[]},{\"level\":3,\"title\":\"2.问：Transformer里自注意力机制的计算过程是怎样的？\",\"slug\":\"_2-问-transformer里自注意力机制的计算过程是怎样的\",\"link\":\"#_2-问-transformer里自注意力机制的计算过程是怎样的\",\"children\":[]},{\"level\":3,\"title\":\"3.问：Multi-Head Attention是什么，有什么作用？\",\"slug\":\"_3-问-multi-head-attention是什么-有什么作用\",\"link\":\"#_3-问-multi-head-attention是什么-有什么作用\",\"children\":[]},{\"level\":3,\"title\":\"4.问：Transformer的中的Attention机制，其中Self-Attention和Encoder-DecoderAttention之间的关系？\",\"slug\":\"_4-问-transformer的中的attention机制-其中self-attention和encoder-decoderattention之间的关系\",\"link\":\"#_4-问-transformer的中的attention机制-其中self-attention和encoder-decoderattention之间的关系\",\"children\":[]},{\"level\":3,\"title\":\"5.问：都有哪些不同类型的注意力，其公式及计算过程和复杂度差异有什么区别。\",\"slug\":\"_5-问-都有哪些不同类型的注意力-其公式及计算过程和复杂度差异有什么区别。\",\"link\":\"#_5-问-都有哪些不同类型的注意力-其公式及计算过程和复杂度差异有什么区别。\",\"children\":[]},{\"level\":3,\"title\":\"6.问：self-attention代码实现？\",\"slug\":\"_6-问-self-attention代码实现\",\"link\":\"#_6-问-self-attention代码实现\",\"children\":[]},{\"level\":2,\"title\":\"参考\",\"slug\":\"参考\",\"link\":\"#参考\",\"children\":[]}],\"readingTime\":{\"minutes\":9.29,\"words\":2788},\"filePathRelative\":\"content/article-5.md\"}")

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}

<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://enze5088.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>算法吐槽菌</title>
    <link>https://enze5088.github.io/</link>
    <description>一个炼丹码农的自我修养</description>
    <language>zh-CN</language>
    <pubDate>Thu, 24 Feb 2022 08:16:39 GMT</pubDate>
    <lastBuildDate>Thu, 24 Feb 2022 08:16:39 GMT</lastBuildDate>
    <generator>@mr-hope/vuepress-plugin-feed</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <category>那些关于发财的白日梦</category>
    <category>科研与求索</category>
    <category>博文</category>
    <item>
      <title>About Me</title>
      <link>https://enze5088.github.io/AboutMe/</link>
      <guid>https://enze5088.github.io/AboutMe/</guid>
      <source url="https://enze5088.github.io/rss.xml">About Me</source>
      <pubDate>Sun, 18 Apr 2021 15:36:53 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="about-me"> About Me</h1>
<h3 id="hi-there-👋"> Hi there 👋</h3>
<p>I will be joining the WeiBo Ai Lab at Sina Group as a research engineer. I received my Master's Degree in June 2021 from the Artificial Intelligence Academy, the University of the Chinese Academy of Sciences.</p>
<p>I have been a student at the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences supervised by Prof. Xiaolong Zheng, focus on Natural language processing.</p>
<p>My research interests are in recommender systems, computational advertising, and information retrieval, and broader interests include comparative learning, Multi-modal Pre-training model, transfer learning, and their applications in advertising, search, and recommendation.</p>
<p>Coding is probably my favorite thing. I am looking for Ph.D. opportunities as well as research assistant/intern positions. Please drop me an <a href="mailto:pu.miao@foxmail.com">email</a> Research Interests</p>
<h2 id="experience-and-education"> Experience and Education</h2>
<h3 id="experience"> Experience</h3>
<ul>
<li><strong>Jul. 2021 – Present.</strong> Algorithm Expert, WeiBo Ai Lab @ <a href="https://www.sina.com.cn/" target="_blank" rel="noopener noreferrer">Sina Group</a>.</li>
</ul>
<h3 id="education"> Education</h3>
<ul>
<li><strong>Sep. 2018 – Jun. 2021.</strong> M.Eng. , Computer Science @  <a href="https://ai.ucas.ac.cn/index.php/zh-cn/" target="_blank" rel="noopener noreferrer">Artificial Intelligence Academy</a>，<a href="http://english.ia.cas.cn/" target="_blank" rel="noopener noreferrer">Institute of Automation, Chinese Academy of Sciences (CASIA)</a>.</li>
<li><strong>Sep. 2014 – Jun. 2018.</strong>  B.Eng. , Department of mechanical (mechanical Science and Engineering) @ <a href="https://www.wtu.edu.cn/" target="_blank" rel="noopener noreferrer">Wuhan Textile University (WTU)</a>.</li>
</ul>
<h2 id="publications"> Publications</h2>
<h2 id="misc"> Misc</h2>
]]></content:encoded>
    </item>
    <item>
      <title>主页</title>
      <link>https://enze5088.github.io/</link>
      <guid>https://enze5088.github.io/</guid>
      <source url="https://enze5088.github.io/rss.xml">主页</source>
      <pubDate>Sun, 18 Apr 2021 15:36:53 GMT</pubDate>
    </item>
    <item>
      <title>About Me</title>
      <link>https://enze5088.github.io/aboutme/README_EN/</link>
      <guid>https://enze5088.github.io/aboutme/README_EN/</guid>
      <source url="https://enze5088.github.io/rss.xml">About Me</source>
      <pubDate>Wed, 23 Jun 2021 13:18:25 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="about-me"> About Me</h1>
<p>Hi there 👋</p>
<p>I will be joining the WeiBo Ai Lab at Sina Group as a research engineer. I received my Master's Degree in June 2021 from the Artificial Intelligence Academy, the University of the Chinese Academy of Sciences.</p>
<p>I have been a student at the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences supervised by Prof. Xiaolong Zheng, focus on Natural language processing.</p>
<p>My research interests are in recommender systems, computational advertising, and information retrieval, and broader interests include comparative learning, Multi-modal Pre-training model, transfer learning, and their applications in advertising, search, and recommendation.</p>
<p>Coding is probably my favorite thing. I am looking for Ph.D. opportunities as well as research assistant/intern positions. Please drop me an <a href="mailto:pu.miao@foxmail.com">email</a> Research Interests</p>
<h1 id="experience-and-education"> Experience and Education</h1>
<h3 id="experience"> Experience</h3>
<ul>
<li><strong>Jul. 2021 – Present.</strong> Algorithm Expert, WeiBo Ai Lab @ <a href="https://www.sina.com.cn/" target="_blank" rel="noopener noreferrer">Sina Group</a>.</li>
</ul>
<h3 id="education"> Education</h3>
<ul>
<li><strong>Sep. 2018 – Jun. 2021.</strong> M.Eng. , Computer Science @  <a href="https://ai.ucas.ac.cn/index.php/zh-cn/" target="_blank" rel="noopener noreferrer">Artificial Intelligence Academy</a>，<a href="http://english.ia.cas.cn/" target="_blank" rel="noopener noreferrer">Institute of Automation, Chinese Academy of Sciences (CASIA)</a>.</li>
<li><strong>Sep. 2014 – Jun. 2018.</strong>  B.Eng. , Department of mechanical (mechanical Science and Engineering) @ <a href="https://www.wtu.edu.cn/" target="_blank" rel="noopener noreferrer">Wuhan Textile University (WTU)</a>.</li>
</ul>
<h1 id="publications"> Publications</h1>
<h1 id="misc"> Misc</h1>
]]></content:encoded>
    </item>
    <item>
      <title>杂谈</title>
      <link>https://enze5088.github.io/articles/</link>
      <guid>https://enze5088.github.io/articles/</guid>
      <source url="https://enze5088.github.io/rss.xml">杂谈</source>
      <pubDate>Mon, 10 May 2021 03:56:56 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="杂谈"> 杂谈</h1>
]]></content:encoded>
    </item>
    <item>
      <title>笔记</title>
      <link>https://enze5088.github.io/articles/article-1/</link>
      <guid>https://enze5088.github.io/articles/article-1/</guid>
      <source url="https://enze5088.github.io/rss.xml">笔记</source>
      <pubDate>Mon, 10 May 2021 03:56:56 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="笔记"> 笔记</h1>
<p>近期一些想要写的话题</p>
<h3 id="近期一些想要写的话题"> 近期一些想要写的话题</h3>
<p>1.人口与生育</p>
<p>2.教育与阶级再生产</p>
<p>3.李广</p>
<p>4.城市规模 陆铭</p>
]]></content:encoded>
    </item>
    <item>
      <title>临街租屋隔音简易改造</title>
      <link>https://enze5088.github.io/articles/article-2/</link>
      <guid>https://enze5088.github.io/articles/article-2/</guid>
      <source url="https://enze5088.github.io/rss.xml">临街租屋隔音简易改造</source>
      <pubDate>Mon, 10 May 2021 03:56:56 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="临街租屋隔音简易改造"> 临街租屋隔音简易改造</h1>
<h2 id="背景"> 背景</h2>
<p>话不多说，租了一间自如临街屋，装修价位还可以，但是忘了是临街的。晚上睡觉的时候时不时能听到来往的车辆声，打扰休息。又懒得换或者转租，本着自己动手丰衣足食的理念。打算搜下知乎自己改造。</p>
<h3 id="房屋整体情况"> 房屋整体情况</h3>
<p>原有情况，二楼临街房屋，双层窗户+较厚的窗帘，但是效果一般。窗户尺寸 待测量，床距离窗户距离 目测30cm。有一说一这户型设计也是鬼才，临街的屋子不放厨房之类的，非得放卧室。</p>
<p>改造成本总体预算500-1000。控制在五百以下最好。花太多了我不如换租。</p>
<h2 id="方案调研"> 方案调研</h2>
<p>本着科研狗的习惯，预先调研一下解决方案。</p>
<p>临街房的噪音的类型，</p>
<ul>
<li>交通噪声</li>
<li>社会生活噪声</li>
<li>工业噪声</li>
<li>建筑施工噪声</li>
</ul>
<p>除了工业噪声外，另外三种噪声比较常见，本人出租屋里以交通噪声为主，也有很多社会生活噪声。都是马路噪音，大车应该都是低频噪音。低楼层噪声值最高。峰值基本出现在 1~3 层。</p>
<blockquote>
<p>交通噪声主要来源于城市道路上运行的机动车辆以及路面 状况。 通常汽车噪声包括发动机噪声、排气噪声、进气噪声、轮胎 噪声和传动机构噪声以及鸣笛噪声这几个主要方面;其中发动机噪声、轮胎噪声、排气噪声和鸣笛噪声是主要的噪声源。 据有关 研究资料分析, 在距离道路 15 m处测量, 以 80 km/h的车速行驶的汽车产生的噪声达 50 dB～ 70 dB;繁忙的城市主干道噪声达 90 dB;距铁路、重型卡车 15 m处达 90 dB～ 100 dB;而汽车喇叭声 通常达 110 dB～ 120 dB。[4]</p>
</blockquote>
<p>晚上睡觉的时候确实主要是汽车行驶过的声音。但是怎么解决呢？</p>
<blockquote>
<ul>
<li>将临街的窗户改成隔音窗、即用两层窗户把声音隔开,每层窗户的缝隙都尽量堵严, 这种方法适用于 一 切有噪声干扰的地方。</li>
<li>墙壁不宜过于光 滑，墙壁过于光滑 , 任何声音都会产生回响。墙 壁可以考虑挂一些 图片 , 既可装饰又可吸噪声。</li>
<li>用木质家具吸收噪音，木质纤维具有多孔性 , 能吸收噪音。但家具不宜过多和过少 . 过多则由于拥挤容易东碰西撞 , 增加声响. 过少则可能使声音在室内共鸣回旋。</li>
</ul>
</blockquote>
<p>我目前的房子确实都是两层窗户，但是是不是隔音窗就两说了。但是堵严窗缝确实是一条思路。</p>
<p>临街的房子如何减弱马路噪音的影响？ - 知乎 https://www.zhihu.com/question/19727650</p>
<h2 id="解决方法"> 解决方法</h2>
<h3 id="窗户"> 窗户</h3>
<ol>
<li>
<p>隔声窗：</p>
<p>一个论文里的参考方案。</p>
<blockquote>
<p>改造外阳台窗， 用塑钢窗+双层玻璃； 二为改造阳台门带窗，保留塑钢门带窗，仅更换玻璃，将单层玻璃改为双层玻璃</p>
</blockquote>
<p>看起来还可以，要不是出租屋我就试一试了。好像大多数改造方案都是从窗户入手。中空夹胶玻璃，改降噪玻璃，改门框。</p>
<p>马路边租房隔音办法有吗？ - 林和的回答 - 知乎 https://www.zhihu.com/question/34019431/answer/1717417402</p>
</li>
<li>
<p>窗扇间的密封
淘宝买了密封横条。参考一下试试。</p>
</li>
<li>
<p>隔音窗帘
100元左右，感觉很靠谱。还没有送到，等买回来了试试。</p>
</li>
<li>
<p>增放家具
考虑入手一两个柔软的棉布或者木制家具。</p>
</li>
</ol>
<h3 id="卧室选材"> 卧室选材</h3>
<ol>
<li>
<p>地板
考虑到是出租屋，只放了垫子，地板啥的就不考虑了。但是好像有隔音地垫，可以参考一下。</p>
</li>
<li>
<p>墙</p>
<p>隔音板；隔音毛毡；墙体隔声构造设计；隔音棉。</p>
<p>不打算对出租屋的墙进行大改，但是感觉也有一些作用。在淘宝上看到了一个叫声博士的店家，如果窗帘和横条效果还不够给力的话，考虑在墙上加上隔音板，隔音棉之类的。</p>
</li>
<li>
<p>房门
房门缝底下打算改造一下。</p>
</li>
<li>
<p>天花板
pass，暂不考虑。</p>
</li>
</ol>
<h3 id="其他"> 其他</h3>
<ol>
<li>街道设计，建筑设计方面。
看了一些论文是直接从建筑设计上入手，出租屋就不操心这个了。</li>
<li>外墙挂声音反射板
这个看起来挺高端的，暂时不需要，但是可以先存着。</li>
<li>耳塞</li>
<li>降噪耳机
这两种方案本人都不考虑，戴着不舒服。</li>
<li>绿化带，种植灌木。
绿化带啥的咱也管不着，试试双层玻璃窗户间摆放盆景</li>
</ol>
<h2 id="方案实施"> 方案实施</h2>
<p>调研做的差不多了，准备接下来实施。</p>
<p>目前购买了隔音窗帘，地垫，以及封条。进一步打算看看能不能在窗户上加装隔音棉或者隔音垫。等到到了之后再记录进一步的进展。</p>
<h2 id="结果"> 结果</h2>
<p>其实最后效果一般，</p>
<h2 id="参考"> 参考</h2>
<ol>
<li>雷艳辉. 城市道路交通噪声对临街建筑的影响研究[J]. 收藏, 2018, 12.</li>
<li>顾向荣. 关于改善临街住宅环境的浅见[J]. 北京规划建设, 1997, 4.</li>
<li>赵敬源, 张琳, 霍小平. 临街建筑声环境的研究与改善[J]. 西安建筑科技大学学报 (自然科学版), 2004, 36(2): 50-52.</li>
<li>徐小燕. 城市住宅声环境污染与控制探讨[J]. 山西建筑, 2011, 3.</li>
<li>刘启龙. 临街建筑物降低交通噪声的几种措施效应分析[J]. 城市道桥与防洪, 1989, 3.</li>
<li>晋美俊, 刘晓华, 雷一彬. 临街住宅环境噪声实测分析与研究[J]. 建筑科学, 2016, 8.</li>
<li>宋莉, 李娟, 张荣. 学生宿舍窗改造隔声效果监测与分析[J]. 门窗, 2009 (11): 52-56.</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>Github进行fork后如何与原仓库同步</title>
      <link>https://enze5088.github.io/articles/article-3/</link>
      <guid>https://enze5088.github.io/articles/article-3/</guid>
      <source url="https://enze5088.github.io/rss.xml">Github进行fork后如何与原仓库同步</source>
      <pubDate>Thu, 24 Feb 2022 03:08:07 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="github进行fork后如何与原仓库同步"> Github进行fork后如何与原仓库同步</h1>
<p>实在是……有太多人同时在帮忙修订错别字或优化 xiaolai 的 <code>the-craft-of-selfteaching</code> 了。如果你提交的 pull request 未被接受且得到回复说：“重新fork”，其实是你遇到一个问题：</p>
<blockquote>
<ul>
<li>在你 fork 之后， xiaolai 的仓库又更新了；</li>
<li>但 github 不会自动帮你把 xiaolai 的仓库 同步给你 fork 后的仓库；</li>
<li>导致你提交 pull request 时的版本和 xiaolai 的版本不一致。</li>
</ul>
</blockquote>
<p>这个问题，用显得更“专业点”的说法，叫做：<code>Github进行fork后如何与原仓库同步</code>。那到底怎么做呢？</p>
<p>最省事的办法可能是：</p>
<blockquote>
<ul>
<li>在你fork的仓库setting页翻到最下方，然后delete这个仓库；</li>
<li>然后重新fork xiaolai 的仓库，并 git clone 到你的本地。</li>
</ul>
</blockquote>
<p>有时候，你需要用到这个省事的办法，比如 xiaolai 的仓库再次整理了 commit 。但在更多情况下，删掉自己fork的库，应该是你的最后选择，而不应该是首选。</p>
<p>和很多人一起向 xiaolai 提交 pull request，这实在是一个反复练习 <code>merge</code> （中文说法：合并，或版本合并）的机会。毫不夸张地讲，版本管理是软件工程极其重要的规范，也是极其基础的必备技能。而 <code>merge</code> 则是版本管理中最必须也最常用的场景。</p>
<p>那要不然，就多练练？以下是傻瓜版操作步骤，还细心配了截图，保管你从 0 也能上手。至于原理嘛，慢慢再搞懂吧。</p>
<h3 id="merge前的设定"> merge前的设定</h3>
<p>step 1、进入到本地仓库的目录。</p>
<p>下面所有操作，如无特别说明，都是在你的本地仓库的目录下操作。比如我的本地仓库为<code>/from-liujuanjuan-the-craft-of-selfteaching</code></p>
<p><a href="https://user-images.githubusercontent.com/31027645/54422899-6938e880-474a-11e9-8768-27ac24673e28.png" target="_blank" rel="noopener noreferrer"><img src="./article-3.assets/54422899-6938e880-474a-11e9-8768-27ac24673e28.png" alt="image" loading="lazy"></a></p>
<p>step 2、执行命令 <code>git remote -v</code> 查看你的远程仓库的路径：</p>
<p><a href="https://user-images.githubusercontent.com/31027645/54422975-95ed0000-474a-11e9-96bf-1018d6bc06f2.png" target="_blank" rel="noopener noreferrer"><img src="./article-3.assets/54422975-95ed0000-474a-11e9-96bf-1018d6bc06f2.png" alt="image" loading="lazy"></a></p>
<p>如果只有上面2行，说明你未设置 <code>upstream</code> （中文叫：上游代码库）。一般情况下，设置好一次 <code>upstream</code> 后就无需重复设置。</p>
<p>step 3、执行命令 <code>git remote add upstream https://github.com/selfteaching/the-craft-of-selfteaching.git</code> 把 xiaolai 的仓库设置为你的 <code>upstream</code> 。这个命令执行后，没有任何返回信息；所以再次执行命令 <code>git remote -v</code> 检查是否成功。</p>
<p><a href="https://user-images.githubusercontent.com/31027645/54423107-d8aed800-474a-11e9-9ab8-7bb901181283.png" target="_blank" rel="noopener noreferrer"><img src="./article-3.assets/54423107-d8aed800-474a-11e9-9ab8-7bb901181283.png" alt="image" loading="lazy"></a></p>
<p>step 4、执行命令 <code>git status</code> 检查本地是否有未提交的修改。如果有，则把你本地的有效修改，先从本地仓库推送到你的github仓库。最后再执行一次 <code>git status</code> 检查本地已无未提交的修改。</p>
<div><pre><code>git add -A` 或者 `git add filename`
`git commit -m &quot;your note&quot;`
`git push origin master`
`git status
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></div></div><p>注1：这一步作为新手，建议严格执行，是为了避免大量无效修改或文本冲突带来的更复杂局面。</p>
<p>注2：如果你已经在fork后的仓库提交了大量对 xiaolai 的仓库并没有价值的修改，那么想要pull request，还是重新回到本文最初的“最省事办法”吧。</p>
<h3 id="merge-的关键命令"> merge 的关键命令</h3>
<p>以下操作紧接着上面的步骤。</p>
<p>step 5、执行命令 <code>git fetch upstream</code> 抓取 xiaolai 原仓库的更新：</p>
<p><a href="https://user-images.githubusercontent.com/31027645/54448734-60b2d300-4787-11e9-9fdf-90fcc2e66052.png" target="_blank" rel="noopener noreferrer"><img src="./article-3.assets/54448734-60b2d300-4787-11e9-9fdf-90fcc2e66052.png" alt="image" loading="lazy"></a></p>
<p>step 6、执行命令 <code>git checkout master</code> 切换到 master 分支：</p>
<p><a href="https://user-images.githubusercontent.com/31027645/54448759-6dcfc200-4787-11e9-8bbc-a5beef23ea88.png" target="_blank" rel="noopener noreferrer"><img src="./article-3.assets/54448759-6dcfc200-4787-11e9-8bbc-a5beef23ea88.png" alt="image" loading="lazy"></a></p>
<p>step 7、执行命令 <code>git merge upstream/master</code> 合并远程的master分支：</p>
<p><a href="https://user-images.githubusercontent.com/31027645/54449526-47128b00-4789-11e9-9add-09217eb91a68.png" target="_blank" rel="noopener noreferrer"><img src="./article-3.assets/54449526-47128b00-4789-11e9-9add-09217eb91a68.png" alt="image" loading="lazy"></a></p>
<p>step 8、执行命令 <code>git push</code>把本地仓库向github仓库（你fork到自己名下的仓库）推送修改</p>
<p>如果担心自己不小心改了哪里，可以再次执行命令 <code>git status</code> 检查哪些文件有变化。这个操作仅是检查，不会改变任何状态，放心用。</p>
<p><a href="https://user-images.githubusercontent.com/31027645/54449665-a07aba00-4789-11e9-9181-bdcc814fffe6.png" target="_blank" rel="noopener noreferrer"><img src="./article-3.assets/54449665-a07aba00-4789-11e9-9181-bdcc814fffe6.png" alt="image" loading="lazy"></a></p>
<p>现在你已经解决了fork的仓库和原仓库版本不一致的问题。可以放心向 xiaolai 发起 pull request 了。如果以上操作你花了不少时间，而 xiaolai 的仓库 又恰好更新了。很好，一次新的练习机会来了……</p>
<h2 id="ps-如何优雅的获得上游最新内容"> PS: 如何优雅的获得上游最新内容?</h2>
<blockquote>
<p>... 必须重新弄一个当前最新版本到本地</p>
</blockquote>
<p>这是一个常见 github 操作, 但是, 其操作涉及复杂的 git 概念,
所以, 在此简要描述一下最小过程(MVP):</p>
<p>1: 首先, 新增 remote (远程上游仓库), 即, 将本书仓库, 追加为 fork 仓库的上游仓库</p>
<blockquote>
<p>git remote add upstream https://github.com/selfteaching/the-craft-of-selfteaching.git</p>
</blockquote>
<p>此时检验本地工作复本仓库的配置就可以看到变化:</p>
<div><pre><code>$  cat .git/config

...

[branch &quot;master&quot;]
	remote = origin
	merge = refs/heads/master
[remote &quot;upstream&quot;]
	url = https://github.com/selfteaching/the-craft-of-selfteaching.git
	fetch = +refs/heads/*:refs/remotes/upstream/*
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div><p>在底部追加了 upstream (上游)仓库信息, 可以用 branch 命令来检验</p>
<div><pre><code>$  git branch -a
* master
  remotes/origin/HEAD -&gt; origin/master
  remotes/origin/master
  remotes/upstream/master
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>2: 同步上游修订, 使用 fetch 命令</p>
<div><pre><code>$  git fetch upstream
remote: Enumerating objects: 2, done.
remote: Counting objects: 100% (2/2), done.
remote: Total 12 (delta 2), reused 2 (delta 2), pack-reused 10
Unpacking objects: 100% (12/12), done.
From https://github.com/selfteaching/the-craft-of-selfteaching
   7b3aeea..4646cc8  master     -&gt; upstream/master
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></div></div><p>然后, 在此基础上进行修订, 以及 Pull-Request 才是正义的.</p>
<p>简单的说:</p>
<ul>
<li>声明上游仓库</li>
<li>每次进行修订前先 fetch 上游修订</li>
<li>如果和本地有冲突, 先在本地解决冲突</li>
</ul>
<h2 id="pps"> PPS:</h2>
<blockquote>
<p>软件工程中协同方式原则</p>
</blockquote>
<p>在哪些情况下可以直接使用master branch来提交Pull Request：</p>
<div><pre><code>你只想为主项目贡献某一处代码，贡献完自己的repo就可以扔的那种。
你打算为主项目长期贡献代码，而且希望追随原项目的主线开发，不保留自己的特性。
你打算为主项目长期贡献代码，默认master branch追随原项目主线，把自己的特性放到别的branch中。
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>在哪种情况下应该使用主题branch来提交Pull Request：</p>
<div><pre><code>想用master branch完全来做自己的开发。在这种情形下：会从上游库合并更新，但是这些merge本身的commits显然不可能作为返还到上游库的Pull Request的一部分。
存在自己的（未被merge或者不想被merge到上游库的）commits。
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div><p>鉴于Git的分布式开发哲学，每一个库均可以看作是一个独立的项目，显然是后一种（为每一个新特性建立一个专门的主题branch来向主项目推送Pull Request）的贡献方式更可取。
解释完毕(｀・ω・´)</p>
<p>如果远程主仓库的历史 commits 被修改整理过，而自己在未同步远程主仓库的情况下有新的 PR 需要提交，可以有两个方案处理：</p>
<ol>
<li>fork 最新的远程主仓库，重新提交
1.1 备份自己的改动
1.2 删除自己的 fork 的仓库，重新 fork 远程主仓库
1.3 将最新改动的应用到新 fork 的仓库，然后提交 PR</li>
<li>强制同步远程主仓库的所有历史 commits，处理冲突，再次提交
1.1 备份自己的改动（可以选择用一个新的分支或者直接将问题复制到本地存储的其他文件目录）
1.2 添加远程主仓库为自己本地仓库的一个 upstream（如命名为 origin），使用以下命令强制同步
<code>git fetch origin &amp;&amp; git reset --hard origin/master &amp;&amp; git clean -f -d</code>
1.3 将自己的改动应用到强制同步过的分支，如果使用的其他分支备份，可以使用 rebase 命令合并过来，如果有冲突，处理冲突
1.4 检查自己的 commits 是否正确合理，无问题则可重新提交 PR</li>
</ol>
<p>从 <a href="https://www.jianshu.com/p/cf64c800da16" target="_blank" rel="noopener noreferrer">origin vs upstream 的说明</a> 这篇文章的解释，总结出合并的简要过程：</p>
<h3 id="版本一-fork-后未做任何改动"> 版本一：fork 后未做任何改动</h3>
<p>fork xiaolai 老师的 repository 后，直接 clone 到自己的电脑上，没有做任何改动，那么，
做完 liujuanjuan 的步骤后，提交 pull requests 的方法：</p>
<ol>
<li>创建一个自己玩耍、测试、修改用的分支，随便怎么折腾；</li>
<li>发现有什么想要修改 xiaolai 老师的原始内容后：
<ol>
<li>切换回到 master 分支；</li>
<li>使用命令 <code>git fetch upstream</code>，把 xiaolai 最新的内容进行更新。</li>
<li>如果自己想要修改的内容已经被别人修改了，恭喜，你迟了。【下面的步骤就不用做了】。</li>
</ol>
</li>
<li>在自己电脑上进行修改，提交到自己的 master 中。</li>
<li>在网页上，按照 xiaolai 老师介绍的方法提交 pull requests 就好了。</li>
</ol>
<h3 id="版本二-fork-后做了改动"> 版本二：fork 后做了改动</h3>
<ol>
<li>先把自己的改动 push 到自己的 repository 中。</li>
<li>从 版本一 中的 2.1 开始操作。（解释：这样应该会把自己做的修改覆盖掉，所以，两个选择：1）备份自己的修改内容先；2）建立一个 branch 进行保留。【注意】：如果使用 delete repository 原始方法，branch 的方法保留的备份也没有啦，虽然现在自己的修改内容可以一点价值都没有，消失掉就消失掉吧/微笑）</li>
</ol>
<h3 id="可能的意外情况"> 可能的意外情况：</h3>
<ol>
<li>发生了 conflict（冲突）。应该是操作慢了，xiaolai 的版本又更新了，那么重新操作版本一，也就是一个不断重复的<strong>循环操作</strong>。</li>
<li>无关紧要的变动，比如 Python 版本不一致，导致很多没用的变动。我是把自己电脑的原来的程序全部重新安装了一遍，遇到了超级多的问题，一度很爆炸，但是过来了。你也可以的。</li>
</ol>
<p>要用 branch 提交 pull requests，而不能用 master 分支进行，要不然一些不顺利的情况发生（比如没有采纳merge、误删文件等等）都无法进行 upstream 的同步。</p>
<p>为什么按照 liujuanjuan 和 xiaolai 的步骤全部做完之后，只有第一次是成功的、后面的提交就会出现问题，就是这个原因。</p>
<p>连接里面有个很关键的信息：</p>
<ol>
<li>无论被 merge 了还是 拒绝了，都可以单独对 branch进行操作，无论成功与否，delete 掉提交 PR 的 branch 是完全无害的；</li>
<li>且这样，master 的同步才不会出现问题。</li>
</ol>
<p>PS：为什么会知道这些？因为创建了两个帐号，互相进行各种可能的操作（upstream 删除了但是 origin 没有操作等等各种组合），捯饬了一个下午，一直在纠结所有分类情况里面的剩下的两种无法解决的：</p>
<p>upstream 删除内容，我的 master 怎么就 fetch 不过来？</p>
<p>我删除了的内容，如何用 upstream 的进行还原？</p>
<p>总结：全都是用分支 branch 进行隔离，这样，永远不会发生 delete 原始 fork。（除非不玩这个项目了）</p>
]]></content:encoded>
    </item>
    <item>
      <title>职场的11条建议</title>
      <link>https://enze5088.github.io/articles/notion/article-1/</link>
      <guid>https://enze5088.github.io/articles/notion/article-1/</guid>
      <source url="https://enze5088.github.io/rss.xml">职场的11条建议</source>
      <pubDate>Wed, 23 Jun 2021 13:18:25 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="职场的11条建议"> 职场的11条建议</h2>
<p>**1、迷茫、看不清的时候，不要试图只从同龄人获取答案。**找大你5岁以上的师兄师姐、长辈聊，这些人和你条件基本面相似，很可能走过你曾经的坑。如果身边没有，不妨直接去知乎付费咨询，花钱买别人的经验少走弯路是一件很赚的事情。看不清的事情要多做研究，少做决策，这里和投资中的道理一样，提高每一次决策的胜率。人生、职业是长跑，少犯错就已经超越了很多人。</p>
<p>**2、要听得进他人的意见，不要一意孤行。**当初想裸辞的时候，倔得谁都拦不住。回头看能够直言不讳，批评你，劝你三思、教你做大概率事情的都是为你的人生着想的人，突然对这些人非常感激。</p>
<p>**3、越是心态差，越不要封闭自己。**尽管当时已经储备了足够半年的现金流，但更多的是心态上的焦虑。长期一个人呆着，背着家里把工作辞了，缺乏好的面试体验，得不到正向反馈，意志会日渐消沉。想通后我放平了心态，偶尔和家里打电话唠唠家常，找朋友唠嗑，找转行的朋友获取经验和信心，因为住在前司附近还常和前同事一起约饭。在参加pointer之夜前也曾想过没有工作跑去会不会很丢脸，后来放下包袱，本着学习放松的姿态反而收获满满。</p>
<p>**4、对自己诚实。其实我对数据分析也谈不上多么喜欢，但是对于投资是真爱。**之所以选数据分析其实也是了解自己做传统金融毫无优势，是对自己的一种自救行为。从地产投资跳出来是对老东家发家史的挖掘，对平台的担心等原因。白天忙着投简历、准备面试，但晚上睡前读《价值》心潮澎湃，带着满足与充实入睡。</p>
<p>**5、不要依附于你的领导。**也曾想过跟着领导多干几年，团队氛围也尚可；但意识到核心问题是体制和平台，长期待下去消磨激情。所以放弃了押宝在领导身上的想法，毅然离开。最近得到消息，原领导这几天已离职。所以，职场上唯有修炼自己，提升技能才是核心。但碰到一个好的领导、团队，短期多呆一下，走稳妥一点比冒进主义更适用于大多数人。</p>
<p>**6、差异化竞争。**坦白说，我之所以想做数据分析，确实有羡慕互联网行业高收入的因素。但了解投递一圈之后发现，依然低端、基础的岗位居多。所以在这种结构性机会的情况下，是选择A.在原有行业用数据分析的技能作为你的亮点？还是B.把跳脱出来硬转和科班出身的学生比硬技能和软技能？如果让我重新选的话，我一定会选A，掌握足够多的技能后再渐进的投互联网的数据分析。同时，在辞掉工作以后，我才意识到很多数据分析的方法和技能可以改造自己的工作，能给工作带来更多亮点。如果综合考虑年龄、工作年限、城市等因素，选A可能更适用于大多数人。</p>
<p>**7、人多的地方不要去。**任何一个行业都会遇到周期性。此前，我一直在抽象的想问题，比方说因为房地产行业融资收紧等原因，自己一直在看空行业。但实际上经济的发展就是这样，经济增长必然放缓，财富分化，老百姓的需求不足是必然。大环境的下行，实际上就是绝大多数行业下行的加总。因此，要抓住行业、岗位的风口本就是一件很难的事情，学会沉淀、前瞻性的思考，机会来临的时候你才能牢牢抓住。</p>
<p>**8、克服又肝又躺的想法。**择业的时候很多人想的是“钱多事少离家近”，但到答题的时候一拍大腿，这tm是道单选题啊。别担心还有个D选项：以上均不符合。所以肝的时候不要想着躺，躺着的时候不要想着肝，多拧巴呀（此句引用某大V）。</p>
<p><strong>9、毕业前在职业方向上做加法，毕业后在选择上做减法，在执行力上做加法。<strong>毕业前做加法的意义在于多尝试，找到喜欢的方向；毕业后做减法的意义在于人的时间精力等都有限。正如梁启超所言，</strong>“书宜读杂，业宜精专”</strong>。</p>
<p>**10、认清自己的能力边界。**我知道自己从小就不是一个特别聪明的小孩，经常后知后觉，甚至有点钝、有点木。但所幸一直喜欢读书，喜欢思考。高考前两天，别人都在复习，我在阅览室看《萌芽》、《人民文学》，感到前所未有的满足；大一大二，常泡在图书馆，看遍古龙的武侠小说，看完余华、路遥的小说，因2015年入股市埋下对金融的兴趣；大三下异想天开，决定从机械跨考金融；随后摸爬滚打，得偿所愿。人生有三重境界：你爸是个普通人，你是个普通人，你孩子是个普通人。虽然认清并接受自己有点难，但接受以后幸福感会很强。认清能力边界的意义在于不苛求远在能力圈之外的东西，而在于做一事儿，成一事儿。</p>
<p><strong>11、向上依然是职场、人生的主旋律。</strong>“我们登上并非我们选择的舞台，演出并非我们选择的剧本”。别人的剧本你羡慕不来，唯有的是演好自己的剧本。虽然“丧”、“躺平”充斥网络，但每一代人其实都活得并不容易。多读史，例如《明朝那些事儿》，把自己放到更大的周期里，就容易摆正自己的心态了。市场上永远都会有结构性的机会，我们能做的就是蛰伏、等待，保持耐心、保持平常心。无论贫穷富贵，歌照唱、舞照跳，保持对生活的热爱，剩下的就是等风来，当机会来临的时候，牢牢抓住。风消逝的时候，降低预期，继续学习、继续</p>
]]></content:encoded>
    </item>
    <item>
      <title>关于向上管理的一些讨论</title>
      <link>https://enze5088.github.io/articles/notion/article-2/</link>
      <guid>https://enze5088.github.io/articles/notion/article-2/</guid>
      <source url="https://enze5088.github.io/rss.xml">关于向上管理的一些讨论</source>
      <category>那些关于发财的白日梦</category>
      <pubDate>Thu, 03 Jun 2021 09:04:18 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="我30岁当上高管的秘诀-可能是向上管理做得好"> 我30岁当上高管的秘诀：可能是向上管理做得好</h1>
<blockquote>
<p>摘抄自公众号  空白女侠</p>
</blockquote>
<p>我步入职场8年，目前是一家私募基金的投资副总监，很多人感叹于我的职业晋升如此之快，在30岁的年纪就已经进入管理层。但其实早在我26岁的时候，我就作为商业分析部门负责人，开始管理一个数据团队了。那有小伙伴可能会问，是不是我工作以及学习能力特别突出，所以即便跨越领域，还能担任管理者？</p>
<p>其实不然，我个人觉得自己是曾国藩那种喜欢”打呆仗“的人，学习能力还行，工作能力过得去，再加上了点运气就到现在这样的状态。</p>
<p>不过值得我骄傲的 是，我与我的每一任领导，都能相处得很好，直到现在都和他们每个人都保持着联系，他们也都愿意在我遇到瓶颈或者困境的关键时刻帮我破局。而且在职场中我从来不主动提加薪升职，基本都是我的领导帮我争取。确切的说不是我不主动，而是我会引导领导，帮我达到目标，只是我不会开这个口。我自认为我最厉害的职场竞争力之一就是向上管理能力。</p>
<p>其实向上管理是一个颇受争议的词，但我个人认为自己的职业生涯能如此顺利，向上管理能力占了至少50%的功劳，运气占了30%，剩下的20%才是由于工作能力。大家可以观察那些从一线员工一步一步走上高管位置的人，绝对深谙向上管理之道。原因是你做任何事，任何重要的项目无一例外都需要上头老板的支持，越是级别高的人，越是懂得这个道理。</p>
<p>此次我把我这8年向上管理的心得，分享给大家。文章有点长，需要大家好好体会，或许这篇文章会助你开启一段全新的职场旅程。</p>
<p>对每一位职场人士而言，领导都是对其职业生涯发展影响最大的人。遇到一名好的领导可能会极大的助力职业发展；如果遇到一名不好的领导，可能会对职业发展产生极大的阻塞。在现实中，我们常常会发现职场上“混的好”、”走上高层“的人并不一定都是能力超群的人。但他们一定通晓与领导的相处之道，懂得如何影响自己的领导，为自己创造最好的条件。这种影响自己领导的过程，我们称之为向上管理。</p>
<p>有些不懂向上管理的小伙伴，看到这里往往会有点愤愤不平。大家会觉得一个人能力一般，只是因为和领导关系好就能顺风顺水，是拍马屁，是走后门。我们大可不必这么想，向上管理其实也是工作能力的一部分。而且向上管理并不等同于溜须拍马或阿谀奉承，其本质是在于读懂领导的想法并能够施加影响。本文就和大家一起深入探讨职场中的向上管理，希望能够对大家有些帮助。</p>
<p><strong>01</strong></p>
<p><strong>向上管理的目的和本质</strong></p>
<p>首先对“管理”这个词，我的认知是“通过他人拿结果”。在任何一个公司里，组织通过自上而下的打造团队，基于一层层的“管理”传导整个公司的目标和战略并拿到结果。换而言之，真正实施“管理”的应该是一个团队的领导，其目的就是为了通过团队拿到结果。所以“管理“的发起方应该是领导，下属和团队都是领导拿结果的手段和载体。然而，向上管理是通过“管理”你的领导来拿结果。</p>
<p>拿什么结果呢？无非是让自己在职场里发展的更好，过的更舒服，升职加薪更快一些，能够达成自己本身的职业发展的诉求等等。从出发点来说，这是一个利己但不一定利于组织的事情，是与任何一家公司的初衷和目标逆向而行的。要说明这点我可以举两个例子：</p>
<p>第一个例子：和珅对乾隆的向上管理。事实上我认为历史上和珅对乾隆的“向上管理”就做的很好。乾隆在位期间和珅几乎就是一人之下万人之上，呼风唤雨俨然已成为皇权的代言人。但和珅做向上管理的目的并不是为了国家富强、人民富足，他的目的是为了自己发财，为了满足自己的权力欲。</p>
<p>第二个例子：我们都耳熟能详的抗战剧《亮剑》主人公李云龙，他虽然是主人公，但大家都应该知道李云龙也是有上司的。在《亮剑》里，有一个非常著名的片段：李云龙为了她新婚的妻子去攻打日军占领的县城，调动了县城周边的很多友军，最终成功的攻下了县城。虽然最终效果很好，但李云龙攻打县城的目的并不是为了抗日救国，而是为了救他妻子。所以即便结果是好的，但目的并不纯。</p>
<p>正是由于向上管理的出发点与组织初衷相违背，这就让向上管理在任何一家企业里都是可做而不可说的事情。之所以说“可做”，是因为客观来说，作为一名有职业发展诉求的个体，我们都希望自己的职业发展顺利，所以在日常的工作中做一些向上管理，只要不过分，我觉得是可以理解的。</p>
<p>而之所以“不可说”，是因为这是与组织相冲突的，是组织所不能容忍的。一旦让别人发觉，很有可能会断送自己在组织里的前程。这点切记、切记、切记！</p>
<p>了解了向上管理的本质、目的以及注意事项，下面我们就聊聊想要做到向上管理，需要什么样的能力。</p>
<p><strong>02</strong></p>
<p><strong>向上管理所需要的能力</strong></p>
<p>向上管理不是随便谁都能做的，真正要“管理”领导，那就必然有过人之处。这里涉及的点很多，我挑选四点重点说一下：</p>
<p><strong>1. 具备超脱“工具人”的能力</strong></p>
<p>这个能力非常重要，我一定要把它放在第一点来讲。因为领导组建团队的目的是为他自己拿结果，所以在领导眼里团队以及团队里面的每个人都是为他拿结果的“工具人”。“工具人”这个词说出来可能不太好听，但却很客观。大家不妨想想看，如果说你自己要组建一个团队，要通过这个团队拿结果，比如去打仗。你去招了100个人组了个连队，你是连长，你要通过这100个人去攻城。那对于你来讲，一定不会在内心里面把每一个人都当人，否则你就是压榨别人，把别人推出去送死，内心太煎熬。所以很多上级看来，下级就是“工具人”。</p>
<p>我们都听过人管工具的，但有听过工具管人的吗？所以如果你想要反过来去影响你的领导，你必须要让他感觉到你是可以跟他在某种程度上平等的，是不可或缺的。</p>
<p>比如上面我们举例的和珅，史书记载是一名才富五车，满腹经纶的学子，乾隆引经据典时他都能对的上来。所以在学识上面和珅与乾隆是对等的，而绝对不是一个不学无术只会拍马屁的人。</p>
<p>再说李云龙。李云龙在亮剑里面就是个惹祸精，不听从上级指令，什么乱七八糟的事都干，这种行为放在军队里面是要被枪毙的。那为什么李云龙能够一而再再而三地犯错而且还不会被枪毙？因为李云龙带兵打仗确实有一套。对于领导来说，这是一个不可或缺的将才。在某种程度来讲，领导就需要依赖于他。这个时候，在他的眼里李云龙就一定不是一个可以随便牺牲的大头兵了，而是一个可与自己平起平坐的人。</p>
<p>之前曾经有位小伙伴问过我一个问题：TA希望领导能够为自己加薪，但领导总是不认可，希望我教教TA如何通过向上管理来做到。我仔细了解了一下，觉得没戏。为什么呢？因为虽然这个小伙伴平时工作很辛苦，天天加班加点，但做的事情可替代性太强了。所以在领导眼里TA就是一个工具人，领导觉得流失就流失了，大不了再招一个就是了。</p>
<p>在这种情况之下，是没有办法通过向上管理达成自己目的的。所以建立自己的竞争力，具备超脱“工具人”的能力，把自己变成一个和领导可以在某些方面平起平坐的人，是我们做向上管理的一个必要前提。</p>
<p><strong>2. 需要非常了解自己的领导</strong></p>
<p>我们了解领导的目的是为了选择合适的沟通方式和方法，古人云“伴君如伴虎”，虽然现代社会不会动不动惹上杀身之祸，但与领导相处稍有不慎就容易惹火上身。在史书里面，我们经常看到一些恃才傲物的人，很有脾气。这种人往往下场都比较惨，得不到善终。为什么呢？不是因为他们在某些方面和领导之间不能平起平坐，相反，他们其实在某些方面非常优秀，但是他们就是不注重自己的沟通方式和方法，惹恼甚至威胁到领导。</p>
<p>这个在历史上最典型的例子就是三国时的杨修。杨修其实是一个很有才的人。曹操想到的一些东西，他都能想到，那就意味着他在认知和思考上是跟曹操在一个等级上的。但杨修为什么最后就不得善终呢？是因为他不了解自己的领导。曹操是一个不世枭雄，是一个非常疑心的人。这个时候，杨修还处处在各种不重要的场合去卖弄自己那点小心思，让曹操觉得自己被看的很透，这样的人曹操不杀他杀谁呢？</p>
<p>在和领导相处时，还需要特别了解领导的个性。比如说你的领导是一个性格耿直，做人很正直的领导，那你跟他沟通的时候，就绝对不能撒谎。因为这种人他本身非常正直，如果知道你有一件事上撒了谎，他可能以后就不再相信你了；</p>
<p>如果说领导本身是一个慢性子，这个时候，你不管发生了多么紧急多么急迫的事情都不能慌张，因为这跟他的气场和节奏完全不一致的。反过来，如果说领导性子很急，那么在发生了紧急的事情时，你就必须得很快速的表达，不能慢悠悠的。</p>
<p>我给大家举的例子，其实都是一些比较通俗的例子，但是大家是需要先了解自己领导的性格，只有真正了解，才能时刻和领导保持在一个节奏上。</p>
<p>还曾有小伙伴提出一个非常有意思的问题。这个小伙伴说他跟他的领导原来是朋友，然后领导升上去之后，这名小伙伴反馈说拿捏不准和领导之间的关系。有的时候聊一些非工作的事情，这名小伙伴还希望大家能够像朋友那样聊天，但感觉回不去了。</p>
<p>这个事儿跟大家说一个我个人的例子：以前我在一家外企工作，外企相对来讲比较没有官架子，大家彼此之间直呼名字不叫title。当时我的老板是香港人，老板的老板是一个英国人。英国老板有一次正好来中国拜访客户，英国老板非常和蔼，喜欢开玩笑。聊天的时候我们都很放松，唯独这个香港老板全程都很谨慎。聊完后英国老板走了，只剩我和香港老板一块出去喝酒。当时我就跟香港老板说感觉与个英国老板汇报时不用太严肃。这时香港老板给我讲一句话：&quot;你以为你能叫他的名字，他就不是你老板了？”</p>
<p>这个例子其实是想告诉大家，职场上上下级关系是永远都不能忘记的，即便是在日常生活里也是如此。回到刚才那个问题，如果你的朋友变成是你的领导，除非你换工作，否则真的就失去一个朋友了。</p>
<p>正是因为如此，我个人不喜欢公司团建，看似是大家平等的出去玩，但事实上我觉得就是在义务加班。因为无论如何，我在领导面前都是放不开的。</p>
<p><strong>3. 重视对信息渠道的管理</strong></p>
<p>信息渠道分成两部分讲，一部分是对领导了解你所管理范围内信息渠道的管理。举个例子，曾经有Pointer反馈了自己的下级喜欢越级上报。这件事从本质上来讲，就是一个信息渠道管理不到位的表现。也就是说，对你负责的事情，你的领导获取信息的来源应该是在你的管理控制范围内的，他不应该能够通过你所不掌控的渠道获取到信息；另一部分管理的是你能够触达到领导的渠道，比如说我想要把信息告诉领导时有哪些渠道？自己得很清楚每个渠道传到信息的效率如何。我们还是拿和珅和李云龙的例子来讲。</p>
<p>我先说和珅和乾隆。大家看电视会发现，很多时候乾隆是被和珅蒙在鼓里的，就是他所能知道的其实是和珅是想让他知道的。事实上，只有在这种情况之下和珅才可能管理好领导。假如乾隆能够绕过和珅控制的渠道获取到真实的信息，那和珅想管理好乾隆几乎就是不可能的；第二个就是李云龙打县城是对信息渠道控制的就很紧。大家想一想李云龙打县城是为了抢老婆，假如他的上级领导在他打县城之前就知道他要去抢老婆，打县城这件事能达成吗？虽然最终结果是皆大欢喜，但是事实上在做这个决策的时候，李云龙是不能让领导知道的。</p>
<p>所以信息渠道的管理是非常重要的。要想做到完全把控，有几点需要做到：</p>
<p>**一， 树立自己的的威信。**比如我们不能阻止领导只通过自己作为唯一的渠道来获取信息，毕竟领导也不傻。但是他所能接触到的所有渠道都必须是能够把控到的。如果不是怎么办？这里我并不准备讲职场厚黑学或者说黑暗管理之类的方法，但不管怎么说，你都要把周围的人管好。不管是绩效考核，亦或是平时的一些工作分配，都需要去让他人有意无意之间能够感觉到不能做越级汇报这样的事。</p>
<p>当然有时候领导会越级找到下属沟通，这是不能避免的。但下属聊完后一定需要第一时间汇报给你。把聊的内容一五一十说清楚，这也算是对信息渠道有很好的把控，是需要向下进行传达的。</p>
<p>**二，能够判断事情孰轻孰重。**在管理好信息通道后，我们更需要能够判断什么信息需要及时汇报，什么事情可以晚点再说。因为关键信息需要及时上报，才能建立领导对我们的信任。</p>
<p>举个例子，比如说现在生产上出了一个事故，死人了。这个时候，你就不要想的是管理好信息渠道，把这事能够隐瞒下去，这是不可能的，这个时候你应该第一时间去上报给你的领导。</p>
<p>再举个例子，假如这个生产上机器出了事故，没有人员伤亡。但是可能延误了进度，本来今天应该生产100个零件的，结果只生产50个。这个时候，你要评估一下，比如这个机器修好之后是不是可以通过加班的方式把这另外的50个补回来。如果能补回来，就可以先不上报。为什么可以先不上报呢？因为即便你先上报了，领导对你的要求也一样是加班把50个完成了。毕竟不能因为事故影响产量对吧？领导一定是会压榨你的。</p>
<p>这种情况之下，你上报给领导除了给你和团队增加压力之外，没有任何的好处。这个时候，不如你就自己把事做了，做完之后再让领导知道一下。这样领导就会对你很放心，同时也给自己留下了处理问题的空间。如果该知道的，领导第一时间能知道，而那些不需要第一时间知道的，你事后也会通知到领导。那对领导来讲，当然是非常踏实和放心的。领导也就不会老想通过其他渠道来获取信息了。</p>
<p><strong>三， 需要对所管理范围内的所有信息都了如指掌</strong>。比如说，你是管理果园的，管了100棵树。然后领导到果园来视察的时候，他可能问一些问题，你要像如数家珍一样回答清楚。如果你能够把地盘上所有的信息都了解得清楚，领导只需要问你就能拿到结果，他就不需要花费时间再去向下挖一层去了解信息了。相反，如果你问题回答不清，那领导就一定要找他人去了解信息，这个时候，你就是在逼领导拓宽信息渠道了。</p>
<p><strong>4.构建自己的影响力</strong></p>
<p>影响力顾名思义就是影响他人的能力。我给大家举个例子体会一下。以前高考的时候信息不像现在这么通畅，很多人高考完不知道该报什么学校和志愿。于是都会找人去咨询，而咨询的对象往往都是当地上过大学的知识分子。这个时候，事实上，这个有建议权的知识分子某种程度上就影响了很多人未来一生的命运和职业，这就是影响力。</p>
<p>影响力为什么对于职场中做向上管理很重要呢？再给大家举个例子：我们现在是在一个商务部门，商务部门的主要的职责是做市场的推广。市场推广要很多部门一同协作，有财务有技术有生产，各个方面一起去协作。如果说商务部门的一个员工A能够在公司的范围内对财务、生产、技术有很强的影响力，那么事实上A是可以通过协同部门来影响商务部门决策的。这个时候商务部门的领导就不得不听A的，这就是典型的叫下克上。领导做出每一个决策，都要下面团队去执行。如果这帮人在A的影响之下变成了一个团体了，那这个时候领导事实上就被A向上管理了。因为领导提出的想法，一旦A觉得不可以实行就能说服下面人都不同意，结果领导问谁都是不可以，那用什么方案呢？用A说的方案。这个事其实是典型的就是下克上。所以很多时候，大家要知道建立自己的影响力是非常重要的。</p>
<p>影响力怎么去构建？影响力构建的点很多，我只能简单给大家举几点，这里可能没办法把它阐述得很清楚。</p>
<p>**第一点，影响力的构建是你需要带着大家去不断地取得成功。**这个怎么来理解？就是在任何一个团队或组织里面如果说你要对他人造成影响，你是要不断的证明自己，让大家觉得跟着我有肉吃。如果一个人过去一直是对的，那么他未来是不是也应该有可能是对的？如果一个人过去不断地证明自己是对的，那未来大家大概率会听他的。这就是第一点，你要能证明自己不断是对的。</p>
<p>**第二点，取得成功之后不能吃独食。要能够把取得的成果分给大家。**有些人带着一帮人或者大家一起把一件事拿下来之后，最喜欢吃独食。其实这是一个最愚蠢的事情，你吃一次独食，以后就没有人陪你玩了，因为知道陪你玩没结果。反过头来，如果你每一次都能大方地，把大量的成绩都分给大家，那大家当然愿意给你干了，换成是我们自己，如果有这样的领导，有这样的兄弟，每次都能成事，成事之后，我拿好处比他拿的还多，那我当然要给他干，在某种程度来讲，它是用利益换取了影响力。</p>
<p>**第三点， 要对责任能够背锅，愿意去背锅。**比如说，你带着一帮兄弟去做一件事儿，这个事儿我们希望是一直能成，但是有可能也会失败，不要去怨别人，把这个责任全揽到自己身上，那大家也都很清楚，如果说是别人责任，你也揽到身上，下一次，你还站出来带兄弟们去干活的时候，大家都觉得事情成了我有肉吃，出了问题他还能帮我担一担，那为什么不跟着他干？</p>
<p>这一部分的内容比较多，我为大家再总结一下：</p>
<p>第一：自己一定要有突出的不可替代的能力，让你的领导能够平等地对待你，不要把你当成“工具人”，这是能做向上管理的前提；</p>
<p>第二：你需要对自己的领导很了解，掌握好合适的沟通方式和方法；</p>
<p>第三：管理好你和领导之间的信息的渠道，做好对信息的全面掌控；</p>
<p>第四：你需要尝试在组织内构建自己的影响力。这就是向上管理的这样的四个关键点。</p>
<p><strong>03</strong></p>
<p><strong>建立对向上管理的正确态度</strong></p>
<p>这一部分我们要讲一下职场中的正道。向上管理固然能够助力我们的职业发展，但毕竟有点“邪气”。要想用好向上管理，就一定要理解职场中的正道。因此我还是需要多说几句，避免大家过犹不及。在这里简单的提三点：</p>
<p>**第一点，一定要分清楚做一件事情的过程和结果。**过程和结果怎么来理解？我经常喜欢说的一句话就是：结果是交给领导的，过程是留给自己的。比如说，现在领导安排我做些事儿，有可能成功也有可能失败。但不管是成功还是失败，结果是交给我的领导的。成功是结果，失败也是结果。但是做这件事情过程中的成长是留给我自己的。</p>
<p>我为什么会特别讲这一点，因为大家经常会举了一些例子，比如说我干活很卖力，我很努力做了很多工作，但是感觉我这个最终评价就不如那种溜须拍马的人评价好。但是大家回过头来想一想，如果一个公司只要溜须拍马的人，这公司能发展得好吗？而且假如真有那种搞这种裙带关系上来的领导，那你可以换工作的，离开很容易。所以你在工作的过程中，更应该关注到你个人有没有成长，这对你整个的职业发展是不是有益。大家即便当前在做一些很苦的工作，或者说觉得自己没有得到相应的回报，我觉得也不用太过气馁，而是应该关注自己在这个工作中有没有成长？只要有成长都可以忍辱负重做下去，等到时机一到，是晋升还是跳槽，这都不是什么大问题。</p>
<p><strong>第二点， 不要做职场中有人身依附关系的下属</strong>。这句话怎么理解呢？我自己见到过一些领导和下属是绑定的，比如领导升职下属就升职，领导走人下属就走人，像连体婴一样。相当于这个下属就跟着这个领导混了。人身依附关系是我选择了一个非常中性的词来描述这种关系。其实人身依附关系本质上就是奴隶主与奴隶之间的关系。我在职场中见过那些和领导绑定的下属，往往被领导骂成孙子还得点头哈腰，为啥？因为离开领导就混不了。我还是建议大家专注于自己的能力，靠真本事和领导建立合作关系。干得顺心就干，干得不顺心就换一个地方发展，这样才对自己最有好处。</p>
<p>**第三点，我个人建议大家除了专注于自己职业技能的发展外，也需要去了解人情世故上。**人情世故不是溜须拍马，或者说溜须拍马只是人情世故的过度表达。毕竟咱们中国人还是一个讲究人情世故的民族，在和周边人相处时，人情世故是非常好的润滑剂。我为什么把这种人情世故放在职场中的正道这一块讲，就是希望大家在职场中表现出一定的情商，一定的人情世故，其实是对你的职场长期发展是一个重要加分，而且是一个正常，合理的一个表现。大家既不应该去溜须拍马，也不应该曲高和寡，两种极端都要避免。</p>
<p>以上就是我对向上管理的理解，希望能够给大家一点启发^ ^</p>
]]></content:encoded>
    </item>
    <item>
      <title>高瓴资本-李岳 谈 投资方法论</title>
      <link>https://enze5088.github.io/articles/notion/article-3/</link>
      <guid>https://enze5088.github.io/articles/notion/article-3/</guid>
      <source url="https://enze5088.github.io/rss.xml">高瓴资本-李岳 谈 投资方法论</source>
      <category>那些关于发财的白日梦</category>
      <pubDate>Thu, 03 Jun 2021 09:04:18 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="高瓴资本李岳-谈-投资方法论"> 高瓴资本李岳 谈 投资方法论</h1>
<p>自我介绍一下，我叫李岳，加入高瓴9年时间了。我之前在公募基金，专门从事消费方式和消费行业的研究和投资。大家对高瓴也有很多了解了，我不再做一些赘述了。关于高瓴的特点，我这么多年很有体会。为什么高瓴能做得这么好？管这么大的规模？我们整个二级市场的团队只有十几个人，而且七八个是比较年轻的新人。能保持这么高的业绩和管理规模，我觉得主要有三个方面。</p>
<h2 id="第一部分-高瓴的投资模式"> 第一部分：高瓴的投资模式</h2>
<p><strong>第一个确实我们是比较深度的</strong>，价值投资里面有很多的门派，有Deep Value，所谓的格雷厄姆型的捡烟蒂型的投资人；也有只看增长的，比如说一些餐饮店1家变2家变4家，这个叫增长型的投资人。而我们对自己的定义叫 Franchise Invest（直译：特许经营权投资），本质是看供给端的，我们会花百分之七八十的时间去研究供给，研究所谓的门槛，去寻找好的生意。</p>
<p><strong>因为我们走的是纵深模式</strong>，跟A股传统的研究员的成长路径会不太一样。在传统的模式下，研究员做了两三年研究之后，可能做个基金经理助理，之后就是管两三个行业的一个小产品，再管大产品。这种模式当然它有它的好处，大家的市场敏感度比较高。但整个A股市场，能坚持把一个行业或者一些领域去做深的人是比较少的。</p>
<p><strong>第二个我觉得是我们确实比较长期</strong>，我们有自己的一套方法论，能够很快得做很多的判断。做出判断以后，我们会把时间只聚焦在两三个领域里面，大家也知道我们公司就做三个领域：<strong>消费互联网加医药</strong>。每个行业里面我们也就投三五家公司，所以在这种情况下，我们能够跟企业家产生非常深的一些关系，也能跟企业去做很深的沟通和成长的一些帮助。</p>
<p><strong>我们内部的人才架构上也是比较长期的</strong>，大家在年轻人身上花的时间非常长，跟短期的业绩挂钩的KPI不是特别多。比如我们评判的一个标准是分享的文化：你是不是真的展现出来长期的这种对公司的价值。当然像我和行业的负责人，就要对业绩负责了。</p>
<p>**第三个高瓴一直对自己的创新要求比较高。**我们最早是做二级市场的，成名在一级市场，但实际上我们从13、14年才开始做，才五六年的时间，很快做到全国第一了。16 、17年我们开始做控股，从百丽、GLP这些案例为人所知；最近大家也看到了新闻：我们开始做真正的Venture，所以我们并不局限于一种方式的投资。</p>
<p>全阶段全时空的投资会产生非常大的威力，二级市场投资这些企业的竞争对手，其实都是在一级市场发生的。天天这个市场里都有人要把阿里挑战下去，把腾讯挑战下去。所以对这个东西要有全时空的理解，否则你很难去对很多的变化有深刻的认识。因为我们是全球的研究和投资，所以有很多时候要跨地域。跨时空、跨地域去理解很多事情，这是我们比较有特点的一些方面。</p>
<h2 id="第二部分-高瓴的投资方法论"> 第二部分：高瓴的投资方法论</h2>
<p>第二部分我重点跟大家分享一下我们的投资方法，就是我们经常说的投资里面的四重维度，当然这是基于纯粹基本面的价值投资。</p>
<p><strong>第一个维度叫信息点的收集</strong>，比如说跟公司、行业相关的有100个点，我们要把这100个点做一个收集。但这个东西其实是跟我们能不能做好投资这件事情，没有太大关系，但这是一个基非常强的基础。</p>
<p><strong>第二个我们叫做变化</strong>，它其实是发现主要矛盾的一个过程。比如影响这个公司可能有100个点，但是决定公司发展方向的可能就是一个两个点，这一两个点就是主要矛盾。举个不太恰当的例子，比如说一个人他是不是一个好的朋友？其他方面可以并不是太强，工作能力也不强，但是这个人特别仗义，这个人在做朋友这件事情上，主要矛盾是非常清楚的。但是这个事情的难点在于：随着时空的变化，主要矛盾可能会发生迁移。比如这个人从朋友现在变成同事了，你对他的要求可能就不一样了。</p>
<p>所以我们经常提到的几句话：**唯一不变的是变化本身，但是辩证法的另外一面，叫我们要寻找的是变化中的不变。**比如说人到底能不能两次踏入同一条河流？当然是可以的，但是反过来讲也不能，因为所谓的时空环境发生了变化。</p>
<p>我们还讲一句话：**这个世界没有完美解，只有时空环境下的最优解。**所以这个过程中，其实是在寻找最优解的过程。</p>
<p><strong>第三个点跟市场有关</strong>，这个层面我们不强调。经常大家讲估值是艺术，基本面是科学。基本面不好也不一定会跌，但市场决定了我们的买点。</p>
<p><strong>第四个层面，叫做时间分配和资本的分配</strong>。为什么我们把它提到这么高的一个高度？做投资这件事情，为什么大家会长期拉开这么大的差别？基本上所有的价值型投资者肯定都有两手茅台，但是为什么你跑不赢茅台？这是一个值得反思的问题：**你到底能不能把有限的时间花费在这些有效的地方？花在真正的好生意，真正的结构性变化上。**我觉得这个过程是、拉开长期差距非常重要的一个点。</p>
<p>我们经常用一个物理学定律来总结这四重维度，叫做超弦理论。大家都知道，电影里面也会讲：这个世界是有一维空间、二维空间、三维空间，我们生活的是个三维世界。在这里面最重要的有两条法则：第一条法则叫做低维生物无法理解高维生物。今天我们作为三维生物是很难理解四维生物是怎么去生存的。第二个就是所有的低维都是高维的投影，三维的投影就是两维世界就是平面，平面的投影就是一个点，就是一维世界。所以投资也是一样的：大部分人把投资理解成一个很低维的东西，但实际是非常复杂的，等会讲到我们的框架，大家可能有一些感受。</p>
<h2 id="第三部分-生意环境人的框架"> 第三部分：生意环境人的框架</h2>
<blockquote>
<p>我们这里一个经典的提法，叫做生意环境人的框架。这个框架我们也一直在演进，试图可以套用任何一个公司或者生意。</p>
</blockquote>
<p>**第一个因素是生意，生意意味着一定的普适性的，很多生意都有它的宿命。**我这里面举一个不太恰当的例子，两个双胞胎什么条件都是一样的，但是一个人进入了金融行业，一个人去工地搬砖了。工地的这个人非常的努力，他的时效可能是别人的5倍，但是它作为一个回报率来说却不太乐观。如果他想向老板要6份工人的钱，老板肯定会把他炒掉，请5个人来把替代他。但是如果进了金融行业，即使是一个非常不优秀的人，但是从回报的角度来说，可能是远远比搬砖的要好。这个想说明什么问题？在生意的宿命里面，其实很多东西是被注定的。</p>
<p><strong>第二个因素是环境</strong>。环境怎么影响投资呢？生意是有普适应的，美国人要喝东西，中国人也要喝东西，美国人要吃，中国人也要吃，这是空间维度。时间维度就是中国人今天饿了得吃，100年前人还是得吃。但是不同时间和空间环境下会表现出不同的形式。这里面我也不用多举例子，大家应该很容易理解中美、古今消费习惯各样的不同。</p>
<p>**环境会扭曲生意的表现的形态，但同时，环境的变化往往会创造巨大的阿尔法。**因为你必须要甄别一件事情，这个生意它到底赚的是生意的钱还是环境的钱？其实你回头想，很多行业和人其实赚的都是环境的钱，并不是赚的生意的钱，更不是人和组织能力的钱。这种状态也是可以赌的，但是我们必须要有一个很清晰的认识。如果环境发生了变化，我们的观点要跟着转移才行。</p>
<p>**第三个因素是人和组织。**历史上我们会认为：生意的属性是大于人和组织的能力的，所谓胳膊拧不过大腿。我们可以看到非常多这样的案例，我们私有化百丽的时候，其实是这样子的。当外界的环境发生变化，不太好的生意，它其实是原形毕露的。即使像盛总这样优秀的企业家，也无法扭转这个格局。</p>
<p>但是在这个框架之下，我们现在有一个新的观点，认为这个事情变得更立体了。<strong>生意、环境、人，在不同的情况下，这三个要素的权重是不同的</strong>。你想找到像10年前那样，又是好生意，环境又很好，然后组织非常牛的这种这种概率是非常低的。一般都是烂生意好人；或者好生意烂人。你需要找到这里面的权重的不同，所以我们在三四年前开始投一些看起来并不是特别好的生意，我们认为牛逼的人和组织反过来会改变生意的属性，所以它又是个辩证统一体。</p>
<blockquote>
<p><strong>什么是真正的好生意？</strong></p>
</blockquote>
<p>那么再往下去再看，什么是真正的好生意？这里面有几个基本的法则：</p>
<p>**第一个，时间是不是你的朋友？**很多行业其实想想时间并不是很长，你必须要在某一刻之前必须要做到怎么样，我们讲叫拼缝，它必须要在某一天之前赶紧上市。这个肯定不是我们的标准，即便是在我们的一级投资里面，这也一定不是我们的标准。</p>
<p>**第二个，是不是在越差的环境里面变得越强？**这一点我觉得这是我们跟Growth型（一般叫做增长型）价值投资的区别。但我们对行业突然放缓大幅放缓这件事情是很兴奋的，等会举到一些案例，很多都是在行业大幅放缓的时候我们去投了。比如我们去年控股格力这件事情，当时我很多人也问我，说房地产周期、库存周期都非常差，觉得这个不应该投。但事实上，行业的需求波动是很难去预测的，比如说谁能预测到今年出这么大的疫情。<strong>只有供给端，或者说是企业的核心能力或者行业的门槛，才是真正穿越周期的。而我们发现最牛的这些公司都是在每一次行业变得更差的时候，它的份额、利润值都是大幅上升的。当下一次曙光来临的时候，它也是复苏最快的，这个时候我们的逆向投资就会启动了。</strong></p>
<p>最后对这个体系做一个总结，可能比较虚，但确实也是在驱动我们自己：**世界是高维的、复杂的、多样的、曲折的不可知的，所以你任何低维的、简单的、单一的、线性的、完美的思考都是危险的。但是驱动事物发展和演进的主要矛盾，又是简单和单一的。**所以我们要追求包括量化，但又需要比较强的感性的认识。</p>
<p>**第三个，规模是不是你的朋友？你会发现很多生意随着规模的增长，它只是变得更大了，而不是变得更强了。**举个简单的例子，从生意的角度，单一维度来看，像餐饮、教培行业都不是什么好生意。我们历史上从来不投餐饮，这里面有大的问题就是：随着你规模增长之后，你只是变得更大，并没有变得更强。你面临的管人的复杂的程度，方差扩大的情况是非常明显的。但是当一家餐饮店扩到1000家店的时候，它到来到底带来了什么优势？它百分之七八十的成本其实是来自于单店的成本，比如说门店的租金，人员的成本。你跟你隔壁的那家单体的店竞争的时候并不会有什么优势。你指望说1000家店说能采购上产生的一些优势，根本没有什么太大的作用。**因为中国的餐菜和肉的供应链是一个很复杂的系统，所谓的集采并不一定比人家去菜市场买会更便宜。**包括教培的行业，为什么历史上也是这样子的？是因为教培的服务及产品，品牌是由口碑来建立的，口碑是由最差的老师来决定的。所以大部分时候随着规模的扩张，口碑是越来越差。</p>
<p>但是这件事情又是辩证的另外一面，为什么我们后来又会投了像海底捞？我们认为：如果所有人的值都很低，都只能搞三家店，但是有人突破了瓶颈的话，就会一览众山小。所以你可以看到海底捞将近2000亿的市值，(我们就说偏服务类的，不跟肯德基这种标准产品比)，第二名100亿都不到。教培行业里面，你会看到新东方和好未来两个都是我们的重仓的公司。比如说最近的疫情，你会发现有很多小机构都会倒闭，比如像线上化的过程，其实都是对这种龙头和寡头非常有利的一个过程。</p>
<p>**第四个，技术的进步，对你是好事还是坏事？**传统的消费品领域，通过渠道的优势，你能控制几百几千个店。但是有了天猫淘宝之后，消费者就变成了自我的决策，基本上不会受到其他的因素的干扰。这个过程当中，我们发现其实只有四五个行业的集中度变得更高了，其他的行业全部都回到它本来应该有的样子。比如说百丽的案例就很经典。当年我们先做多这家公司，从12年开始又做空这家公司，最后在底部又把它给私有化了，这里面每个阶段的主要矛盾是不一样的。</p>
<p>当时我们看到从线下转移到线上的过程当中，鞋和服装这种生意回到它本来应该有的样子，那就是离散。为什么是这样子？因为服装和鞋最大的特点是：第一大家对品牌不会有什么忠诚度的，大家是款式驱动的。第二个天生互斥，意思是我今天不愿意跟我昨天穿的衣服一样，也不愿意跟我的同事穿一样的衣服。这个过程中你会发现传统做服装公司的宿命都很差，就是因为它天生互斥。</p>
<p>在原来的百货的模式下，它有各种各样的渠道的垄断的优势，这种优势让它变得很集中。但是一旦到了线上，到了自我决策的时候，这个就回到它本来应该有的样子。但是有几个行业会变得非常的集中，比如说像白酒、运动、超高端的化妆品，但是低端的化妆品其实是变得更加的离散了。</p>
<p>所以技术的进步，它会把很多东西会给显露出来：<strong>这个生意到底赚的是环境的钱？还是对你的生意的属性天生就非常的厉害？</strong></p>
<p>线上化的过程中白酒为什么会变得这么集中？比如说我要买个2000块钱的酒，我还需要教育我要买什么牌子吗？我只是要在保证真货的情况下买到最便宜的茅台。当理解这一点的时候，你知道什么酒仙网这些故事是不成立的。因为作为一个垂直的平台，根本不可能创造更多的价值。所有的价值都是要以更高的效率或者更低的流量成本，所以最后一定是大平台把你给吃掉，综合型的平台把垂直型的平台吃掉。</p>
<p>以上都是我们判断好生意的标准，接下来我们再把它进行细分。</p>
<h2 id="生意最重要的属性-品牌"> 生意最重要的属性——品牌</h2>
<p><strong>上篇讲到了“生意环境人”的框架，接下来细分下去讲生意最重要的属性——品牌。</strong></p>
<p>**第一个：我们怎么去看品牌？**巴菲特也会跟大家讲要投很多品牌，但是巴菲特不会跟你说我为什么要投卡夫亨氏？这里我们常说两句话：<strong>我们认为费用的本质是你获取用户的成本。那么毛利率的本质是什么？毛利率的本质是消费者离开你的成本。</strong></p>
<p>假如我们发现了一个很好的生意，这个生意成本是50块钱，可以卖100块钱，能赚到20块钱的净利润。但是很快B也发现了这个生意，他也搞了一个成本50块钱的东西，他卖98块钱。如果消费者离开你的成本为零，那么消费者会全部转移到B的身上，B可以赚18块钱。A一看没有生意了，就会通过降价的方式或者增加费用的方式把用户再拉回来。这样一来一去平均回报率回到4-5%，还不如存在银行，这时生意才会进入到一个稳态。</p>
<p>但是为什么茅台大家愿意付那么多的溢价？市场上有那么多号称跟茅台一个产区一样品质的酒，消费者可能也不会去买。茅台的溢价，就是它的超额毛利，就是品牌的价值。</p>
<p><strong>我们认为品牌有十四个维度，这十四个维度又分成五个大类。第一个叫情感性属性、第二个叫安全性属性、第三个叫差异化属性、第四个叫便利性属、第五个叫成瘾性属性。情感性属性里面又可以分成四种，包括自我的彰显、与悲伤欢乐的这种气氛的结合、社交等等</strong>，这里面最经典的品类当然是酒了。</p>
<p>大家都知道酒是好生意，酒提供了情感性的价值，但是你会发现每一种酒的特点都不一样。你能提供所谓的情感性的属性，悲伤快乐、自我彰显。这种价值越多，其实门槛是越高的。比如一个人要显示他自己有品位，带个大金项链肯定不是个好选择。但是我能告诉你这款红酒来自哪个产区？它是哪一年的葡萄？那一年的气候环境是怎样的？它在某一种桶里面沉淀了多少年？它要与什么样的食物搭配--于是大家觉得这个人还挺有品位的。</p>
<p>但是为什么红酒公司这些属性非常好，但是它并不是一个好的生意？</p>
<p>红酒的特点是维度非常的多，有新世界、旧世界、年份、葡萄种类、开盖的时间、食物搭配--它非常的复杂。从生意的本质来说，它提供的价值很多，但它没有办法规模化。因为他太细分了，太复杂了，不是专业的人根本搞不懂。</p>
<p>全世界两家最大的烈性酒公司：帝亚吉欧和保乐力加，大家可能都喝过他们的产品。这两家公司是怎么做规模化的呢？我们以保乐力加的干邑为例，它分成四个档次：V.S.O.P、XO、Extra、收藏级，像人头马路易十三就是收藏级的产品。它用了四个维度来定义这个产品，给了消费者一定的差异化选择，但又不足够的复杂，能够规模化。</p>
<p>再看主打威士忌的帝亚吉欧公司。威士忌有两种，一种叫单一麦芽威士忌；一种叫混合型威士忌。单一麦芽型威士忌也是非常像红酒的，它很复杂多样。但是大家能想到某个产品的品牌吗？其实是很难的。真正给帝亚吉欧公司创造最大价值的是什么呢？是Johnny walker，一款复合型威士忌。复合型威士忌分为蓝牌、黑牌、红牌--四五种维度，更像干邑的定义的方式，而且它不用年份。在苏格兰，如果要写年份，比如说芝华士12年，那么这瓶酒里最年轻的酒应该是12年以上的，而且要在桶里面真正放12年。所以即便是在威士忌里面，我们认为复合型威士忌的生意属性远远好于单一麦芽威士忌。</p>
<p>我们再来看白酒，白酒的生意属性其实更牛。比如说茅台15年，在中国人的定义里面茅台15年，其实只有几滴的茅台15年的原酒。相比威士忌，这两个生意的资产的回报率是天壤之别。其实啤酒也是非常牛逼的一个生意，它非常像饮料，但是它又有酒的特点。所以我们可以看到像百威英博，别看他只卖10块钱或一点几美元一瓶，在很多地方它都能赚到40%以上的净利润率，基本上可以跟茅台五粮液一样的净利润。</p>
<p>所以理解各种各样的酒的属性也是一件非常有意思的事情。</p>
<p><strong>这里面我们穿插的可以讲一个问题，就是环境创造的α。为什么这个生意在中国会更牛？</strong></p>
<p>在西方社会，大家知道这些烈酒都是餐后去喝的。比如说今天有10个人去夜店，每个人点的酒会不一样，只要遵从自己内心的选择就可以了，这个叫做<strong>单杯文化</strong>。但在中国，百分之七八十的烈酒是跟餐一起饮用的，中国的文化叫<strong>围桌文化</strong>。围桌文化决定了两件事情：第一你不可能跟别人喝的不一样；第二个就是今天喝什么牌子的酒，并不是由你自己决定的。而是由你尊贵的客人，或者家里的长辈，这些人去决定的。这同样也导致了中国人对情感价值很高的高端酒需求跟国外是不一样的，全世界100美金以上一瓶的酒，中国占了95%，这是个挺夸张的数字。围桌文化让高端酒在中国变得更加的集中。</p>
<p>2013年的时候，大家很多人都知道，我们那时候开始重仓的白酒。市场暴跌的时候，大家都觉得白酒没有机会了，我们反而逆市买入。为什么我们当时会做这件事情？一个是我们对这些生意的判断：这个简直是世界上仅次于Du品的生意，我们等了10年终于等到这么一个超级大的机会。大家也知道从2013年的现在，,这些白酒公司基本上都是涨了10倍左右。</p>
<p>那么当时白酒面临两个什么问题呢？</p>
<p>第一个叫年轻人不喝酒。年轻人都不喝酒了，你还有什么谈什么PE？当时茅台大概交易的七八倍的PE，我记得茅台最低的时候跌到780块一瓶，五粮液跌到了400块钱。还有中央反腐的八项规定在那，感觉好像永远没有希望了。</p>
<p>但是我们当时做了两个判断，第一个是年轻人为什么不能不喝酒？当我们理解它的情感性价值和中国的围桌文化时，你就知道年轻人是不可能不喝酒的。原因有两个，第一个当他很年轻的时候，游戏规则的制定者，还是一波60后70后。比如说我堂弟在毕业以后去了家银行，他在学校的时候也是不喝酒的，但是进了银行之后，大家也都知道这个东西会变成什么样子。所以就像一个熔炉一样，很难有人短期内离开这个市场。第二个事情是当你到了30岁40岁的时候，以前喝的啤酒，确实是喝不动了。我们当时也做过一个研究，你喝进去的价值是和你所能吃的食物是成反比的，是有替代关系的。</p>
<p>所以当你到了三四十岁，有了一定的社会地位或者经济基础的时候，你会发现喝啤酒是很难在一个局上去彰显你的地位。比如我们今天吃一顿饭，如果把这个餐厅的名字盖住，把这个包厢的名字盖住，你可能不知道这个菜到底值多少钱。但这个酒放在桌子上，你就知道今天这个局是个什么样的定义的局。</p>
<p>我们研究了美国在过去的100年里面的烈酒人均消费量的变化，发现除了1927年禁酒令的时候，美国的烈酒人均消费量一直在快速上升。大家跟大家想象的并不一样，大家觉得美国是个很健康的社会，到美国基本上人人都在跑步。但其实烈酒的饮用量，是一直在上升的。另外日本的在过去的三四十年里面的演进，烈酒的消费量也在快速上升，这个主要是因为老龄化。</p>
<p><strong>关于当时的价格的崩塌，这就是我们另外一个话题，就是所谓的什么叫长期结构性机会？</strong></p>
<p>当时我们是怎么判断这个事情的呢？中国最大的问题是统计局统计数据的缺失，或者是不完全可信。所以当时我们大概花了两个月的时间，把中国所有的消费的结构做了一个拼图。2012年中国800万吨白酒的消费量里面，它大概怎么分布的？800元以上，就是所谓的高端酒，当时是4万吨；300800元，当时是5万吨；100-300元，大概是30万吨；100元以下是750万吨。它就像一个红酒的醒酒器一样，并不是个标准的金字塔。在2013年就是反腐之后，800元以上变成2万吨了，掉了一半。300-800元变成2万吨了，也基本上掉了一半，但是100-300元涨了30%，它变成了40万吨。总量是没有什么太大的增长的，甚至还略有下降的。也就意味着，政务消费出清之后，高端也是有底的，2万吨就是底部。</p>
<p>另外一个结构性的机会就是：就是在底部750万吨不变的情况下，中高端有一个升级的过程。这个升级的过程不光是因为老百姓越来越有钱了，还有一个很重要的原因是：中国真正的粮食酒和名酒，一共只有100万吨的产量。它是个1:8的关系，所以它是一个供给和需求同时拉动的消费升级。</p>
<p>**我们当时的判断：长期看这750万吨里面会出现两个很极端的变化，第一个变化是可能有200万吨的量长期会升到100元以上，分布在各个价位段里面，这会促成中档价位段、中高价位段、高档价位的酒都可能有3-5倍以上的空间，这是长期结构性的机会。**我们回头看过去几年基本上按照这个路径在演进，中间叠加的一些经济周期的因素，因为这个是一个长期的锚定值。那么经济周期的波动它只是一个放大的过程，我们很清楚这里面的驱动因素是什么。</p>
<p><strong>另外一个判断：我们认为剩下的500多万吨里面有一部分会消亡，老酒鬼会退出这个市场。但是中低档价位段里面会出现离散，场景化的离散。</strong></p>
<p>所以它是两个方向发展，一个是往上跑，一个是横向的离散化。那么离散化的最大的特点是什么？</p>
<p>我们回到刚才年轻人不喝酒这个问题，年轻人跟领导吃饭，跟重要的客户吃饭，当然要喝酒，还要喝茅台，喝五粮液。但是别忘了一点，年轻人当他独处的时候，当他跟朋友们在一起的时候，他不用遵守这个游戏规则。我们认为在未来会随着场景的多元化和人群的多元化，会细分出很多样的品牌，所以我们就在PE阶段投了江小白。江小白的本质是什么？其实营销这些我认为都不是本质，江小白的本质是它把围桌文化变成了单杯文化。人其实是多样的，需要不同的品牌，不同的产品来满足不同的需求。在一些正式的商务场合，你可能喝的是茅台，但是跟着朋友或者大学同学，穿着短裤在路边撸串，就需要一人一瓶江小白。我们认为年轻人的场景会多元化，长期看白酒还会是主流，但是它可能从今天的99%的份额变成70%或者80的份额，那么这里面会有酒精的本身的多元化的过程。</p>
<p><strong>以上是我们在酒里面是怎么去想这个问题的。接下来我们再举个例子，说说品牌的第二属性：安全性属性。</strong></p>
<p>安全性属性并不一定是说用了就安全了，而是不用会不安全。最简单的是婴儿产品：纸尿裤、婴儿奶粉这些。这里面最大的特点消费者和决策者是分离的，消费者是孩子，但决策者是父母。孩子并不会告诉你感受，他只会哭。所以几乎所有的父母都会在他自己能力范围内买最高的品牌，一旦这个品牌小孩子用的比较好，是很少能换的。所以我们认为这些生意里面，客户的离开成本会比较高。</p>
<p>回想到我们最开始说的模型，一个婴儿产品100元，我搞成另外一个产品，跟这个产品绝对一模一样，只要90元钱。消费者就不愿意拿孩子做这个实验。</p>
<p>化妆品里面也是一样，刚才我们讲到在电商场景的时候，只有超高端的化妆品是变得更集中了？我们知道护肤和美妆是两个市场，护肤就像每天吃饭一样，是很讲究的。但是彩妆就像时尚的衣服一样，经常要换的，可以尝试不同的种类。</p>
<p>这里面有意思的地方在于，护肤的三种主要的功能：补水、美白、抗皱，这三种属性的宿命会完全不同。补水的证明时间很短，最高端的化妆品品牌中，补水的可能要500块钱一瓶，你能说比5块钱的好多少？我不觉得不一定，因为它的证明的时间会非常的短。但是所有高端的产品几乎全部都是以抗皱线为主的，因为它的证明时间需要20年30年，这个过程其实是一个让用户对未来充满惶恐的过程，导致用户的离开成本会非常高。所以在化妆品里面也出现两个很明显的变化：第一个是叫做高端的产品是高度集中化，我们看到比如LaMer在的中国的成长，每年达60%。我们要去看的其他的一些高端的品牌，它的集中度会不会也呈现这样的态势。</p>
<p>我们再看彩妆这个市场，它是快速的离散的。我们认为：在一个快速变化的生意里面，新公司能快速的打败这些老的公司，所以我们投了完美日记。</p>
<p>**刚才我们讲了一些关于品牌的东西，接下里讲讲我们对另外一些品类也会有不同的一些认识，比如说B to B的东西。B to B的很多领域其实是没有品牌的。**我们在A股投了建材公司、墙开，还在日本投了一家公司涂料公司。在传统的划分的方式里面，他们三个产品之间是没有啥太大的关系的，但是我给大家讲讲这里面的真正的共同点在什么地方。</p>
<p>比如建材公司，它做石膏板的，大家肯定说不出来一个石膏板的品牌。原因很简单，最好的石膏板大概10-15块钱一平米，装修个100平米的房才花1万块钱，所以你根本不可能去学习这些什么石膏板品牌哪个好。但是石膏板真正的高频用户其实是安装工：安装一平方米石膏板的价格大约是100块钱，其中50块钱是人工成本，另外30块钱是龙骨的成本。如果要装个100平米的房子，会发现材料才1000块钱，但人工要大几千块钱，要花好几天的时间来安装吊顶啊隔墙之类的。如果用了一个杂牌，它裂了怎么办？工人还得上门来修。</p>
<p>今天一个人工的成本是多少钱呢？一线城市大概500-600块钱。那么有一件事情是必然发生的，**我们认为在未来的5年里面，白领会越来越不值钱，但是安装工会越来越值钱，我们认为在一线城市很快会看到1000-1500块钱一个工。**这跟人口结构有很大的关系，低端劳动力人口是大量的减少的，这个是公开的数据。所以当人工成本越来越贵的时候，安装工就根本不可能再去用那些杂牌了。</p>
<p>另外一个公司比如说施耐德，是全球做电器的专家，它最好的生意就是墙上的开关。这个东西的逻辑跟刚才我们说的是一样的，在国外电工的工资大概一小时50美金，而且老外不像中国都是新房，老外都是替换性需求，可能就坏了一个墙开，得找个人工上门。电工开着车过来，两个小时100美金。那开关多少钱？大约只值5美金。所以这些安装工他怎么可能去用一些杂牌呢？不光是返工很耽误事儿，万一漏电了，把人家电到，那问题更大。所以这些人的反悔成本会非常的高。</p>
<p>当你真正理解这件事情的时候，我们并不在意房地产周期，房地产周期当然也很重要，这个决定了我们的买点。但是核心就是：这个生意会随着安装工的成本越来越高，生意属性会变得越来越好。</p>
<h2 id="为什么环境这个因素在中国很重要"> 为什么环境这个因素在中国很重要？</h2>
<p>中国确实跟我们研究的对象——所谓的西方成熟社会，有很大的不同。不光是消费习惯，执行力这些方面，我认为有两个根本的不同在于：</p>
<p>**第一：中国的社会的周期其实是很短的。**大家可以想一想：日本的天皇大概有1400年的历史，虽然中间的幕府在不断更迭，但是天皇作为一个象征，它有极强的延续性。我们研究日本的时候，会发现日本其实有真正的百年品牌，印度都有，但中国是没有的，所以它的周期会比较短。</p>
<p>**第二：中国所有的商业模式都是在同一时间内出现的。**比如看零售的时候，就会发现美国是从杂货店到百货店到超市一路演进过来的，后来从沃尔玛模式又裂变出来COSCO模式、折扣店模式、便利店模式``等等。但在中国，所有东西是同时间出现的。有人问我说COSCO在中国会不会成功？我说我不知道，但是如果认为历史还是123456789这样演进的，那在中国市场就不一定了，因为中国是经历了从123直接变成10的过程。10的意思是什么？就是电商。为什么电商这个东西这么恐怖？核心是它是拥有数据的，它和历史上所有的零售形态都不同。</p>
<p><strong>我再举几个例子，说明中国和其他国家生意环境的不同。</strong></p>
<p>**第一个例子：为什么可口可乐在美国是很好的生意？**可口可乐是巴菲特的重仓股，但在中国，这种生意我们基本上都不会花太多的时间（除非碰到特别牛的人组织，我们现在只说的是从生意和环境的维度）。美国跟中国消费上最大的两个不同：<strong>第一个美国人吃的很简单</strong>，大家去美国最大的感受是天很蓝，空气很好， 但是没东西吃，天天要找火锅。因为美国的SQ（食品品类），比中国要少多了，所以在美国就容易出现麦当劳这样的公司。但在中国是很难的，中国每顿饭吃的SQ比美国人整一年吃SQ都多。</p>
<p>美国的饮料，先区分含不含酒精？不含酒的要热的还是冷的？热的80%是咖啡，20%是茶。凉的就4种：水、碳酸水、果汁和红牛。那在中国、日本就很复杂了，我们的SQ是巨多的，这对消费者很好，每个星期都出个新品才好呢！但是对企业来说却是很悲催的一件事情。</p>
<p>**第二个：美国当年的生态系统是怎么生长出来的？**因为可口可乐是卖给罐装商的，然后灌装商在一个地方去分销，所以他们很早的时候就形成了地盘，大家可以看到这几家公司都上市公司。另外，可口可乐以前是玻璃瓶包装，这个很重要，这样使它的生意属性就变得跟啤酒很像。因为它瓶子很重，所以运输半径是很短的，而且它是双向的物流，就是说可乐既要运出去，瓶子还要收回来，所以它对整个系统的产能利用率要求很高。当它形成了规模优势之后，美国还有“三层分销法”保护它。<strong>三层分销法</strong>是什么意思呢？就是品牌商是不能直接拥有、控制经销商，不能控制门店，必须是三层结构，不然就叫垂直垄断。</p>
<p>再来看中国，第一个亚洲人的口味简直是太丰富了；第二个就日本人发明了可抛弃的 pet，这让整个生意的属性发生了巨大的改变；第三，中国没有三层分销法，所以你会发现康师傅有四五万人的团队，统一有三四万人的团队，农夫山泉也有几万人团队~~大家在同一个市场里面，不光是品牌之间的竞争激烈，几乎每一个环节都是over supply的。但美国是这样的：可乐通过罐装商分销，红酒也通过可乐的灌装商分销，所以它没有那么高的恶性竞争。</p>
<p><strong>刚才说的是空间不同产生的α，接下来我再几个举个例子，看看时间的不同对投资结果有什么样的影响。</strong></p>
<p>海螺水泥大家都很知道的，太传统、太夕阳的一个产业了。但是你会发现海螺水泥最大的涨幅是过去3年，并不是在头10年。我们第一直观的感觉，中国房地产和基建的扩张周期应该是前10年。海螺作为龙头，应该是在前10年的股价回报最高，什么原因导致了这个时间差？</p>
<p>在地产快速扩张周期中，海螺水泥每吨毛利率一旦超过100块钱的时候，一堆小厂就复活了。所以当需求快速上升的时候，产能也在快速上升。当然优秀的公司总是在这过程中不断地提高份额，但这是个痛苦的过程，海螺水泥的单吨毛利会从230元跌到60元，把别人干死，它也只能挣60元。然后进入房地产的下降周期，随着地产周期再一次回归，它的吨毛利又变成230元，然后一堆小厂又复活，海螺的吨毛利又变成60元，把小厂干死，来回的重复~~所以这个公司以前是一个波段股，大家叫周期股。</p>
<p>但是为什么2016年之后事情发生了改变？2016年这一波中国的房产和基建的复苏应该是历史上增速最弱的。但是这一年出了一件非常牛的事“蓝天计划”，这个计划就直接把产能给控制住了：矿山不让乱挖了，锅炉不能乱烧了，治理污染、雾霾变成了头等大事。所以上一轮产能出清的时候，所有的产能都给卡住了。在最弱的一个需求复苏过程中，海螺水泥终于能站着把钱赚了。</p>
<p><strong>所以投资中很多事情其实并不是我们直观的感受，这个过程中我们要去理解时间维度上环境的变化。</strong></p>
<p>我们再讲一个比较有意思的案例，我刚说啤酒是个好生意，它终极结局在很多市场上能挣40个点，但中国只挣两三个点的利润率。这个是很好的生意，但在过去的十年，可以看到无数英雄都折腰在此。这个行业我们看了10年，如果你是在2006年介入，那就是十年三倍的涨幅；但如果你是在2016年介入，那就是三年三倍的涨幅。为什么会有这么大的差别？</p>
<p>中国的啤酒经历了4个阶段，第一个阶段是从1990年到2010年，人均消费量快速上升，产能也在快速的上升，一团混战，诸侯割据。2010年到2015年，人均消费量已经涨不动了，基本持平，还有略有下降，但是产能还在上升。我记得当时2013、2014年的时候，当时大家已经厮杀到什么程度？最好的两家公司，百威英博和雪花啤酒大概只挣了两三个点的利润率，以前他们可是挣了15%~ 20%的利润率。在整个行业已经负增长的情况下，龙头公司惨烈的竞争，看起来好像永无出头之日。</p>
<p>当时很多人都已经绝望了，但我们兴奋了起来。因为我们认为所有的行业都要经历四个阶段：</p>
<p><strong>第一阶段需求爆发，供给扩张</strong>，大家看到这个生意不错，都来搞，快速把这个市场做大。</p>
<p>**第二阶段叫需求放缓，供给快速扩张。**因为经历了第一阶段以后，每个人都觉得我肯定是最后的赢家，开始捉对厮杀，这时候就看到利润率开始下降。</p>
<p><strong>第三阶段叫需求继续放缓，但是产能开始扩不动了</strong>，因为大家都发现老大只挣三个点，老二可能都已经开始亏损了，老三是血亏，不可能再扩产能了。</p>
<p>**到了第四个阶段，新的驱动力形成需求开始复苏的时候，你会发现供给已经被卡到一个非常小的范围。**这四个阶段里面我们主要投的是第四阶段，当然了也会投第一阶段，但第一阶段很多公司实际上是VC和PE的阶段。</p>
<p>在不同行业不同的阶段的时候，其实需要的能力值也是不一样的。第四阶段的最大的特点是什么呢？**当第四阶段的逻辑展开的时候，最大的特点是看起来需求的复苏或者增速是没有第一阶段快的，但是供给被卡死，赢家非常清楚，**犯错的风险是极低的。所以回到啤酒的生意，我们认为从2015年是一个新老划断，很明显的指标就是中国啤酒的人均消费量已经涨不动了。但是请记住彼得林奇说过一句话：**增长不一定是你的朋友，**那么这句话的反面是什么？<strong>不增长也不一定是你的敌人！</strong></p>
<p>第四阶段往往增长速度也不快，但是它获得的回报的空间是巨大的。为什么我们认为第四个阶段要展开了？一个因素是：中国最大的机会在于中国的啤酒的价格是比国外低很多的，中国现在真的比国外便宜的东西已经不是很多了：车子、房子、衣服、婴儿用品都比国外贵，这里面有三四倍的空间。另一个因素是：90后的消费者开始加入到这个市场了，他们也有二十四五岁了。新的消费者驱动力的形成以后，加上供给端经历了这些年惨烈的竞争，大家一定要找到一个新的方式来处理这个问题，高端化是一个新的驱动力。并且经历过产能出清之后，龙头快速集中了份额，虽然只挣两三个点的利润率，但小公司已经被清出去了，大公司之间要开始形成默契了。所以在这过程中会发生一些边际的变化，加上之前那个过程中投资者的预期已经被彻底的毁灭了，所以我们叫<strong>All Star A line</strong>，就是叫所有的星星都排到了一条线上，这种状态是我们需要重仓的时候。</p>
<h2 id="第三个层面-人和组织"> 第三个层面：人和组织</h2>
<p>我们要投好的企业家，有洞见力的企业家，有远见的企业家，我相信这是大家的共识。但是这里面仍然有两个问题：</p>
<p>**第一个我们认为企业家们现在面临着一个很大的问题，就是如何能够突破行业的瓶颈？**比如为什么我们要去投海底捞？明明这个行业不是好生意，它有各种管理上的瓶颈，但是如果有人有效地突破了它，它就一览众山小了。所以海底捞市值约2000亿，第二大的餐饮公司连100亿市值都没有。突破瓶颈是很重要的，我们要给一个巨大的溢价。</p>
<p>**第二个问题是：很多公司都面临一个很大的问题，怎么从一个单产品型的公司变成一个平台型的公司？**我们认为这也是很不容易的，当你组织的架构不够清晰、有效的时候，大的事业部会把小的事业部的资源全部吸走。</p>
<p>我们当年投资百丽也面临着这个问题，百丽后来收购了很多品牌，这些收购的品牌都没做起来。百丽当年收了妙丽，非常个性的品牌，在平台上做得也不好。这里面很重要的一个一点就是：**一个销售能力或渠道能力很强的公司，它会把很多东西给同化掉的。**任何公司都有两个重要部门：一个是产品部门，一个叫需求的部门。需求部门是什么？它接近消费者，它永远要卖好卖的东西。它不要太多的花样，一切以消费者的需求为主。但产品创造部门，也就是设计师们，他们往往天马行空。那么核心的一点是：到底是谁掌握了资源？</p>
<p>我们见过太多这样的公司，比如说之前香港最牛的消费股，大概10年涨了30倍，但是后来股价从最高时候的23块钱跌到了现在6块钱。因为当消费者发生转移的时候，公司没有跟上这个时代。为什么没跟上？我认为核心是：组织内部掌握资源的人是在销售端，而这些销售端的人不思进取，都是一些当年打打过胜仗的五六十岁的老人，他们并没有改变组织形态，来适应市场。</p>
<p>我们认为当一个公司突破了自身的瓶颈之后，如何把单事业部的公司变成一个平台级的公司？核心的一点是：**能不能把个人能力变成组织能力？能不能把事业部能力变成集团能力？**这是非常重要的两句话，我认为在所有的领域都适用。</p>
<p>多说两句，我们认为有两种不同的组织：一种叫control型组织，就是控制型组织。控制性组织它的逻辑是自上而下的，跟创始人的性格有很大的关系。这种组织的特点是当它在跨越新的一些领域的时候，它能够调动大量的资源。坏处是因为它是一个自上而下的组织，所以自下而上的创新力不够。如果它不能形成中台和中后台能力的话，那么它在扩张性上会遇到领导人天生的能力瓶颈。</p>
<p>第二个叫context型组织，就是内容型组织，它是一个自下而上的生长型的组织，内部可能有赛马机制。这种组织的特点是它在每个地方都能够全面开花，这是互联网里面最经典的两个案例。</p>
<p>这两种组织都有它自己的特点，也都会有自己的问题。所以面对不同的产品线，不同的时间和空间维度，我们认为这里面要关注的点是不一样的。</p>
<p>回到投资的话题，最近大家都会问到一个问题：现阶段哪些公司是真正的好生意？茅台其实大家都懂了，但是估值也这么高了，怎么继续获得阿尔法？</p>
<p>我认为这是两个问题，第一个问题是大家对高质量生意的看法，其实我不认为大家是真正理解了，大家只是在追求增长。你看2019年是牛市，但2018年这些生意被抛弃成什么样子？第二个问题是在很多的领域里面，环境之上的到底是生意重要，环境重要还是人重要？那么我们认为很多领域里面人和组织的价值，在未来的5~10年会变得更加的重要，原因是我认为它会穿透周期。</p>
<p>穿透周期是什么意思？随着信息的对称，被低估的公司会越来越少。大家慢慢形成了共识：净资产以下的公司很多，但就是没人买，因为这个东西没有未来，大家怎么愿意付费？但是如果你去投的是一个很牛的组织的话，你会发现他可以看到3~5步棋之后。今天如果这家公司是值100块钱，市场给了130元，短期看是有点被高估的。但是如果这公司5年后市值1000块钱的，那今天的130块钱就是被低估了。<strong>但是什么样的公司才具有这种穿透组织的能力？这不光是对生意和环境的研究，同时要对企业家有非常深刻的理解。</strong></p>
<p>最后回到我们的模型本身，在未来的环境，到底是生意重要，还是环境重要，或是和组织更重要？我们的体系也是一个不停的演进和迭代的过程。但是回到我们的指导思想：我们认为这个世界是高维的、复杂的、多样的、曲折的、不可知的，任何以低维的简单的、单一的、线性的、完美的思考都危险的。但是我们认为驱动事物发展和演进的主要矛盾又是单一的，它是辩证法的两面。</p>
<p>另一个重要的指导思想是：**你要知道自己不知道。**方法本身是可以被演进的，但是关键是不是能够保持知道自己不知道的一个状态。二级市场的演进速度是很快的，十几年前，只要能把PE算清楚，就能赚钱，今天已经没有这样的机会了。以前大家都不知道白酒是好生意，但现在大家也都有理解了。<strong>所以核心并不是方法本身，而是你对方法论本身的迭代和演进</strong>，那么最后最主要的逻辑叫：<strong>知道自己不知道。</strong></p>
<p>比较好的一个状态应该是什么样的？**首先把书读厚，这就是我们说的信息点的收集，最基础的状态；第二点是把书读薄，这其实是要找到主要矛盾的一个过程。这两点其实很多人都能做到，但还有第三点：把书合起来扔到一边去，写下三句话。**这个过程是一个抽象的过程，它是对整个世界观重新塑造的过程。所以我认为要保持这种知道自己不知道的状态，我们的体系才会不停的演进和迭代，才能够保持未来持续的竞争力。</p>
]]></content:encoded>
    </item>
    <item>
      <title>关于「算法工程师」职业发展的思考</title>
      <link>https://enze5088.github.io/articles/notion/article-4/</link>
      <guid>https://enze5088.github.io/articles/notion/article-4/</guid>
      <source url="https://enze5088.github.io/rss.xml">关于「算法工程师」职业发展的思考</source>
      <category>那些关于发财的白日梦</category>
      <pubDate>Thu, 03 Jun 2021 09:04:18 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="关于「算法工程师」职业发展的思考"> 关于「算法工程师」职业发展的思考</h1>
<hr>
<blockquote>
<p>转自[Hulu研发副总裁诸葛越]讲演稿</p>
</blockquote>
<p>近年来人工智能技术取得了巨大的突破，应用到工业界的场景也越来越多，吸引了大量的人才涌进算法研究领域。我认识的很多朋友也投身其中，在学术界做研究的有，在工业界搞应用研究的也有。同时因为工作的原因，我也接触了很多算法方向的候选人，有些是学生，有些是来自于工业界的其他公司。更深的接触则是来自于与Hulu内部算法团队的同事们，不论是日常的研究还是人员晋升时让我有机会和大家有很多深入的交流讨论，其中就包括了算法工程师/研究员在工业界职业发展的问题。算法工程师/研究员作为一个比较新的职业，还没有成熟的职业发展路径做参考，所以我在这个领域的很多朋友、同事都会遇到这个问题：<strong>&quot;接下来，我该怎么办？我的出路有哪些？&quot;我今天的分享不会给大家指出具体的路径，因为每个人和每个公司都不一样，但希望通过提出一些具体的问题，引发算法工程师/研究员群体对未来职业发展的思考</strong>，最终找到一条适合自己的职业发展路径。我今天的分享会分为三部分：</p>
<ul>
<li><strong>公司组织架构</strong></li>
<li><strong>人才模型</strong></li>
<li><strong>值得思考的问题</strong></li>
</ul>
<h2 id="_01-公司组织架构"> <strong>01.公司组织架构</strong></h2>
<p>很多人可能会有疑问，聊个人职业发展为什么要先介绍公司的组织架构？它和职业发展有什么关系呢？我想说不仅有关系，关系还挺大。首先，每个公司的组织架构都不同，不同的架构决定了不同的晋升通道，第二，职业路径规划是为了在职场上走的更高更远，所以脱离实际的晋升通道谈规划是不实际的。你在做职业路径规划时，首先就要了解所处公司的架构情况，明确可能的职业路径有哪些，然后才能确立目标并为之计划和执行。接下来我就简单介绍下三种常见的组织架构：</p>
<p><strong>第一种</strong>是AI LAB。这种模式在大公司很常见，老牌的有以MSRA为代表的外企研究机构，国内的如百度的深度学习研究院、阿里达摩院、头条的AI LAB等。AI Lab可以算是工业界中顶级的人工智能研究机构，汇聚了众多的业界大牛和顶尖人才，做出很多根植于工业界的研究贡献。在这样的结构下，你向上走需要具备的技能和产出、能达到的高度肯定与在研发线的同学是不一样的。</p>
<img src="https://www.hualigs.cn/image/60b89081533b2.jpg" style="zoom:60%;" align="center"/>
<p><strong>第二种</strong>的AI团队是在研发线，位于CTO下面，支持不同产品线和公司级别的AI需求。对这个架构下的AI团队，公司希望它提供的是一种通用的类似于中台的能力。</p>
<img src="https://www.hualigs.cn/image/60b8908154101.jpg" style="zoom:60%;" align="center"/>
<p><strong>第三种</strong>的AI团队位于不同的产品线上，这种结构在大公司常见。产品线比较丰富，且用户量较大，因此需要有专门的AI团队进行支持。比如在现场与我沟通的一个小朋友，他是腾讯看一看团队的算法实习生，听他介绍了解到腾讯里面团队分工很细致，算法团队和工程团队是分开的，算法团队又分为独立的召回团队和排序团队，每个团队在自己的方向上做的很精细。</p>
<img src="https://www.hualigs.cn/image/60b88fbd48132.jpg" style="zoom:60%;" align="center"/>
<p>以上三种结构是比较普遍的，大家可以对照想一想你们团队在哪个架构下面，你又在团队的什么位置上。两三年后你肯定是想往上晋升的，那在现有的晋升通道下需要具备什么样的技能和产出呢？在向上呢，还需要怎么做？继续向上又如何呢？这些都是需要大家提前思考的。</p>
<img src="https://www.hualigs.cn/image/60b8908144500.jpg" style="zoom:60%;" align="center"/>
<h2 id="_02-人才模型"> 02.人才模型</h2>
<img src="https://www.hualigs.cn/image/60b88fbd3dd41.jpg" style="zoom:60%;" align="center"/>
<p>接下来给大家介绍一下人才模型，在给定了公司架构后，你要怎么做呢？大家可以看这个图，一个大写的字母“T”。这是我经常给Hulu的同事讲的，我们要想象我们的技能就像一个“T-shaped”。也可以比喻为大家熟识的“wide&amp;deep”算法，就是你要走多宽，你要走多深。</p>
<img src="https://www.hualigs.cn/image/60b88fbd4faeb.jpg" style="zoom:60%;" align="center"/>
<p>有一类人，尤其是我们今天的受众，应该是倾向于偏深的，从而成为一个专家型人才。比如有一个人，他想成为一个专家，我们对话如下：</p>
<blockquote>
<p>我问：5年之后你想干什么？ 他答：我想做一个专家； 我问：那10年之后呢？ 他答：更专的专家。</p>
</blockquote>
<p>想成为专家很好，但只说专家是不够的，假如说要成为Machine Learning的专家，好像有点太泛了。所以接下来还要问自己另外一个问题，那就是我要“专”什么？对于技术同学来讲，大概有三个维度上的专家：</p>
<p>第一，<strong>某一项技术的专家</strong>。比如说我是强化学习方向的专家，我对强化学习演进的历史特别清楚，过往的技术点有哪些，未来可能的研究趋势有哪些，更重要的是我还为这项技术的发展做出过某些重要的贡献。那我就算是强化学习的专家了，这是一个很确定的。</p>
<p>第二，<strong>某一技术领域的专家</strong>。比如说CV是一个技术领域，这个领域里有很多特定的算法，虽然我不是某一特定算法的专家，但是我都了解，更关键的是我可以利用这些成熟的算法解决特定的业务问题，这也是一类专家；例如推荐系统也是一个领域，也可以成为这个领域的专家。</p>
<p>第三，<strong>某一商业领域的技术专家</strong>。比如说拿商业广告来说，它包含了一整套的技术，例如说怎么优化网上广告的配型，比如说怎么做广告的个性化推荐等，你不仅都知道还可以带领团队搞定整个系统的问题，所以你也可以成为这一商业领域的算法专家。</p>
<p><strong>如果你想成为一个专家，那到底是你想成为上述三种的哪一种呢？有过思考吗？</strong></p>
<img src="https://www.hualigs.cn/image/60b88fbd4f0f3.jpg" style="zoom:60%;" align="center"/>
<p>我们接下来说第二类，与专家相对应的是偏宽度，简称为“杂家”吧，也有三个维度：</p>
<p>第一，<strong>全能型算法工程师</strong>；这类工程师的学习能力特别强，给他任何一个问题，都能通过快速学习掌握相关知识点，然后把问题解决；再抛给他另一个完全不同的问题，依然可以快速解决。其实在某些类型的企业或者某些阶段的企业，给算法工程师营造的工作学习环境就是这样的，你没有办法决定精专一个方向，因为每天都要面对各种层出不穷的问题，且都需要你尽快解决。但是大家可以想象在这种环境长期发展下去的话，对算法工程师长期发展会造成什么影响；</p>
<p>第二，可能不完全包含算法工程师，在特定的领域内，你对数据全流程都了解，对数据怎么从头到尾应用到机器学习领域都了解，有点<strong>数据科学家</strong>的意思。</p>
<p>第三类，在大厂或者大平台有过丰富的经验，用同样的算法解决不同量级的问题；相较于其他没有相关经历的算法工程师，你知道怎么去处理更大的问题面对更大的挑战，这是很重要的优势。</p>
<p>总结来说，即使说你什么都能干，也要想一想是在一个什么样的框架下处理什么样的问题。</p>
<img align="center" src="https://www.hualigs.cn/image/60b891eb365da.jpg" style="zoom:60%;" />
<p>再说第三类，还有些人在特定阶段要走向管理，转向管理需要具备的技能与前两类又不一样。</p>
<p>其实当你已经是一个领域的专家后，大家都会在继续走专家线还是走向管理线间有过思考和摇摆。但是不管你最终走向何方，一定要知道适合的才是最好的。怎么判断自己更适合哪个呢？那就需要把专家线和管理线对人才模型的要求搞清楚。专家线前面已经说过，我们在介绍下管理人才需要具备的技能。</p>
<p>第一点，“学而优则仕”。怎么理解呢？只有那些已经成为技术专家的人，才有选择的机会。转管理是有门槛的，发展路径会一般是技术专家-技术leader-管理。；</p>
<p>第二点，需要学习相应的软技能。这个涵盖的范围还挺大，简单列举些：如何招人、如何用人、如何保人，领导力，沟通能力，项目管理能等等。</p>
<p>如果你未来想要走向管理岗，在技术上一定要有建树，还要提前学习和储备相应的软技能，而不是转过之后在补充。</p>
<h2 id="_03-值得思考的问题"> 03.值得思考的问题</h2>
<img src="https://www.hualigs.cn/image/60b88fbd52e75.jpg" style="zoom:60%;" align="center"/>
<p>这次分享不是为了给大家具体的答案，而是希望能够引发大家对于算法工程师/研究员职业发展问题的思考。人无远虑必有近忧，想清楚后早做相应的规划和准备。接下来会给大家提出几个问题：</p>
<p><strong>1. Deep and wide, which domain?</strong></p>
<p>这个问题在之前讨论过，大家可以回顾下。希望你在平时对这个问题就有深入的思考，不要到三五年之后被问到的时候回答“我还没有想好”。找不到方向时，往哪个方向走都是错误的。</p>
<p><strong>2. How much do i want to work with people? And how good am i?</strong></p>
<p>大部分人都有转管理的想法，但是你一定要清楚做研发与做管理是很不一样的。简单提醒几点：</p>
<p>第一，何时选择转管理？如何从技术专家过渡到管理岗位？这是需要面对的第一个问题；</p>
<p>第二，如何获取团队的信任？是不是只要你的技术足够强，就可以获取团队成员的认可和尊敬，就可以把老板交代的任务都干好。是不是真的是这样子的？</p>
<p>第三，如何尽快进入管理角色？你要充分意识到管理工作更多是和人打交道，包括上级老板、一起配合的兄弟团队以及你所带领的团队成员，涉及到很多与人沟通的细节，你该如何应对。</p>
<p><strong>3. How to balance engineering skills vs research skills?</strong></p>
<p>这个问题是算法工程师群体们普遍都会遇到的，算法研究能力or工程能力。您在其中一个方向上成为专家是没有问题，企业都会有合适的位置给到你。但如果你想走到更高的职级，就需要不断拓宽与现有工作相关的技术栈。大家可以回到开头讲的团队架构图，你会发现在组织的特定层级的特定位置，算法研究和工程能力会结合起来，这就要求位于这个位置的人必须兼顾两个方向，两个方向都要通，否则很难做到把两个方向的团队带好。</p>
<p>随着近两年算法岗位的成熟，有一种说法重新被大家认同，那就是“算法工程师首先是一名工程师”，可见对算法工程师工程能力的重视已经是普遍的观点了。</p>
<p><strong>4. How to balance working on the real problems vs exploring advanced tech?</strong></p>
<p>对于这点，它不是一边好于另外一边的问题，更多的是环境塑造或者个人偏好。尤其近几年，企业对算法应用的需求激增，但偏偏人才供给短期内又很难被满足，导致很多算法同学一个项目接一个项目的做下去，解决了一个又一个的实际问题，在这个过程中自然积攒了大量的实际经验；但是另外一些身在Ai Lab的算法同学就不一样了，顶会paper一篇接一篇的发，保持着对新技术新发现的持续跟进。你很难说一个好过另外一个，需要认清个人更喜欢哪类工作，以及在特定的环境下做符合预期的事情。</p>
<p><strong>5. How to develop methodology over time so you can solve larger and more complex problems?</strong></p>
<p>最后是关于发展个人方法论的问题，这个问题我在内部也经常提起。拿我们团队举例子，你是团队的一个reasearcher，业界一些前沿的技术成果都跟上了，那你如何才能在此基础上做的更好呢？</p>
<p>大家可以思考这样一个问题，工作经验分别为1年、5年、10年的三个工程师在解决同一个问题时的差别是什么呢？结果可能都一样，那就是问题都被解决了，但真正的差别在于大家看问题的角度和维度不一样。作为工作经验久的工程师，你看到的不应该仅仅是问题的表面，更要看到问题的前因后果，以及其他与此相关的事情。为什么会这样呢？因为通过多年的工作积累，资深的工程师已经有了解决问题的框架，可以把问题放在框架下拆解分析，最后找到解决问题的方法。</p>
<p>随着工作年限的增加，大家一定要总结并提炼出与工龄相匹配的方法论，否则你是无法对“后浪”形成竞争优势。形成了自己的方法论后，不仅可以更高效的应对老问题，还能从容的面对新问题。为什么会呢？因为老问题可以直接套用现有解决方案，效率肯定提高；面对新问题时，将问题放在现有的方法论下进行拆解，很快也会找到方案。你一定要知道，随着职级增加的不仅是收入，还有需要处理问题的数量。如果没有积累，你在应付激增的问题时会显得很吃力。</p>
<p><strong>今天的分享就到这里，希望通过这些问题引发你对职业发展的思考，早日找到适合自己的发展路径，谢谢大家~</strong></p>
<p>[TOC]</p>
]]></content:encoded>
    </item>
    <item>
      <title>关于笔记</title>
      <link>https://enze5088.github.io/articles/notion/</link>
      <guid>https://enze5088.github.io/articles/notion/</guid>
      <source url="https://enze5088.github.io/rss.xml">关于笔记</source>
      <pubDate>Thu, 03 Jun 2021 09:04:18 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="关于笔记"> 关于笔记</h1>
<p>个人自用总结笔记</p>
]]></content:encoded>
    </item>
    <item>
      <title>图文预训练模型总结</title>
      <link>https://enze5088.github.io/content/article-1/</link>
      <guid>https://enze5088.github.io/content/article-1/</guid>
      <source url="https://enze5088.github.io/rss.xml">图文预训练模型总结</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="图文预训练模型总结"> 图文预训练模型总结</h1>
<h3 id="inductive-bias"> Inductive bias</h3>
<h4 id="_2-1-1-什么是-inductive-bias"> 2.1.1 什么是 Inductive bias</h4>
<p>Inductive bias 是指在通过人为偏好，认为某一种解决方案优先于其他解决方案。这里的解决方案既可以指数据假设上，也可以指模型设计等。在深度学习时代，卷积神经网络认为信息具有空间局部性，可以用滑动卷积共享权重方式降低参数空间和提高性能；循环神经网络强调时序信息时间顺序的重要性；图神经网络则是认为中心节点与邻居节点的相似性会更好引导信息流动。可以说深度学习时代，不同网络结构的创新就体现了不同的归纳性偏。</p>
<h4 id="_2-1-2-图像数据的-inductive-bias"> 2.1.2 图像数据的 Inductive bias</h4>
<ul>
<li>
<p>Local prior</p>
<p>图像具有 locality（局域性），例如一个像素与它邻近的像素更相关，与它远离的像素更不相关。</p>
</li>
<li>
<p>Global capacity</p>
<p>图像还具有 long-rangeS dependencies，例如一个像素与距离更远的像素同属于一个物体的相关性。</p>
</li>
<li>
<p>Positional prior</p>
<p>有些图像有 positional prior，例如人脸图片中，脸一般在图像中间位置，眼睛总是在脸部上方，嘴巴总是在脸部下方。</p>
</li>
</ul>
<h3 id="_2-2-cv任务的网络结构"> 2.2 CV任务的网络结构</h3>
<h4 id="_2-2-1-mlp"> 2.2.1 MLP</h4>
<p>MLP 的工作模式可以是这样的：将 feature map 展开成一维向量，通过 FC层，最终再 reshape 成原来 feature map 的形状。因为 FC 的参数的作用是与位置相关的，因此 FC 就有建模 positional prior 的能力。又因为输出 feature map 的每一个点都与输入 feature map 的每一个点有关，所以 FC 也有捕捉 long-range dependencies 的能力。</p>
<h4 id="_2-2-2-cnn"> 2.2.2 CNN</h4>
<p>CNN 通过手工设计卷积核尺寸、个数、stride 等超参数，进而在数据上自动学习卷积核参数。CNN 具有捕捉 local prior 的能力，在图像识别任务中取得成功。但是传统的 CNN 只能通过加深卷积层数、增大感受野来建模 long-range dependencies。这种建模 long-range dependencies 的模式效率较低，并且可能导致优化困难。</p>
<h4 id="_2-2-3-transformer"> 2.2.3 Transformer</h4>
<p>Transformer 最早被用于 NLP 任务。最近 ViT[3] 等论文将 Transformer 用于 CV，取得了成功。Transformer 中的 self-attention，具有 global capacity 和 positional prior，MLP-Blocks 具有 global capacity。但是由于 Vision Transformer 不具有建模 local prior 的能力，因此需要大量的训练数据。最近这些基于 Transformer 的研究表明，更长的训练时间、更多的参数、更多的数据和或更多的正则化，就足以恢复像 ImageNet 分类这样复杂任务的重要先验。</p>
<h3 id="_3-图像-文本多模态预训练模型"> <strong>3. 图像-文本多模态预训练模型</strong></h3>
<h4 id="_3-1-cross-stream"> <strong>3.1 Cross-Stream</strong></h4>
<h5 id="_3-1-1-vilbert-1"> <strong>3.1.1 ViLBERT[1]</strong></h5>
<p><strong>模型细节</strong></p>
<p>输入的文本经过文本Embedding层后被输入到文本的单模态Transformer编码器中提取上下文信息。使用预训练Faster R-CNN对于图片生成候选区域提取特征并送入图像Embedding层生成Embedding。然后将获取好的文本和图像的Embedding通过Co-attention-transformer模块进行相互交互融合，得到最后的表征。</p>
<p>ViLBERT模型图如图1所示，Co-attention-transformer模块如图2所示。</p>
<p>图1 ViLBERT模型图</p>
<p>图2 Co-attention transformer模块</p>
<p><strong>预训练任务</strong></p>
<p><strong>掩蔽文本预测（masked multi-modal modelling）</strong> 与BERT的设计思路一样，根据概率随机替换掉文本中部分词，使用[MASK]占位符替代，需要模型通过文本中上下文，以及对应图片中给出的信息，预测出被替换的词。</p>
<p><strong>掩蔽图像预测（masked multi-modal modelling）</strong> 通过掩蔽经过Faster R-CNN提取到的预候选区域，使模型通过对应文本以及其他区域的图像预测出被遮掩区域的类别。</p>
<p><strong>图片-文本对齐（multi-modal alignment）</strong> 给定构造好的图文关系对，让模型来判断文本是否是对应图片的描述，具体是使用<code>&lt;IMG&gt;</code>以及<code>&lt;CLS&gt;</code>表示来判断文本是否是对于图像的描述。</p>
<p><strong>下游任务</strong></p>
<p>作者将该模型应用到视觉问答（Visual Question Answering）、视觉常识推理（Visual Commonsense Reasoning）、指示表达定位（Grounding Referring Expressions）、图像检索（Caption-Based Image Retrieval）等下游任务上，并且取得了较好的结果。</p>
<h5 id="_3-1-2-lxmert-2"> <strong>3.1.2 LXMERT[2]</strong></h5>
<p><strong>模型细节</strong></p>
<p>类似于ViLBERT，对于文本和图像经过Embedding层之后被送入各自的单模态编码器，然后通过跨模态编码器进行融合。</p>
<p>图3 LXMERT模型图</p>
<p><strong>预训练任务</strong></p>
<p><strong>掩蔽文本预测（Masked Cross-Modality LM）</strong> 该任务的设置与BERT的MLM任务设置一致。作者认为除了从语言模态中的非模态词中预测被掩蔽词外，LXMERT还可利用其跨模态模型架构，从视觉模态中预测被掩蔽词，从而解决歧义问题，所以将任务命名为Masked Cross-Modality LM以强调这种差异。</p>
<p>**掩蔽图像类别预测（Detected-Label Classification）**该任务要求模型根据图像线索以及对应文本线索预测出直接预测被遮蔽ROI的目标类别。</p>
<p>**掩码图像特征回归（RoI-Feature Regression）**不同于类别预测，该任务以L2损失回归预测目标ROI特征向量。</p>
<p><strong>图片-文本对齐（Cross-Modality  Matching）</strong> 通过50%的概率替换图片对应的文本描述，使模型判断图片和文本描述是否是一致的。</p>
<p><strong>图像问答（Image Question Answering）</strong> 作者使用了有关图像问答的任务，训练数据是关于图像的文本问题。当图像和文本问题匹配时，要求模型预测这些图像有关的文本问题的答案。</p>
<p>图4 LXMERT预训练任务</p>
<p>作者将该模型在多个下游任务上进行了测试，分别在视觉问答任务（Visual Question Answering）、面向现实世界视觉推理（Visual Reasoning in the Real World）等取得了很好的效果。</p>
<h5 id="_3-1-3-ernie-vil-3"> <strong>3.1.3 ERNIE-ViL[3]</strong></h5>
<p><strong>模型细节</strong></p>
<p>模型结构采用双流架构，对于图像和文本分别使用单模编码器进行编码然后使用跨模态Transformer实现两个模态的信息交融。值得一提的是该模型引入了场景图信息，通过将场景图知识融入多模态预训练中，使得模型更能精准把握图像和文本之间细粒度的对齐信息。模型图如图5所示。</p>
<p>图5 RNIE-ViL模型图</p>
<p>图6 场景图实例</p>
<p>模型在预训练任务中融入了场景图（如图6所示）的信息。场景图中有目标（objects）、属性（attributes）、关系（relationships）三种类别。</p>
<p><strong>预训练任务</strong></p>
<p>**场景图预测（Scene Graph Prediction）**根据给定的一段文本解析出场景图结构，然后根据解析出的场景图设计了三个子任务，分别是目标预测（object prediction）、属性预测（attribute prediction）、关系预测（relationship prediction），通过掩蔽图像和文本中场景图解析出来的目标、属性以及关系，使用模型进行预测，以让模型学习到跨模态之间的细粒度语义对齐信息。</p>
<p>同时模型还使用了传统的预训练任务，分别是<strong>掩蔽文本预测（Masked Cross-Modality LM）</strong>、<strong>掩蔽图像类别预测（Detected-Label Classification）</strong>，以及<strong>图片-文本对齐（Cross-Modality  Matching）</strong>。</p>
<p><strong>下游任务</strong></p>
<p>作者在下游多个任务上进行检测都取得了比较大的提升，具体有视觉常识推理（Visual Commonsense Reasoning）、视觉问答（Visual Question Answering）、图像检索（Image Retrieval）、文本检索（Text Retrieval）、指示表达定位（Grounding Referring Expressions）。</p>
<h4 id="_3-2-single-stream"> 3.2 Single-Stream</h4>
<h5 id="_3-2-1-vl-bert-4"> <strong>3.2.1 VL-BERT[4]</strong></h5>
<p><strong>模型细节</strong></p>
<p>图7 VL-BERT模型图</p>
<p>模型架构与BERT相似，如图7所示。整个模型的输入有四部分embedding。</p>
<p>**Token embedding层：**对于文本内容使用原始BERT的设定，但是添加了一个特殊符[IMG]作为图像的token。</p>
<p>**Visual feature embedding层：**这层是为了嵌入视觉信息新添加的层。该层由视觉外部特征以及视觉几何特征拼接而成，具体而言，对于非视觉部分的输入是整个图像的提取到的特征，对应于视觉部分的输入即为图像经过预训练之后的Faster R-CNN提取到的ROI区域图像的相应视觉特征。</p>
<p>**Segment embedding层：**模型定义了A、B、C三种类型的标记，为了指示输入来自于不同的来源，A、B指示来自于文本，分别指示输入的第一个句子和第二个句子，更进一步的，可以用于指示QA任务中的问题和答案；C指示来自于图像。</p>
<p>**Position embedding层：**与BERT类似，对于文本添加一个可学习的序列位置特征来表示输入文本的顺序和相对位置。对于图像，由于图像没有相对的位置概念，所以图像的ROI特征的位置特征都是相同的。</p>
<p>作者在视觉-语言数据集以及纯语言数据集上都进行了大规模的预训练，使用概念标题数据库（Conceptual Captions）数据集作为视觉-语言语料库，该数据集包含了大约330万张带有标题注释的图片，图片来自于互联网。但是这个数据集存在一个问题就是图像对应的标题是简短的句子，这些句子很短并且很简单，为了避免模型只关注于简单子句，作者还使用了BooksCorpus和英语维基百科数据集进行纯文本的训练。</p>
<p><strong>预训练任务</strong></p>
<p><strong>掩蔽文本预测（Masked Language Model with visual Clues）</strong> 此任务与BERT中使用的Masked Language Modeling（MLM）任务非常相似。关键区别在于，在VL-BERT中包含了视觉线索，以捕获视觉和语言内容之间的依存关系。</p>
<p><strong>掩蔽图像类别预测（Masked RoI Classification with Linguistic Clues）</strong> 类似于掩蔽文本预测，每个RoI图像以15%的概率被随机掩蔽，训练的任务是根据其他线索预测被掩藏的RoI的类别标签。值得一提的是为了避免由于其他元素的视觉特征的嵌入导致视觉线索的泄漏，在使用Faster R-CNN之前，需要先将被Mask的目标区域的像素置零。</p>
<p><strong>下游任务</strong></p>
<p>作者将模型应用于视觉常识推理（Visual Commonsense Reasoning）、视觉问答（Visual Question Answering）、引用表达式理解（Referring Expression Comprehension）任务，并且都取得了显著的效果。</p>
<h5 id="_3-2-2-image-bert-5"> <strong>3.2.2 Image-BERT[5]</strong></h5>
<p><strong>模型细节</strong></p>
<p>图8 Image-BERT模型图</p>
<p>ImageBERT在图像Embedding层添加了图像位置编码，即将通过Faster R-CNN得到的物体对应的ROI区域相对于全局图的位置信息，编码为五维向量，作为位置编码添加进图像的特征表示中。</p>
<p><strong>预训练任务</strong></p>
<p><strong>掩蔽文本预测（Masked Language Modeling）</strong> 此任务与BERT中使用的Masked Language Modeling（MLM）任务设定基本一致。</p>
<p><strong>掩蔽图像类别预测（Masked Object Classification）</strong> 此任务是MLM任务的扩展。与语言建模类似，通过对视觉对象进行掩蔽建模，期望模型预测出被掩蔽的图像token的类别。</p>
<p><strong>掩蔽图像特征回归（Masked Region Feature Regression）</strong> 该任务旨在预测被掩蔽的视觉对象的嵌入特征。通过在相应位置的输出特征向量后添加一个全连接层，以将其投影到与原始RoI对象特征相同的维度上，然后应用L2损失来进行回归。</p>
<p><strong>图片-文本对齐（Image-Text Matching）</strong> 除了语言建模任务和视觉内容建模任务之外，作者还添加了图片-文本对齐任务以学习图像-文本对齐。对于每个训练样本，对每个图像随机抽取负例句子，对每个句子随机抽取负例图像以生成负例训练数据，让模型判断给定的图像文本对是否对应。</p>
<p><strong>下游任务</strong></p>
<p>作者在MSCOCO以及Filcker30k数据上分别测试模型在图像检索（Image Retrieval）以及文本检索（Sentence Retrieval）任务上的性能，取得了一定的提升。</p>
<p>表1 图像-文本预训练模型概览表</p>
]]></content:encoded>
    </item>
    <item>
      <title>对比学习论文列表</title>
      <link>https://enze5088.github.io/content/article-2/</link>
      <guid>https://enze5088.github.io/content/article-2/</guid>
      <source url="https://enze5088.github.io/rss.xml">对比学习论文列表</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="contrastive-learning-and-pre-trained-model"> Contrastive Learning and Pre-trained model</h1>
<p>对比学习、预训练模型等总结</p>
<h2 id="contrastive-learning-paper-list"> Contrastive Learning Paper List</h2>
<p>论文总结</p>
<h3 id="natural-language-processing"> Natural Language Processing</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Title</th>
<th>Conference</th>
<th>Codes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://arxiv.org/abs/2104.08821" target="_blank" rel="noopener noreferrer">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a></td>
<td></td>
<td>[<a href="https://github.com/princeton-nlp/SimCSE" target="_blank" rel="noopener noreferrer">Torch</a>]</td>
</tr>
<tr>
<td style="text-align:left"><a href="http://proceedings.mlr.press/v119/wang20k/wang20k.pdf" target="_blank" rel="noopener noreferrer">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</a></td>
<td>ICML2020</td>
<td><a href="https://github.com/SsnL/align_uniform" target="_blank" rel="noopener noreferrer">[Torch]</a></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://arxiv.org/abs/2105.11741" target="_blank" rel="noopener noreferrer">ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</a></td>
<td>ACL 2021</td>
<td><a href="https://github.com/yym6472/ConSERT" target="_blank" rel="noopener noreferrer">github</a></td>
</tr>
</tbody>
</table>
<h3 id="computer-vision"> Computer Vision</h3>
<table>
<thead>
<tr>
<th>Title</th>
<th>Conference</th>
<th>Codes</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/pdf/1911.05722.pdf" target="_blank" rel="noopener noreferrer">MoCo - Momentum Contrast for Unsupervised Visual Representation Learning</a></td>
<td>CVPR20</td>
<td><a href="https://github.com/facebookresearch/moco" target="_blank" rel="noopener noreferrer">Torch(official)</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/2003.04297.pdf" target="_blank" rel="noopener noreferrer">MoCo v2 - Improved Baselines with Momentum Contrastive Learning</a></td>
<td></td>
<td><a href="https://github.com/facebookresearch/moco" target="_blank" rel="noopener noreferrer">Torch(official)</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/2002.05709.pdf" target="_blank" rel="noopener noreferrer">SimCLR - A Simple Framework for Contrastive Learning of Visual Representations</a></td>
<td>ICML20</td>
<td><a href="https://github.com/google-research/simclr" target="_blank" rel="noopener noreferrer">TF(official)</a>, <a href="https://github.com/PatrickHua/SimSiam/blob/main/models/simclr.py" target="_blank" rel="noopener noreferrer">Torch</a></td>
</tr>
<tr>
<td><a href="">SimCLR v2 - Big Self-Supervised Models are Strong Semi-Supervised Learners</a></td>
<td>NIPS20</td>
<td><a href="https://github.com/google-research/simclr" target="_blank" rel="noopener noreferrer">TF(official)</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/2006.07733.pdf" target="_blank" rel="noopener noreferrer">BYOL - Bootstrap your own latent: A new approach to self-supervised Learning</a></td>
<td></td>
<td><a href="https://github.com/deepmind/deepmind-research/tree/master/byol" target="_blank" rel="noopener noreferrer">JAX(official)</a>, <a href="https://github.com/PatrickHua/SimSiam/blob/main/models/byol.py" target="_blank" rel="noopener noreferrer">Torch</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/2006.09882.pdf" target="_blank" rel="noopener noreferrer">SwAV - Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</a></td>
<td>NIPS20</td>
<td><a href="https://github.com/facebookresearch/swav" target="_blank" rel="noopener noreferrer">Torch(official)</a>, <a href="https://github.com/PatrickHua/SimSiam/blob/main/models/swav.py" target="_blank" rel="noopener noreferrer">Torch</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/2011.10566.pdf" target="_blank" rel="noopener noreferrer">SimSiam - Exploring Simple Siamese Representation Learning</a></td>
<td></td>
<td><a href="https://github.com/PatrickHua/SimSiam/blob/main/models/simsiam.py" target="_blank" rel="noopener noreferrer">Torch</a></td>
</tr>
</tbody>
</table>
<h3 id="other"> Other</h3>
<table>
<thead>
<tr>
<th>Title</th>
<th>Conference</th>
<th>Codes</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/joshr17/HCL" target="_blank" rel="noopener noreferrer">[HCL] - Contrastive Learning With Hard Negative Samples</a></td>
<td>ICLR 2021</td>
<td><a href="https://github.com/joshr17/HCL" target="_blank" rel="noopener noreferrer">Link</a></td>
</tr>
</tbody>
</table>
]]></content:encoded>
    </item>
    <item>
      <title>图表示学习中的对比学习</title>
      <link>https://enze5088.github.io/content/article-3/</link>
      <guid>https://enze5088.github.io/content/article-3/</guid>
      <source url="https://enze5088.github.io/rss.xml">图表示学习中的对比学习</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="图表示学习中的对比学习"> 图表示学习中的对比学习</h1>
]]></content:encoded>
    </item>
    <item>
      <title>Transformer面经总结</title>
      <link>https://enze5088.github.io/content/article-4/</link>
      <guid>https://enze5088.github.io/content/article-4/</guid>
      <source url="https://enze5088.github.io/rss.xml">Transformer面经总结</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="transformer面经总结"> Transformer面经总结</h1>
<h2 id="高频提问"> 高频提问</h2>
<ol>
<li>
<p>介绍一下Transformer的原理。</p>
<p><strong>延伸提问</strong></p>
<ol>
<li>Encoder中的Feed Forward的结构是如何的?使用了什么激活函数？相关优缺点？</li>
<li>讲一下Transformer中的残差结构以及意义？</li>
<li>Encoder端和Decoder端是如何进行交互的？</li>
<li>Transformer为什么用+不用concat？</li>
<li>wordpiece的作用是什么，简单描述一下wordpiece model 和 byte pair encoding？</li>
</ol>
</li>
<li>
<p>Transformer为什么可以并行？</p>
<p><strong>延伸提问</strong></p>
<ol>
<li>Decoder端可以做并行化吗？</li>
<li>LSTM时间复杂度如何计算，Transformer 时间复杂度如何计算？</li>
</ol>
</li>
<li>
<p>Transformer的position embedding和BERT的position embedding的区别？</p>
<p><strong>延伸提问</strong></p>
<ol>
<li>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</li>
</ol>
</li>
<li>
<p>Transformer里面LayerNorm的作用？</p>
<p><strong>延伸提问</strong></p>
<ol>
<li>为什么transformer块使用LayerNorm而不是BatchNorm？</li>
<li>LayerNorm 在Transformer的位置是哪里？</li>
<li>简答讲一下BatchNorm技术，以及它的优缺点。</li>
</ol>
</li>
<li>
<p>Transformer里多头注意力机制/自注意力机制的计算过程是怎样的？</p>
<p><strong>延伸提问</strong></p>
<ol>
<li>自注意力的计算公式是怎样的？</li>
<li>Multi-Head Attention是什么，有什么作用？</li>
<li>为什么在进行softmax之前需要对attention进行scaled/Attention计算时为什么要除根dk，q\k\v分别是如何算的？</li>
<li>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？加性、乘性attention的公式？</li>
<li>Transformer的中的attention机制，其中self-attention和encoder-decoder attention之间的关系？</li>
<li>bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</li>
<li>self-attention代码实现？</li>
</ol>
</li>
<li>
<p>Transformer的结构，作为特征处理器它跟LSTM、CNN的主要区别在哪里？各有哪些优缺点？</p>
<p><strong>延伸提问</strong></p>
<ol>
<li>Transformer相对于传统的RNN网络有什么好处？</li>
<li>Transformer相对RNN为什么能避免梯度消失？</li>
<li>Transformer与Bert，GPT的联系与区别？</li>
<li>Transformer XL和Transformer的主要区别是什么？XLNet有哪些突出的有点，有哪些创新的地方？</li>
<li>Transformer的计算代价瓶颈在哪里？</li>
</ol>
</li>
</ol>
<h2 id="原始参考"> 原始参考</h2>
<p>1.<a href="https://www.nowcoder.com/discuss/648356" target="_blank" rel="noopener noreferrer">https://www.nowcoder.com/discuss/648356</a></p>
<p>transformer介绍一下原理，transformer为什么可以并行，它的计算代价瓶颈在哪？多头注意力机制计算过程？</p>
<p>BERT介绍一下原理，怎么用BERT计算文本相似度，有哪两种计算方法？（我回答的是，第一种方法是两个文本拼接作为BERT的输入，顶层加一个sigmod函数，第二种是，两个文本分别输入到BERT，得到特征向量，然后计算余弦相似度。）这两种方法的复杂度哪个高？</p>
<p>transformer（果然是nlp面试必考），介绍transformer里自注意力机制的计算过程，为什么要进行缩放，介绍下bert位置编码和transformer的区别，哪个好，为什么（为什么。。这个是真不会，我也如实说了不会，面试官说，开放题，说说你的理解，我就说通过学习出来的，可能会过拟合，也可能会学到更细微的特征）</p>
<p>2.<a href="https://www.nowcoder.com/discuss/648119" target="_blank" rel="noopener noreferrer">https://www.nowcoder.com/discuss/648119</a></p>
<p>你认为Transformer同LSTM这些有什么区别和关系?</p>
<p>transformer的position embedding和BERT的position embedding的区别.</p>
<p>了解seq2seq吗?有没有用过对应的transformer进行对应的使用项目?</p>
<p>3.<a href="https://www.nowcoder.com/discuss/641848" target="_blank" rel="noopener noreferrer">https://www.nowcoder.com/discuss/641848</a></p>
<p>介绍transformer（从encode端到decode端完整的说了一遍</p>
<p>bert介绍（跟transformer差不多把，就是多了两个预训练任务</p>
<p>4.<a href="https://www.nowcoder.com/discuss/639224" target="_blank" rel="noopener noreferrer">https://www.nowcoder.com/discuss/639224</a></p>
<p>说一下BERT和transformer的positional embedding有啥区别（多了一个矩阵，多了一个dropout几里哇啦）</p>
<p>5.<a href="https://zhuanlan.zhihu.com/p/266540739" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/266540739</a></p>
<p>你讲下Transformer里面LayerNorm的作用？</p>
<p>Encoder中的Feed Forward?</p>
<p>由两层全连接层构成，第一层全连接层的W的维度为[3072，768]，bias的维度为[3072]；第二层的全连接层的W的维度为[768,3072]，bias的维度为[768]。输出再经过Gelu激活函数，就得到了FeedForward的输出。</p>
<p>6.<a href="https://blog.csdn.net/qq_40092110/article/details/109247383" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_40092110/article/details/109247383</a></p>
<p>Transformer模型原理和它的结构；</p>
<p>Multi-Head Attention是什么，有什么作用？multi head什么意思，attention计算时为什么要除根dk，q\k\v分别是如何算的</p>
<p>attention和self-attention</p>
<p>self-attention如何实现的，介绍了一下原理，面试官又问代码具体如何实现</p>
<p>7.<a href="https://www.nowcoder.com/discuss/486194" target="_blank" rel="noopener noreferrer">https://www.nowcoder.com/discuss/486194</a></p>
<p>bert 为什么scale product，transformer里encoder的什么部分输入给decoder， MLM 为什么mask一部分保留一部分， albert，roberta， electra做了什么改进</p>
<p>8.<a href="https://blog.csdn.net/weixin_40920183/article/details/107777228" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/weixin_40920183/article/details/107777228</a></p>
<p>Transformer与GPT的联系与区别</p>
<p>9.<a href="https://zhuanlan.zhihu.com/p/153333432" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/153333432</a></p>
<p>Transformer的结构，作为特征处理器它跟LSTM的主要区别在哪里？各有哪些优缺点？</p>
<p>Transformer XL和Transformer的主要区别是什么？XLNet有哪些突出的有点，有哪些创新的地方？</p>
<p>10.<a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1620807181&amp;ver=3063&amp;signature=SYGLpJUrt0gwGWRtN48oTtCOWxfBOtSRvZuwTBflU8rXZW4ToIKjj-HWiyMATUrVyDS6EW4ezMsM2-RqiOX6CvGSJhzW4-f2oEh1i9RRO8zpXlQkAp5iz9XtVjviY92V&amp;new=1" target="_blank" rel="noopener noreferrer">链接</a></p>
<p>bert如何使用transformer的encoding模块-bert的输入和transformer有什么不同</p>
<p>transformer的中的attention机制，其中self-attention和encoder-decoder attention之间的关系</p>
<p>transformer为什么用+不用concat</p>
<p>已有总结文章</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/149799951" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/149799951</a>
<ol>
<li>Transformer为何使用多头注意力机制？（为什么不使用一个头）</li>
<li>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</li>
<li>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</li>
<li>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</li>
<li>在计算attention score的时候如何对padding做mask操作？</li>
</ol>
</li>
<li>https://zhuanlan.zhihu.com/p/363466672
<ol>
<li>为什么在进行多头注意力的时候需要对每个head进行降维？</li>
<li>大概讲一下Transformer的Encoder模块？</li>
<li>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？</li>
<li>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</li>
<li>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</li>
<li>简单讲一下Transformer中的残差结构以及意义。</li>
<li>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</li>
<li>简答讲一下BatchNorm技术，以及它的优缺点。</li>
<li>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</li>
<li>Encoder端和Decoder端是如何进行交互的？</li>
<li>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</li>
<li>Transformer的并行化提现在哪个地方？</li>
<li>Decoder端可以做并行化吗？</li>
<li>简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</li>
<li>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</li>
</ol>
</li>
<li><a href="https://zhuanlan.zhihu.com/p/151412524" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/151412524</a>
<ol>
<li>wordpiece的作用</li>
<li>self-attention相比lstm优点是什么？</li>
</ol>
</li>
<li><a href="https://zhuanlan.zhihu.com/p/129409553" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/129409553</a>
<ol>
<li>Transformer相对于传统的RNN网络有什么好处；</li>
<li>Transformer里的Self-Attention作用是什么，有什么优势；</li>
<li>你提到了梯度消失的问题，那么Transformer相对RNN为什么能避免梯度消失；</li>
</ol>
</li>
<li>https://zhuanlan.zhihu.com/p/359555994
<ol>
<li>LSTM时间复杂度，Transformer 时间复杂度
<ul>
<li>LSTM时间复杂度：序列长度*向量长度²</li>
<li>transformer时间复杂度：序列长度²*向量长度</li>
</ul>
</li>
</ol>
</li>
<li><a href="https://www.zhihu.com/question/318355038" target="_blank" rel="noopener noreferrer">bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</a></li>
</ol>
<p>Transformer的position embedding和BERT的position embedding的区别？</p>
<p>答：Transformer在编码词向量时引入了位置编码（Position Embedding）的特征。其编码公式如下：</p>
<p><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.05764em;">PE</span><span>(</span><span>p</span><span>os</span><span>,</span><span style="margin-right:0.1667em;"></span><span>2</span><span>i</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:2.4em;vertical-align:-0.95em;"></span><span>sin</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>(</span></span><span><span></span><span><span><span><span style="height:0.7475em;"><span style="top:-2.19em;"><span style="height:3.0395em;"></span><span><span><span>1000</span><span><span>0</span><span><span><span><span style="height:1.485em;"><span style="top:-3.7375em;margin-right:0.0714em;"><span style="height:3em;"></span><span><span><span><span></span><span><span><span><span style="height:1.0465em;"><span style="top:-2.468em;"><span style="height:3em;"></span><span><span><span>d</span><span><span><span><span style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span style="height:2.6944em;"></span><span><span><span>model </span></span></span></span></span><span>​</span></span><span><span style="height:0.3496em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.2255em;"><span style="height:3em;"></span><span style="border-bottom-width:0.049em;"></span></span><span style="top:-3.387em;"><span style="height:3em;"></span><span><span>2</span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.8816em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.2695em;"><span style="height:3.0395em;"></span><span style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4856em;"><span style="height:3.0395em;"></span><span><span><span>p</span><span>os</span></span></span></span></span><span>​</span></span><span><span style="height:0.8495em;"><span></span></span></span></span></span><span></span></span><span style="top:0em;"><span>)</span></span></span></span></span></span>
<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.05764em;">PE</span><span>(</span><span>p</span><span>os</span><span>,</span><span style="margin-right:0.1667em;"></span><span>2</span><span>i</span><span style="margin-right:0.2222em;"></span><span>+</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:1em;vertical-align:-0.25em;"></span><span>1</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:2.4em;vertical-align:-0.95em;"></span><span>cos</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>(</span></span><span><span></span><span><span><span><span style="height:0.7475em;"><span style="top:-2.19em;"><span style="height:3.0395em;"></span><span><span><span>1000</span><span><span>0</span><span><span><span><span style="height:1.485em;"><span style="top:-3.7375em;margin-right:0.0714em;"><span style="height:3em;"></span><span><span><span><span></span><span><span><span><span style="height:1.0465em;"><span style="top:-2.468em;"><span style="height:3em;"></span><span><span><span>d</span><span><span><span><span style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span style="height:2.6944em;"></span><span><span><span>model </span></span></span></span></span><span>​</span></span><span><span style="height:0.3496em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.2255em;"><span style="height:3em;"></span><span style="border-bottom-width:0.049em;"></span></span><span style="top:-3.387em;"><span style="height:3em;"></span><span><span>2</span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.8816em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.2695em;"><span style="height:3.0395em;"></span><span style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4856em;"><span style="height:3.0395em;"></span><span><span><span><span> pos </span></span></span></span></span></span><span>​</span></span><span><span style="height:0.8495em;"><span></span></span></span></span></span><span></span></span><span style="top:0em;"><span>)</span></span></span></span></span></span></p>
<p><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.625em;vertical-align:-0.1944em;"></span><span>p</span><span>os</span></span></span></span>示单词的位置， <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6595em;"></span><span>i</span></span></span></span>表示单词的维度。关于位置编码的实现可在Google开源的算法中<code>get_timing_signal_1d()</code>函数找到对应的代码。</p>
<p>作者这么设计的原因是考虑到在NLP任务中，除了单词的绝对位置，单词的相对位置也非常重要。根据公式 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span>sin</span><span>(</span><span style="margin-right:0.0037em;">α</span><span style="margin-right:0.2222em;"></span><span>+</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.05278em;">β</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>sin</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.0037em;">α</span><span style="margin-right:0.1667em;"></span><span>cos</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.05278em;">β</span><span style="margin-right:0.2222em;"></span><span>+</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>cos</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.0037em;">α</span><span style="margin-right:0.1667em;"></span><span>sin</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.05278em;">β</span></span></span></span>以及<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span>cos</span><span>(</span><span style="margin-right:0.0037em;">α</span><span style="margin-right:0.2222em;"></span><span>+</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.05278em;">β</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>cos</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.0037em;">α</span><span style="margin-right:0.1667em;"></span><span>cos</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.05278em;">β</span><span style="margin-right:0.2222em;"></span><span>−</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>sin</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.0037em;">α</span><span style="margin-right:0.1667em;"></span><span>sin</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.05278em;">β</span></span></span></span> ，这表明位置 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.7778em;vertical-align:-0.0833em;"></span><span style="margin-right:0.03148em;">k</span><span style="margin-right:0.2222em;"></span><span>+</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:0.625em;vertical-align:-0.1944em;"></span><span>p</span></span></span></span> 的位置向量可以表示为位置  <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6944em;"></span><span style="margin-right:0.03148em;">k</span></span></span></span>的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p>
<h2 id="all-attentions-in-transformer"> All attentions in Transformer</h2>
<p>为了深入理解transformer中attention的应用。我们不妨先来回归一下 transformer的模型结构。套用这张经典的图片，可以看到，transformer采用的是典型的encoder-decoder架构。</p>
<p>而encoder和decoder又是由什么组成的呢？</p>
<p>对于公式(1)其实很好理解，注意力公式主要就是算 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6833em;"></span><span style="margin-right:0.22222em;">V</span></span></span></span> 的加权后的表示，说到加权，必要得有权重啊。权重就是前面的<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span><span>softmax</span></span><span>(</span><span>∗</span><span>)</span></span></span></span>部分，为什么要加<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6944em;"></span><span><span>softmax</span></span></span></span></span> ，因为权重必须为概率分布即和为1。  里面部分<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.6275em;vertical-align:-0.538em;"></span><span><span></span><span><span><span><span style="height:1.0895em;"><span style="top:-2.5864em;"><span style="height:3em;"></span><span><span><span><span><span><span style="height:0.8622em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span><span>d</span><span><span><span><span style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span style="height:2.5em;"></span><span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span><span>​</span></span><span><span style="height:0.1512em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1778em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span style="height:3em;"></span><span style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span style="height:3em;"></span><span><span><span>Q</span><span><span style="margin-right:0.07153em;">K</span><span><span><span><span style="height:0.9191em;"><span style="top:-2.931em;margin-right:0.0714em;"><span style="height:2.5em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span style="height:0.538em;"><span></span></span></span></span></span><span></span></span></span></span></span>算的就是注意力的原始分数，通过计算Q(query)与K(key)的点积得到相似度分数，其中<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.04em;vertical-align:-0.1828em;"></span><span><span><span><span style="height:0.8572em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1828em;"><span></span></span></span></span></span></span></span></span>起到一个调节作用，不至于过大或过小，导致 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6944em;"></span><span><span>softmax</span></span></span></span></span> 之后就非0即1。因此这种注意力的形式也叫缩放点积注意力机制。</p>
<h3 id="谈一谈decoder模块"> 谈一谈Decoder模块</h3>
<p>本文主要是谈一些比较容易误解的细节点，说实话，把自己的理解用文字表达出来真是个细致活。</p>
<p>如果觉得对您有点帮助，帮忙点个在看或者赞。</p>
<h4 id="一个小小的问题"> 一个小小的问题</h4>
<p>我先说一个自己花了点时间才琢磨出来的东西，其实不难，就是当时没转过弯来。</p>
<p>我们都知道，decoder的交互层，Q矩阵来自本身，K/V矩阵来自整个Encoder层输出。</p>
<p>但是对于每个单词都会有一个encoder的输出，那么K/V矩阵是用的其中哪个输出计算过来的？</p>
<p>我这个问题的问法其实是错误的。</p>
<p>我当时的理解背景是认为这个交互的过程很类似seq2seq的attention，它一般是使用最后一个时刻的隐层输出作为context vector。</p>
<p>我基于此产生了上面这个问题，这个K/V矩阵应该由哪个位置单词（对比RNN就是哪个时刻的单词）的输出生成。</p>
<p>后来看了一下代码，才明白自己错在哪里？</p>
<p>K/V矩阵的计算不是来自于某一个单词的输出，而是所有单词的输出汇总计算K/V矩阵。这个过程和在Encoder中计算K/V矩阵是一样的，只不过放在了交互层，一时没想明白。</p>
<h4 id="正文"> 正文</h4>
<p>与Encoder很类似，Decoder同样由完全相同的N个大模块堆叠而成，原论文中N为6。</p>
<p>每个大的模块分为三部分：多头注意力层，交互层，前馈神经层；每个层内部尾端都含有 Add&amp;Norm。</p>
<p>和Encoder重复的内容我就跳过了，之前讲过，没看过的同学可以去看那个文章。</p>
<h5 id="多头自注意力层"> 多头自注意力层</h5>
<p>首先谈一下多头自注意力层，这里需要注意的细节点是，需要对当前单词和之后的单词做mask。</p>
<p>为什么需要mask？</p>
<p>最常规的解释就是在预测阶段，你的模型看不见当前时刻的输出以及未来时刻单词。</p>
<p>这句话其实有点绕，如果读的不仔细会让人误解为mask的时候需要把当前时刻的单词也mask掉...(拖出去斩了吧)。</p>
<p>从代码角度讲，你只需要把当前时刻之后所有单词mask掉就好了。</p>
<p>我自己对这句话的理解是我们需要确保模型在训练和测试的时候没有GAP。</p>
<p>举个简单的例子来理解，如果做机器翻译，你需要翻译出来的句子是 &quot;我/爱/吃/苹果&quot;。</p>
<p>当前时刻是”爱“这个单词作为输入的一部分，另一部分是上一个时刻”我“作为输入的时候的输出值。</p>
<p>当然在机器翻译中，我们一般使用 teacher forcing加速收敛，所以这里就使用”我“作为当前时刻输入的另一个部分。</p>
<p>所以这个时候，输入就是”我“的编码信息和”爱“的编码信息（当然还有位置编码）。</p>
<p>我要预测的是”吃“这个单词。</p>
<p>如果我们没有mask，模型也是可以运行的，也就说此时”吃“和”苹果“两个词对”爱“这个时刻的输出是有贡献的。</p>
<p>那么问题来了，测试数据中你根本没有ground truth，你怎么办？</p>
<p>也就说，训练的时候，你的模型是基于知道这个时刻后面的单词进行的训练，但是测试的时候，做机器翻译，你不知道自己应该翻译出来什么东西。</p>
<p>这就是问题的核心。</p>
<p>你训练模型的时候，一部分精力花在了”吃“和”苹果“两个词上，这不就是无用功吗？</p>
<p>所以，确保模型在训练和测试的时候没有GAP，我们需要mask掉”吃“和”苹果“两个词。</p>
<h5 id="交互模块"> 交互模块</h5>
<p>这一块需要注意的就是之前文章提到的，Q矩阵来自本身，K/V矩阵来自encoder的输出。</p>
<p>还有一个细节点是，K/V矩阵对应的是来自整个encoder的输出。</p>
<p>如果看transformer那个经典图的话，初期很容易理解为encoder和decode对应的每一层互相交互，这是不对的。</p>
<p>是整个输出与decoder做交互。</p>
<p>对于任意的<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.9805em;vertical-align:-0.2861em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>s</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.05724em;">j</span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>,以某种attention计算两者之间的分数，再将所有的结果形成的魏进行归一化。</p>
<p>Attention 机制计算过程大致可以分成三步：</p>
<blockquote>
<p>① 信息输入：将 Q，K，V 输入模型
用 <img src="https://www.zhihu.com/equation?tex=X%3D[x_1%2Cx_2%2C...x_n]" alt="[公式]" loading="lazy"> 表示输入权重向量</p>
<p>② 计算注意力分布 α：通过计算 Q 和 K 进行点积计算相关度，并通过 softmax 计算分数
另 <img src="https://www.zhihu.com/equation?tex=Q%3DK%3DV%3DX" alt="[公式]" loading="lazy">，通过 softmax 计算注意力权重，<img src="https://www.zhihu.com/equation?tex=α_i%3Dsoftmax(s(k_i%2Cq))%3Dsoftmax(s(x_i%2C+q))" alt="[公式]" loading="lazy"></p>
<p>我们将 <img src="https://www.zhihu.com/equation?tex=α_i" alt="[公式]" loading="lazy"> 称之为注意力概率分布，<img src="https://www.zhihu.com/equation?tex=s(x_i%2C+q)" alt="[公式]" loading="lazy"> 为注意力打分机制，常见的有如下几种：
加性模型：<img src="https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3Dv^Ttanh(Wx_i%2BUq)" alt="[公式]" loading="lazy">
点积模型：<img src="https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3Dx_i^Tq" alt="[公式]" loading="lazy">
缩放点积模型：<img src="https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3D{x_i^Tq}%2F\sqrt{d_k}" alt="[公式]" loading="lazy">
双线性模型：<img src="https://www.zhihu.com/equation?tex=s(x_i%2Cq)%3Dx_i^TWq" alt="[公式]" loading="lazy"></p>
<p>③ 信息加权平均：注意力分布 <img src="https://www.zhihu.com/equation?tex=α_i" alt="[公式]" loading="lazy"> 来解释在上下文查询 <img src="https://www.zhihu.com/equation?tex=q_i" alt="[公式]" loading="lazy"> 时，第 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]" loading="lazy"> 个信息受关注程度。
<img src="https://www.zhihu.com/equation?tex=att(q%2CX)%3D\sum_{i%3D1}^N{α_iX_i}" alt="[公式]" loading="lazy"></p>
</blockquote>
<h2 id="attention的通用定义如下"> Attention的通用定义如下：</h2>
<p>给定一组向量集合values，以及查询向量query，我们根据query向量去计算values加权和，即成为attention机制。</p>
<p>attention的重点即为求这个集合values中每个value的权值。我们也称attention的机制叫做query的输出关注了（考虑到了）原文的不同部分。</p>
<p>如seq2seq模型中，St是后面的query向量，而编码过程的隐藏状态hi是values。其就是根据某些规则（或额外信息query）从向量表达集合values中抽取特定的向量进行加权组合的方法，只要从部分向量里用了加权和，计算使用了attention机制。</p>
]]></content:encoded>
      <enclosure url="https://www.zhihu.com/equation?tex=X%3D%5Bx_1%2Cx_2%2C...x_n%5D" type="image/"/>
    </item>
    <item>
      <title>The attention in transformer （面经问题总结）</title>
      <link>https://enze5088.github.io/content/article-5/</link>
      <guid>https://enze5088.github.io/content/article-5/</guid>
      <source url="https://enze5088.github.io/rss.xml">The attention in transformer （面经问题总结）</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="the-attention-in-transformer-面经问题总结"> The attention in transformer （面经问题总结）</h1>
<p>在NLP算法相关的面试里，Transformer和其中的Attention机制显然是重中之重。而相关的高频提问笔者总结主要有如下几个问题形式。</p>
<h3 id="_1-问-自注意力-self-attention-的计算公式是怎样的"> 1. 问：自注意力(self-attention)的计算公式是怎样的？</h3>
<p>答：</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span><span>Attention </span></span><span>(</span><span>Q</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.07153em;">K</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.22222em;">V</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:2.4684em;vertical-align:-0.95em;"></span><span><span>softmax</span></span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>(</span></span><span><span></span><span><span><span><span style="height:1.5183em;"><span style="top:-2.2528em;"><span style="height:3em;"></span><span><span><span><span><span style="height:0.8572em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span style="height:3em;"></span><span style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span style="height:3em;"></span><span><span>Q</span><span><span style="margin-right:0.07153em;">K</span><span><span><span><span style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span style="height:0.93em;"><span></span></span></span></span></span><span></span></span><span style="top:0em;"><span>)</span></span></span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.22222em;">V</span></span><span><span style="height:2.4684em;vertical-align:-0.95em;"></span><span><span>(</span><span><span>1</span></span><span>)</span></span></span></span></span></span></p>
<p>其中<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8778em;vertical-align:-0.1944em;"></span><span>Q</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.07153em;">K</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.22222em;">V</span></span></span></span>为<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span>u</span><span style="margin-right:0.03588em;">ery</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03148em;">k</span><span style="margin-right:0.03588em;">ey</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">v</span><span>a</span><span style="margin-right:0.01968em;">l</span><span>u</span><span>e</span></span></span></span>， 而<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>so</span><span style="margin-right:0.10764em;">f</span><span>t</span><span>ma</span><span>x</span></span></span></span> 的作用是使得输出权重的概率分布和为1</p>
<h3 id="_2-问-transformer里自注意力机制的计算过程是怎样的"> 2.问：Transformer里自注意力机制的计算过程是怎样的？</h3>
<p>答：</p>
<div align="center">
<img src="https://www.hualigs.cn/image/60b88b2c3c9ab.jpg" alt="image-20210525031947534" style="zoom:50%;" align="center"/>
</div>
<blockquote>
<ol>
<li>将输入单词列表(句子)转化成嵌入向量列表<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span>[</span><span style="margin-right:0.02691em;">w</span><span style="margin-right:0.02778em;">or</span><span><span>d</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.02691em;">w</span><span style="margin-right:0.02778em;">or</span><span><span>d</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.02691em;">w</span><span style="margin-right:0.02778em;">or</span><span><span>d</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>3</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>.....</span><span style="margin-right:0.02691em;">w</span><span style="margin-right:0.02778em;">or</span><span><span>d</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>]</span></span></span></span></li>
<li>根据嵌入向量得到 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.625em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span></span></span></span> ,<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6944em;"></span><span style="margin-right:0.03148em;">k</span></span></span></span> ,<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.4306em;"></span><span style="margin-right:0.03588em;">v</span></span></span></span>三个向量以及对应的列表,<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span>[</span><span><span style="margin-right:0.03588em;">q</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span style="margin-right:0.03588em;">q</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>...</span><span><span style="margin-right:0.03588em;">q</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>]</span><span>,</span><span style="margin-right:0.1667em;"></span><span>[</span><span><span style="margin-right:0.03148em;">k</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span style="margin-right:0.03148em;">k</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>...</span><span><span style="margin-right:0.03148em;">k</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>]</span><span>,</span><span style="margin-right:0.1667em;"></span><span>[</span><span><span style="margin-right:0.03588em;">v</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span style="margin-right:0.03588em;">v</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>...</span><span><span style="margin-right:0.03588em;">v</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>]</span></span></span></span>；self-attention 中，<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03148em;">k</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">v</span></span></span></span>由 embedding 的结果经过不同的线性变换得到，维度都是 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>h</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>hi</span><span>dd</span><span>e</span><span>n</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="margin-right:0.2222em;"></span><span>×</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span style="margin-right:0.01968em;">l</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>src</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (前两步都是在自注意力计算之前)；</li>
<li>为每个向量计算一个score：<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.4306em;"></span><span>score</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.6389em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span style="margin-right:0.2222em;"></span><span>⋅</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:0.6944em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span> ；(<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6597em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span style="margin-right:0.2222em;"></span><span>∗</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:1em;vertical-align:-0.25em;"></span><span>[</span><span><span style="margin-right:0.03148em;">k</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span style="margin-right:0.03148em;">k</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>..</span><span><span style="margin-right:0.03148em;">k</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>]</span></span></span></span>)</li>
<li>为了梯度的稳定，Transformer使用了score归一化，具体论证见下文,即除以 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.04em;vertical-align:-0.1828em;"></span><span><span><span><span style="height:0.8572em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1828em;"><span></span></span></span></span></span></span></span></span> ；</li>
<li>对score施以softmax激活函数；</li>
<li>softmax点乘Value值 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.4306em;"></span><span style="margin-right:0.03588em;">v</span></span></span></span> ，得到加权的每个输入向量的评分 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.4306em;"></span><span style="margin-right:0.03588em;">v</span></span></span></span>  ；</li>
<li>相加之后得到最终的输出结果 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.4306em;"></span><span style="margin-right:0.04398em;">z</span><span style="margin-right:0.2778em;"></span><span>:</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.4306em;"></span><span style="margin-right:0.04398em;">z</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.104em;vertical-align:-0.2997em;"></span><span><span style="position:relative;top:0em;">∑</span><span><span><span><span style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span style="height:0.2997em;"><span></span></span></span></span></span></span><span style="margin-right:0.1667em;"></span><span><span style="margin-right:0.03588em;">v</span></span></span></span></span> 。</li>
</ol>
</blockquote>
<h3 id="_3-问-multi-head-attention是什么-有什么作用"> 3.问：Multi-Head Attention是什么，有什么作用？</h3>
<p>答：Multi-Head Attention相当于 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6944em;"></span><span>h</span></span></span></span> 个不同的self-attention的集成（ensemble），在这里我们以 <img src="https://www.zhihu.com/equation?tex=h%3D8" alt="[公式]" loading="lazy"> 举例说明。Multi-Head Attention的输出分成3步</p>
<blockquote>
<ol>
<li>将数据 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6833em;"></span><span style="margin-right:0.07847em;">X</span></span></span></span>分别输入到8个self-attention中，得到8个加权后的特征矩阵 <img src="https://www.zhihu.com/equation?tex=Z_i%2C+i\in\{1%2C2%2C...%2C8\}" alt="[公式]" loading="lazy"> 。</li>
<li>将8个 <img src="https://www.zhihu.com/equation?tex=Z_i" alt="[公式]" loading="lazy"> 按列拼成一个大的特征矩阵；</li>
<li>特征矩阵经过一层全连接后得到输出 <img src="https://www.zhihu.com/equation?tex=Z" alt="[公式]" loading="lazy"> 。</li>
</ol>
</blockquote>
<p>原论文中认为，将模型分为多个头，期望其形成多个相互独立子空间，可以让模型去关注不同方面的信息。但是也有很多论文不这么认为。也有人认为是计算复杂度的取舍（BERT中，multi-head 与直接使用768*768矩阵统一计算，有什么区别？ - 苏剑林的回答 - 知乎 https://www.zhihu.com/question/446385446/answer/1752279087），同时multi-head相对来说也比较冗余，mask掉一定比例的head对结果影响不大。</p>
<h3 id="_4-问-transformer的中的attention机制-其中self-attention和encoder-decoderattention之间的关系"> 4.问：Transformer的中的Attention机制，其中Self-Attention和Encoder-DecoderAttention之间的关系？</h3>
<p>答：</p>
<div align="center">
    <img src="https://www.hualigs.cn/image/60b88b6b67d05.jpg"  style="zoom:50%;" align="center"/>

<div align="center">
    <img src="https://www.hualigs.cn/image/60b88b8da7f06.jpg" alt="image20210525022950224" style="zoom: 67%;" align="center"/>

<p>Encoder-Decoder Attention如图所示，编码器一般有两层，自注意力和前馈神经网络（不算正则化和残差连接），解码器则一般有三层：</p>
<ul>
<li>自注意力层</li>
<li>Encoder-Decoder Attention 层</li>
<li>与位置无关的前馈网络层</li>
</ul>
<p>在解码器中，Decoder block比Encoder中多了个Encoder-Decoder Attention。在Encoder-Decoder Attention中， <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8778em;vertical-align:-0.1944em;"></span><span>Q</span></span></span></span>来自上一层的自注意力层， 而其则从编码器的输出中获取<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6833em;"></span><span style="margin-right:0.07153em;">K</span></span></span></span> 矩阵和 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6833em;"></span><span style="margin-right:0.22222em;">V</span></span></span></span>矩阵。其计算方式完全和encoder的过程相同。</p>
<p>因为在机器翻译这类问题里，解码过程是一个顺序操作的过程，也就是当解码第 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]" loading="lazy"> 个特征向量时，我们只能看到第 <img src="https://www.zhihu.com/equation?tex=k-1" alt="[公式]" loading="lazy"> 及其之前的解码结果，类似于LSTM根据前一个来生成后一个的过程。所以也把这种情况下的multi-head attention叫做masked multi-head attention。</p>
<h3 id="_5-问-都有哪些不同类型的注意力-其公式及计算过程和复杂度差异有什么区别。"> 5.问：都有哪些不同类型的注意力，其公式及计算过程和复杂度差异有什么区别。</h3>
<p>这个问题也会被分为几个子问题</p>
<blockquote>
<ol>
<li>加性注意力(Additive Attention、乘性注意力(Dot Product Attention),缩放点积注意力(Scaled Dot-Product Attention)的公式与计算过程？</li>
<li>Transformer计算attention的时候为何选择点乘而不是加法？</li>
<li>两者计算复杂度和效果上有什么区别？</li>
<li>为什么在进行<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>so</span><span style="margin-right:0.10764em;">f</span><span>t</span><span>ma</span><span>x</span></span></span></span>之前需要对attention进行scaled (attention计算时为什么要除<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.04em;vertical-align:-0.1828em;"></span><span><span><span><span style="height:0.8572em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1828em;"><span></span></span></span></span></span></span></span></span> ) ？</li>
</ol>
</blockquote>
<p>答：这几个问题在理解上紧密相连，所以我们放在一起进行讨论。</p>
<p>首先我们来看一下三者的公式，</p>
<p><strong>Additive Attention</strong>(加性注意力模型)</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span>s</span><span>(</span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">q</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.1413em;vertical-align:-0.25em;"></span><span><span style="margin-right:0.03588em;">v</span><span><span><span><span style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span>t</span><span>anh</span><span>(</span><span style="margin-right:0.13889em;">W</span><span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="margin-right:0.2222em;"></span><span>+</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.10903em;">U</span><span style="margin-right:0.03588em;">q</span><span>)</span></span><span><span style="height:1.1413em;vertical-align:-0.25em;"></span><span><span>(</span><span><span>2</span></span><span>)</span></span></span></span></span></span></p>
<p><strong>dot product attention</strong> （乘性注意力/点积模型）</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.05764em;">S</span><span>(</span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">q</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.1383em;vertical-align:-0.247em;"></span><span><span>x</span><span><span><span><span style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span><span>​</span></span><span><span style="height:0.247em;"><span></span></span></span></span></span></span><span style="margin-right:0.03588em;">q</span></span><span><span style="height:1.1413em;vertical-align:-0.25em;"></span><span><span>(</span><span><span>3</span></span><span>)</span></span></span></span></span></span></p>
<p><strong>Scaled Dot-Product Attention</strong>(缩放点积模型)：</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.05764em;">S</span><span>(</span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">q</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.2561em;vertical-align:-0.25em;"></span><span><span>x</span><span><span><span><span style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span><span>​</span></span><span><span style="height:0.247em;"><span></span></span></span></span></span></span><span style="margin-right:0.03588em;">q</span><span>/</span><span><span><span><span style="height:1.0061em;"><span style="top:-3.2em;"><span style="height:3.2em;"></span><span style="padding-left:1em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.9661em;"><span style="height:3.2em;"></span><span style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.2339em;"><span></span></span></span></span></span></span><span><span style="height:1.2561em;vertical-align:-0.25em;"></span><span><span>(</span><span><span>4</span></span><span>)</span></span></span></span></span></span></p>
<p>当然常见的还有 很多其他模型，如双线性模型 $ S(x_i,q)=x^{T}_{i}Wq $ 等：这里主要讨论上述三种</p>
<p><strong>计算过程详解：</strong></p>
<p>想象成一句中文与其对应的英文翻译，每个词为一个向量,两个序列也可以不等长，我们要计算两组序列彼此之间的attention权重。其过程大体可以被抽象为三步：</p>
<blockquote>
<p>① 信息输入：将 语言输入模型，转换为对应的向量，<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03148em;">k</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">v</span></span></span></span> 或者 <span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03148em;">k</span><span style="margin-right:0.03588em;">ey</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">v</span><span>a</span><span style="margin-right:0.01968em;">l</span><span>u</span><span>e</span></span></span></span>
我们这里假设存在两组向量序列来表示一句话与其对应的翻译。</p>
<p><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6861em;"></span><span><span>H</span></span><span style="margin-right:0.2778em;"></span><span>:</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span><span>h</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>h</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span>...</span><span><span>h</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6861em;"></span><span><span>S</span></span><span style="margin-right:0.2778em;"></span><span>:</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.625em;vertical-align:-0.1944em;"></span><span><span>s</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>s</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span>...</span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>s</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，任意一个元素的维度为<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>② 对于任意的<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.9805em;vertical-align:-0.2861em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>s</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.05724em;">j</span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>,以某种attention函数计算两者之间的分数</p>
<p>③ 信息加权平均</p>
</blockquote>
<p>这个过程对于Additive Attention而言，即为</p>
<blockquote>
<p>1.计算任意向量<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>与向量<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.625em;vertical-align:-0.1944em;"></span><span><span>s</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>s</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span>...</span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>s</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>n</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的attention权重</p>
<p>2.然后拼接<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="margin-right:0.2222em;"></span><span>−</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:0.5806em;vertical-align:-0.15em;"></span><span><span>s</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>1</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>通过一个特定的线性层计算出其分数，再拼接<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="margin-right:0.2222em;"></span><span>−</span><span style="margin-right:0.2222em;"></span></span><span><span style="height:0.5806em;vertical-align:-0.15em;"></span><span><span>s</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>2</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>重复通过线性层计算，...，直到所有的均计算出一个结果</p>
<p>3.再把所有结果通过<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>so</span><span style="margin-right:0.10764em;">f</span><span>t</span><span>ma</span><span>x</span></span></span></span>。</p>
</blockquote>
<p>其计算过程公式如下</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:7.4501em;vertical-align:-3.475em;"></span><span><span><span><span><span><span style="height:3.975em;"><span style="top:-5.975em;"><span style="height:3.6514em;"></span><span><span><span>c</span><span><span><span><span style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="margin-right:0.2778em;"></span><span>=</span></span></span><span style="top:-3.5574em;"><span style="height:3.6514em;"></span><span><span><span style="margin-right:0.0037em;">α</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span><span>,</span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span><span style="margin-right:0.2778em;"></span><span>=</span></span></span><span style="top:-1.4704em;"><span style="height:3.6514em;"></span><span><span>=</span></span></span></span><span>​</span></span><span><span style="height:3.475em;"><span></span></span></span></span></span><span><span><span><span style="height:3.975em;"><span style="top:-5.975em;"><span style="height:3.6514em;"></span><span><span></span><span style="margin-right:0.1667em;"></span><span><span><span><span style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span style="height:3.05em;"></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span style="top:-3.05em;"><span style="height:3.05em;"></span><span><span>∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span style="height:3.05em;"></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span style="height:1.2777em;"><span></span></span></span></span></span><span style="margin-right:0.1667em;"></span><span><span style="margin-right:0.0037em;">α</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span><span>,</span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span><span><span><span><span>h</span></span></span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="margin-right:1em;"></span><span>;</span><span style="margin-right:0.1667em;"></span><span>输出的上下文向量</span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.5574em;"><span style="height:3.6514em;"></span><span><span></span><span style="margin-right:0.1667em;"></span><span><span>align</span></span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="top:0em;">)</span></span><span style="margin-right:1em;"></span><span style="margin-right:0.1667em;"></span><span>;</span><span style="margin-right:0.1667em;"></span><span>两个单词</span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>和</span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>的对齐情况</span></span></span><span style="top:-1.4704em;"><span style="height:3.6514em;"></span><span><span></span><span><span></span><span><span><span><span style="height:1.427em;"><span style="top:-2.3057em;"><span style="height:3em;"></span><span><span><span style="position:relative;top:0em;">∑</span><span><span><span><span style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span><span>i</span><span><span><span><span style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span style="height:2.5em;"></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>=</span><span>1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span style="height:0.2997em;"><span></span></span></span></span></span></span><span style="margin-right:0.1667em;"></span><span>exp</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span>score</span></span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span><span><span>s</span></span></span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span style="height:0.2083em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span><span><span>h</span></span></span><span><span><span><span style="height:0.328em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span><span>i</span><span><span><span><span style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span style="height:2.5em;"></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="top:0em;">)</span></span><span style="top:0em;">)</span></span></span></span><span style="top:-3.23em;"><span style="height:3em;"></span><span style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span style="height:3em;"></span><span><span>exp</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span>score</span></span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span>s</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span style="height:0.2083em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span><span><span>h</span></span></span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="top:0em;">)</span></span><span style="top:0em;">)</span></span></span></span></span><span>​</span></span><span><span style="height:0.994em;"><span></span></span></span></span></span><span></span></span><span style="margin-right:1em;"></span><span>;</span><span style="margin-right:0.1667em;"></span><span>对自定义的对齐分数进行</span><span>so</span><span style="margin-right:0.10764em;">f</span><span>t</span><span>ma</span><span>x</span></span></span></span><span>​</span></span><span><span style="height:3.475em;"><span></span></span></span></span></span><span style="width:1em;"></span><span><span><span><span style="height:3.975em;"><span style="top:-5.975em;"><span style="height:3.6514em;"></span><span></span></span><span style="top:-3.5574em;"><span style="height:3.6514em;"></span><span></span></span><span style="top:-1.4704em;"><span style="height:3.6514em;"></span><span></span></span></span><span>​</span></span><span><span style="height:3.475em;"><span></span></span></span></span></span></span></span></span><span><span style="height:7.4501em;vertical-align:-3.475em;"></span><span><span>(</span><span><span>5</span></span><span>)</span></span></span></span></span></span></p>
<p>从这个角度讲，Additive Attention注意力的公式也可以看作</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.0496em;vertical-align:-0.3552em;"></span><span><span style="margin-right:0.10764em;">f</span><span><span><span><span style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.1076em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>score</span><span>(</span><span>i</span><span>,</span><span style="margin-right:0.05724em;">j</span><span>)</span></span></span></span></span><span>​</span></span><span><span style="height:0.3552em;"><span></span></span></span></span></span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.1774em;vertical-align:-0.2861em;"></span><span><span style="margin-right:0.03588em;">v</span><span><span><span><span style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>a</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span><span>​</span></span><span><span style="height:0.247em;"><span></span></span></span></span></span></span><span style="margin-right:0.1667em;"></span><span>tanh</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span><span>W</span></span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>a</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">[</span><span><span><span>h</span></span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>;</span><span style="margin-right:0.1667em;"></span><span><span><span>s</span></span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.05724em;">j</span></span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span><span style="top:0em;">]</span></span><span style="top:0em;">)</span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.2413em;vertical-align:-0.35em;"></span><span><span></span><span><span style="margin-right:0.03588em;">v</span><span><span><span><span style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span style="margin-right:0.1667em;"></span><span>tanh</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span style="margin-right:0.13889em;">W</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span><span>s</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="margin-right:0.2222em;"></span><span>+</span><span style="margin-right:0.2222em;"></span><span><span style="margin-right:0.13889em;">W</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.05724em;">j</span></span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span><span style="top:0em;">]</span></span><span style="top:0em;"><span>)</span></span></span></span><span><span style="height:1.2465em;vertical-align:-0.3552em;"></span><span><span>(</span><span><span>6</span></span><span>)</span></span></span></span></span></span></p>
<p>其中<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.5806em;vertical-align:-0.15em;"></span><span><span style="margin-right:0.03588em;">v</span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>a</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8361em;vertical-align:-0.15em;"></span><span><span><span>W</span></span><span><span><span><span style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>a</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是学习的注意参数。这里<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6944em;"></span><span><span>h</span></span></span></span></span>是指编码器的隐藏状态，<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.4444em;"></span><span><span>s</span></span></span></span></span>是解码器的隐藏状态。</p>
<p><strong>Dot Product Attention</strong> and <strong>Scaled Dot-Product Attention</strong></p>
<p>同样地，我们也可以将<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.0496em;vertical-align:-0.3552em;"></span><span><span style="margin-right:0.10764em;">f</span><span><span><span><span style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.1076em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>score</span><span>(</span><span>i</span><span>,</span><span style="margin-right:0.05724em;">j</span><span>)</span></span></span></span></span><span>​</span></span><span><span style="height:0.3552em;"><span></span></span></span></span></span></span></span></span></span>函数替换为乘性注意力，即</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.0496em;vertical-align:-0.3552em;"></span><span><span style="margin-right:0.10764em;">f</span><span><span><span><span style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.1076em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>score</span><span>(</span><span>i</span><span>,</span><span style="margin-right:0.05724em;">j</span><span>)</span></span></span></span></span><span>​</span></span><span><span style="height:0.3552em;"><span></span></span></span></span></span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.0361em;vertical-align:-0.2861em;"></span><span style="margin-right:0.13889em;">F</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span>s</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.05724em;">j</span></span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span><span style="top:0em;">)</span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.1774em;vertical-align:-0.2861em;"></span><span><span>s</span><span><span><span><span style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span><span>​</span></span><span><span style="height:0.247em;"><span></span></span></span></span></span></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.05724em;">j</span></span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span></span><span><span style="height:1.2465em;vertical-align:-0.3552em;"></span><span><span>(</span><span><span>7</span></span><span>)</span></span></span></span></span></span></p>
<p>而上述的计算过程不变，依次对任意的<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.9805em;vertical-align:-0.2861em;"></span><span><span>s</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.05724em;">j</span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>求出其乘性注意力分数,然后统一再<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span>so</span><span style="margin-right:0.10764em;">f</span><span>t</span><span>ma</span><span>x</span></span></span></span>。细心的同学也应该已经发现了，这个过程对于序列<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6861em;"></span><span><span>H</span></span></span></span></span>和<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6861em;"></span><span><span>S</span></span></span></span></span>而言,就是矩阵相乘的过程。</p>
<p>Dot Product Attention 和 Additive Attention两者在复杂度上是相似的，Additive Attention增加了三个可学习的矩阵，所以相比另外两个效果会更好，同时也增加了更多的模型参数，计算效率会较低。而且Dot Product Attention因为是直接坐矩阵运算，在实际情况下一般会更快一些与高效一些，因为一般的深度学习框架底层都有对矩阵运算的一些优化，有很多的并行算法和硬件加速，比如5X5的矩阵相加，框架底层可能会是25个数字同时计算。手动计算则是for循环一个一个相加。速度就会差上很多。而Self Attention则如上文所讲，对于一句话引入了<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03148em;">k</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">v</span></span></span></span>三个矩阵，也保证了计算速度,所以综合来看是最好的。其计算过程如上述内容。</p>
<p>同时我们也可以看到，对于任意两个维度为<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的元素<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.9805em;vertical-align:-0.2861em;"></span><span><span>s</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span><span>h</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.05724em;">j</span></span></span></span><span>​</span></span><span><span style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>相乘，在<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>较小时二者表现相似，但是<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>较大时，Dot Product Attention表现不如Additive Attention，因为<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>较大时点积之后的结果较大，方差也会变大，求softmax之后，梯度会很小,容易梯度消失，不利于计算，需要做一定的缩放。所以才会有了进一步的Scaled Dot-Product Attention</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.05764em;">S</span><span>(</span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03588em;">q</span><span>)</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.2561em;vertical-align:-0.25em;"></span><span><span>x</span><span><span><span><span style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.13889em;">T</span></span></span></span></span><span>​</span></span><span><span style="height:0.247em;"><span></span></span></span></span></span></span><span style="margin-right:0.03588em;">q</span><span>/</span><span><span><span><span style="height:1.0061em;"><span style="top:-3.2em;"><span style="height:3.2em;"></span><span style="padding-left:1em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.9661em;"><span style="height:3.2em;"></span><span style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.2339em;"><span></span></span></span></span></span></span><span><span style="height:1.2561em;vertical-align:-0.25em;"></span><span><span>(</span><span><span>8</span></span><span>)</span></span></span></span></span></span></p>
<p>除以<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.04em;vertical-align:-0.1828em;"></span><span><span><span><span style="height:0.8572em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1828em;"><span></span></span></span></span></span></span></span></span>的具体证明则如下(参考自paper原文)，</p>
<p>假设输入元素<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.5806em;vertical-align:-0.15em;"></span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>转化而来的<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span>,</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03148em;">k</span></span></span></span>每一维是均值为0、方差为1的独立随机变量，一共有<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>维，那么它们的点积<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8889em;vertical-align:-0.1944em;"></span><span style="margin-right:0.03588em;">q</span><span>⋅</span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.03148em;">k</span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.2887em;vertical-align:-0.2997em;"></span><span><span style="position:relative;top:0em;">∑</span><span><span><span><span style="height:0.989em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span><span>d</span><span><span><span><span style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span style="height:2.5em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span style="height:0.2997em;"><span></span></span></span></span></span></span><span style="margin-right:0.1667em;"></span><span><span style="margin-right:0.03588em;">q</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span><span style="margin-right:0.03148em;">k</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span>i</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的均值为0、方差为<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>因为，如果两个变量<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6833em;"></span><span style="margin-right:0.07847em;">X</span></span></span></span>和<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.6833em;"></span><span style="margin-right:0.22222em;">Y</span></span></span></span>是独立随机变量，则其乘积的方差与期望为</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.8em;vertical-align:-0.65em;"></span><span><span>var</span></span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>(</span></span><span style="position:relative;top:0em;">∑</span><span style="margin-right:0.1667em;"></span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="top:0em;"><span>)</span></span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.03148em;">k</span><span style="margin-right:0.1667em;"></span><span><span>var</span></span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">(</span><span><span>x</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="top:0em;">)</span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1.2141em;vertical-align:-0.35em;"></span><span style="margin-right:0.03148em;">k</span><span style="margin-right:0.05764em;">E</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>[</span></span><span><span>x</span><span><span><span><span style="height:0.8641em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>1</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span style="height:0.247em;"><span></span></span></span></span></span></span><span style="top:0em;"><span>]</span></span></span><span style="margin-right:0.1667em;"></span><span style="margin-right:0.05764em;">E</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>[</span></span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.8641em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>1</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span style="height:0.247em;"><span></span></span></span></span></span></span><span style="top:0em;"><span>]</span></span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.8641em;"></span><span style="margin-right:0.03148em;">k</span><span><span style="margin-right:0.03588em;">σ</span><span><span><span><span style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>4</span></span></span></span></span></span></span></span></span></span><span><span style="height:1.8em;vertical-align:-0.65em;"></span><span><span>(</span><span><span>9</span></span><span>)</span></span></span></span></span></span></p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.8em;vertical-align:-0.65em;"></span><span style="margin-right:0.05764em;">E</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>[</span></span><span style="position:relative;top:0em;">∑</span><span style="margin-right:0.1667em;"></span><span><span>x</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>i</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="top:0em;"><span>]</span></span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:1em;vertical-align:-0.25em;"></span><span style="margin-right:0.03148em;">k</span><span style="margin-right:0.05764em;">E</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;">[</span><span><span>x</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span><span style="margin-right:0.03588em;">y</span><span><span><span><span style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span><span style="top:0em;">]</span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.6444em;"></span><span>0</span></span><span><span style="height:1.8em;vertical-align:-0.65em;"></span><span><span>(</span><span><span>10</span></span><span>)</span></span></span></span></span></span></p>
<p>所以换算到上式中即方差为<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:0.8444em;vertical-align:-0.15em;"></span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,我们要控制方差为1，所以就要除以<span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:1.04em;vertical-align:-0.1828em;"></span><span><span><span><span style="height:0.8572em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1828em;"><span></span></span></span></span></span></span></span></span>，即下式</p>
<p class='katex-block'><span><span><span><i>Not supported content</i></span><span aria-hidden="true"><span><span style="height:2.4297em;vertical-align:-0.9797em;"></span><span style="margin-right:0.02778em;">D</span><span style="margin-right:0.1667em;"></span><span><span style="top:0em;"><span>(</span></span><span><span></span><span><span><span><span style="height:1.3714em;"><span style="top:-2.1778em;"><span style="height:3em;"></span><span><span><span><span><span><span style="height:0.9322em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span>d</span></span></span><span style="top:-2.8922em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1078em;"><span></span></span></span></span></span><span><span><span><span style="height:0.3286em;"><span style="top:-2.5425em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span><span>​</span></span><span><span style="height:0.1575em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span style="height:3em;"></span><span style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span style="height:3em;"></span><span><span style="margin-right:0.03588em;">q</span><span style="margin-right:0.2222em;"></span><span>⋅</span><span style="margin-right:0.2222em;"></span><span style="margin-right:0.03148em;">k</span></span></span></span><span>​</span></span><span><span style="height:0.9797em;"><span></span></span></span></span></span><span></span></span><span style="top:0em;"><span>)</span></span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:3.2655em;vertical-align:-1.894em;"></span><span><span></span><span><span><span><span style="height:1.3714em;"><span style="top:-2.11em;"><span style="height:3.354em;"></span><span><span><span><span style="top:0em;"><span>(</span></span><span><span><span><span><span style="height:0.9322em;"><span style="top:-3em;"><span style="height:3em;"></span><span style="padding-left:0.833em;"><span>d</span></span></span><span style="top:-2.8922em;"><span style="height:3em;"></span><span style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span>​</span></span><span><span style="height:0.1078em;"><span></span></span></span></span></span><span><span><span><span style="height:0.3286em;"><span style="top:-2.5425em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span><span>​</span></span><span><span style="height:0.1575em;"><span></span></span></span></span></span></span><span style="top:0em;"><span>)</span></span></span><span><span><span><span style="height:1.354em;"><span style="top:-3.6029em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span>2</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.584em;"><span style="height:3.354em;"></span><span style="border-bottom-width:0.04em;"></span></span><span style="top:-4.031em;"><span style="height:3.354em;"></span><span><span><span>d</span><span><span><span><span style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;"></span><span><span><span style="margin-right:0.03148em;">k</span></span></span></span></span><span>​</span></span><span><span style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span style="height:1.894em;"><span></span></span></span></span></span><span></span></span><span style="margin-right:0.2778em;"></span><span>=</span><span style="margin-right:0.2778em;"></span></span><span><span style="height:0.6444em;"></span><span>1</span></span><span><span style="height:3.344em;vertical-align:-1.894em;"></span><span><span>(</span><span><span>11</span></span><span>)</span></span></span></span></span></span></p>
<p>将方差控制为1，也就有效地控制了前面提到的梯度消失的问题。</p>
<h3 id="_6-问-self-attention代码实现"> 6.问：self-attention代码实现？</h3>
<p>答：</p>
<div><pre><code>
import torch.nn as nn

def clones(module, N):
    &quot;&quot;&quot;
    Produce N identical layers.
    &quot;&quot;&quot;
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


def attention(query, key, value, mask=None, dropout=None):
    &quot;&quot;&quot;
    Compute &#39;Scaled Dot Product Attention&#39;
    &quot;&quot;&quot;
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn


class MultiHeadedAttention(nn.Module):
    &quot;&quot;&quot;
    Implements &#39;Multi-Head Attention&#39; proposed in the paper.
    &quot;&quot;&quot;

    def __init__(self, h, d_model, dropout=0.1):
        &quot;&quot;&quot;
        Take in model size and number of heads.
        &quot;&quot;&quot;
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model =&gt; h x d_k
        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
                             for l, x in zip(self.linears, (query, key, value))]

        # 2) Apply attention on all the projected vectors in batch.
        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)

        # 3) &quot;Concat&quot; using a view and apply a final linear.
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br></div></div><h2 id="参考"> 参考</h2>
<ol>
<li>详解Transformer （Attention Is All You Need） - 大师兄的文章 - 知乎 https://zhuanlan.zhihu.com/p/48508221</li>
<li>拆 Transformer 系列二：Multi- Head Attention 机制详解 - 随时学丫的文章 - 知乎 https://zhuanlan.zhihu.com/p/109983672</li>
<li>Attention的一些笔记 - 你小妹儿儿儿的文章 - 知乎 https://zhuanlan.zhihu.com/p/137464173</li>
<li>transformer中的attention为什么scaled? - TniL的回答 - 知乎 https://www.zhihu.com/question/339723385/answer/782509914</li>
<li>Neural Machine Translation by Jointly Learning to Align and Translate</li>
<li>http://jalammar.github.io/illustrated-transformer/</li>
<li>https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</li>
<li>http://nlp.seas.harvard.edu/2018/04/03/attention.html</li>
</ol>
]]></content:encoded>
      <enclosure url="https://www.zhihu.com/equation?tex=h%3D8" type="image/"/>
    </item>
    <item>
      <title>对比学习研究</title>
      <link>https://enze5088.github.io/content/article-6/</link>
      <guid>https://enze5088.github.io/content/article-6/</guid>
      <source url="https://enze5088.github.io/rss.xml">对比学习研究</source>
      <category>科研与求索</category>
      <pubDate>Wed, 23 Jun 2021 13:18:25 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="对比学习科研"> 对比学习科研</h1>
]]></content:encoded>
    </item>
    <item>
      <title>技术博文列表</title>
      <link>https://enze5088.github.io/content/</link>
      <guid>https://enze5088.github.io/content/</guid>
      <source url="https://enze5088.github.io/rss.xml">技术博文列表</source>
      <category>博文</category>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="技术博文列表"> 技术博文列表</h1>
]]></content:encoded>
    </item>
    <item>
      <title>博客</title>
      <link>https://enze5088.github.io/home/</link>
      <guid>https://enze5088.github.io/home/</guid>
      <source url="https://enze5088.github.io/rss.xml">博客</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
    </item>
    <item>
      <title>Intro Page</title>
      <link>https://enze5088.github.io/intro/</link>
      <guid>https://enze5088.github.io/intro/</guid>
      <source url="https://enze5088.github.io/rss.xml">Intro Page</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="intro-page"> Intro Page</h1>
<h3 id="我是谁"> 我是谁</h3>
<p>我是中国科学院大学人工智能学院的学生MP。我目前由Prof. Xiaolong Zheng指导，主要研究自然语言处理以及跨模态检索。 我也是新浪微博AiLab的NLP实习生，预计7月份正式入职。 我正在寻找博士学位的机会以及相应的研究助理/实习生职位。如果您有什么事，可以<a href="mailto:pu.miao@foxmail.com">邮件</a>联系我。</p>
<h1 id="about-me"> about me</h1>
<h3 id="hi-there-👋"> Hi there 👋</h3>
<p>I'm PuMiao, a student at the University of the Chinese Academy of Sciences major in Artificial Intelligent.
I am currently a student at the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences supervised by Prof. Xiaolong Zheng, focus on Natural language processing. I'm also an NLP intern at Sina Weibo AI Lab now.</p>
<p>Coding is probably my favorite thing. I am looking for Ph.D. opportunities as well as research assistant/intern positions. Please drop me an <a href="mailto:pu.miao@foxmail.com">email</a></p>
<hr>
<p><img align="left" alt="Fred's GitHub Stats" src="https://github-readme-stats.vercel.app/api?username=enze5088&show_icons=true&count_private=true&theme=chartreuse-dark&hide_border=true" height="140"/><img align="center" src="https://github-readme-stats.vercel.app/api/top-langs/?username=enze5088&layout=compact&theme=chartreuse-dark&hide_border=true" height="140"/></p>
]]></content:encoded>
    </item>
    <item>
      <title>幻灯片页</title>
      <link>https://enze5088.github.io/slides/</link>
      <guid>https://enze5088.github.io/slides/</guid>
      <source url="https://enze5088.github.io/rss.xml">幻灯片页</source>
      <pubDate>Sat, 15 May 2021 07:21:12 GMT</pubDate>
      <content:encoded><![CDATA[
<i>Not supported content</i>]]></content:encoded>
      <enclosure url="https://enze5088.github.io/logo.svg" type="image/svg+xml"/>
    </item>
  </channel>
</rss>